<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>人工智能科技与文献网</title>
      <link href="/2022/11/20/ai1/"/>
      <url>/2022/11/20/ai1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/36963108/163676068-3aac29a3-95d5-4fd1-9e04-ae54ceb415fb.png" alt="image"></p><p>AI新闻网：<a href="https://www.marktechpost.com/">https://www.marktechpost.com/</a></p><p>算法核心基础与AI模型设计【我的CSDN技术博客】：<a href="https://blog.csdn.net/weixin_41194129/category_11362509.html">https://blog.csdn.net/weixin_41194129/category_11362509.html</a></p><p>AI算法学习社区: <a href="https://github.com/Algorithm-learning-community-for-python">https://github.com/Algorithm-learning-community-for-python</a></p><p>YOLO系列资料汇总：<a href="https://github.com/KangChou/Cver4s">https://github.com/KangChou/Cver4s</a></p><iframe src="https://player.bilibili.com/player.html?aid=556216400&bvid=BV1Se4y1975L&cid=781812098&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="440"> </iframe><p>NVIDIA-CUDA编程:<a href="https://github.com/KangChou/deepcv_project_demo/tree/main/CUDA%E7%BC%96%E7%A8%8B">https://github.com/KangChou/deepcv_project_demo/tree/main/CUDA%E7%BC%96%E7%A8%8B</a></p><p>自动驾驶点云技术: <a href="https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/point-cloud">https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/point-cloud</a></p><p>计算机视觉技术： <a href="https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/visual">https://github.com/KangChou/deepcv_project_demo/tree/main/CVPR/visual</a></p><p><img src="https://miro.medium.com/max/700/1*m416DZjEp9-_cmRgG9i10w.jpeg" alt="jpeg"></p><p>专业的聊天机器人: <a href="https://github.com/salesforce/Converse">https://github.com/salesforce/Converse</a></p><p>基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化:<a href="https://github.com/EssayKillerBrain/EssayKiller_V2">https://github.com/EssayKillerBrain/EssayKiller_V2</a></p><p>高质量中文预训练模型集合:<a href="https://github.com/CLUEbenchmark/CLUEPretrainedModels">https://github.com/CLUEbenchmark/CLUEPretrainedModels</a></p><p>自然语言基础模型:<a href="https://github.com/lpty/nlp_base">https://github.com/lpty/nlp_base</a></p><p>BERT模型从训练到部署全流程:<a href="https://github.com/xmxoxo/BERT-train2deploy">https://github.com/xmxoxo/BERT-train2deploy</a></p><p>中文BERT-wwm系列模型:<a href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>深度学习入门教程, 优秀文章: <a href="https://github.com/Mikoto10032/DeepLearning">https://github.com/Mikoto10032/DeepLearning</a></p><p>3D视觉、VSLAM、计算机视觉的干货资料: <a href="https://github.com/qxiaofan/awesome_3d_slam_resources">https://github.com/qxiaofan/awesome_3d_slam_resources</a></p><p>自动驾驶系统实现:<a href="https://github.com/sunmiaozju/smartcar">https://github.com/sunmiaozju/smartcar</a></p><p>身份证自动识别,银行卡识别,驾驶证识别,行驶证识别：<a href="https://github.com/wenchaosong/OCR_identify">https://github.com/wenchaosong/OCR_identify</a></p><p>MVision 机器视觉 机器视觉：<a href="https://github.com/Ewenwan/MVision">https://github.com/Ewenwan/MVision</a></p><p>Computer Vision: Algorithms and Applications：<a href="https://szeliski.org/Book/">https://szeliski.org/Book/</a></p><p>自动驾驶的激光雷达点云处理: <a href="https://github.com/beedotkiran/Lidar_For_AD_references">https://github.com/beedotkiran/Lidar_For_AD_references</a></p><p>动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库:<a href="https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic">https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic</a></p><p>基于视频的目标检测算法研究:<a href="https://github.com/guanfuchen/video_obj">https://github.com/guanfuchen/video_obj</a></p><p>TensorRT-7 Network: <a href="https://github.com/Syencil/tensorRT">https://github.com/Syencil/tensorRT</a></p><p>C++ TensorRT-CenterNet: <a href="https://github.com/CaoWGG/TensorRT-CenterNet">https://github.com/CaoWGG/TensorRT-CenterNet</a></p><p>yolox-deepsort:<a href="https://github.com/Sharpiless/yolox-deepsort">https://github.com/Sharpiless/yolox-deepsort</a></p><p><img src="https://github.com/CaoWGG/TensorRT-CenterNet/raw/master/img/show3.png" alt="jpeg"></p><p>BirdNet+：LiDAR 鸟瞰图中的端到端 3D 对象检测:<a href="https://github.com/AlejandroBarrera/birdnet2">https://github.com/AlejandroBarrera/birdnet2</a></p><p>关于nuScenes 数据集的开发套件:<a href="https://github.com/nutonomy/nuscenes-devkit">https://github.com/nutonomy/nuscenes-devkit</a></p><p>A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR:<a href="https://github.com/hku-mars/loam_livox">https://github.com/hku-mars/loam_livox</a></p><p>激光雷达论文：<a href="https://arxiv.org/search/?query=+LiDAR&amp;searchtype=all&amp;source=header">https://arxiv.org/search/?query=+LiDAR&amp;searchtype=all&amp;source=header</a></p><p>使用CUDA PCL 加速Jetson的点云处理：<a href="https://developer.nvidia.com/zh-cn/blog/cuda-pcl-1-0-jetson/">https://developer.nvidia.com/zh-cn/blog/cuda-pcl-1-0-jetson/</a></p><p>PCT: Point Cloud Transformer: <a href="https://github.com/MenghaoGuo/PCT">https://github.com/MenghaoGuo/PCT</a></p><p><img src="https://img-blog.csdnimg.cn/60fc01ba8b7147768343bfd11a7721d0.png" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新版OpenCV5(C++)版本目标检测:YOLOv4打包成可用AI软件(离线或者上线使用)</title>
      <link href="/2022/09/20/yolocv5/"/>
      <url>/2022/09/20/yolocv5/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/c3f2497028cb440f884e3526669d3ca3.png" alt="在这里插入图片描述"></p><p>上一篇文章:<br>1、<a href="https://blog.csdn.net/weixin_41194129/article/details/126650822?spm=1001.2014.3001.5501">C++版本的OpenCV 5.x编译生成opencv-python&#x3D;&#x3D;5.x(GPU版本)接口并进行调用</a><br>2、<a href="https://blog.csdn.net/weixin_41194129/article/details/126511773?spm=1001.2014.3001.5501">【强力推荐】基于Nvidia-Docker-Linux(Ubuntu18.04)平台：新版OpenCV5.x(C++)联合CUDA11.1(GPU)完美配置视觉算法开发环境</a><br>3、<a href="https://blog.csdn.net/weixin_41194129/article/details/123951358">AI模型C++部署:【配置OpenCV4++环境】与【三种在 C++ 中部署 TensorFlow 模型的方式】【准备阶段】</a><br>4、<a href="https://blog.csdn.net/weixin_41194129/article/details/126684495">yolov4视频目标检测：使用C++版本联合CUDA11.2的OpenCV 5.x编译生成opencv-python&#x3D;&#x3D;5.x进行推理</a></p><h1 id="需要软件请在平论区留言，源码会开源，持续关注本博客。"><a href="#需要软件请在平论区留言，源码会开源，持续关注本博客。" class="headerlink" title="需要软件请在平论区留言，源码会开源，持续关注本博客。"></a><em><strong>需要软件请在平论区留言，源码会开源，持续关注本博客。</strong></em></h1><ul><li>1、打包成linux软件，后续增加QT功能</li><li>2、软件增加了加密功能，后续会开源代码，敬请期待<br><img src="https://img-blog.csdnimg.cn/2df3df92db9646f292fda28a6e468a80.png" alt="在这里插入图片描述"></li><li>3、增加计算使用剩余天数</li></ul><p><img src="https://img-blog.csdnimg.cn/828102f1fcc74cb5abde55cd0b14eeb7.png" alt="在这里插入图片描述"></p><ul><li>4、增加等等功能组件…不说了，后面会一一展示..</li></ul><h1 id="本YOLO目标检测软件只在ubuntu18-04平台使用"><a href="#本YOLO目标检测软件只在ubuntu18-04平台使用" class="headerlink" title="本YOLO目标检测软件只在ubuntu18.04平台使用"></a>本YOLO目标检测软件只在ubuntu18.04平台使用</h1><p>工程目录：<br><img src="https://img-blog.csdnimg.cn/6ac843a04035496eb7eb8c06323d47b4.png" alt="在这里插入图片描述"></p><p>可执行文件为：sudo .&#x2F;main.sh<br>其他就是yolo的配置文件和模型了：<br><img src="https://img-blog.csdnimg.cn/0b9f6195a63c4fe28955cf52c478a3d2.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f6cd02ef3e0d49c789a1121c160c08f0.png" alt="在这里插入图片描述"></p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><p>测试图片存放位置,格式只要是jpg,随便往里仍：<br><img src="https://img-blog.csdnimg.cn/f5104c4c8c524dc995a94c6209817337.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/32e0a6bcb59f49a5a838005489bc0b6d.png" alt="在这里插入图片描述"></p><p>保存的结果在result文件目录下：<br><img src="https://img-blog.csdnimg.cn/8a0a309855e14347b474cc543efef7f8.png" alt="在这里插入图片描述"></p><p>直接将软件在拿走在UBUNTU18使用即可：sudo .&#x2F;main.sh</p><p>视频操作教程:  <a href="https://live.csdn.net/v/239286?spm=1001.2014.3001.5501">https://live.csdn.net/v/239286?spm=1001.2014.3001.5501</a></p>]]></content>
      
      
      <categories>
          
          <category> AI目标检测算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI模型C++部署:TensorFlow2图像分类模型之金钱豹大战齐天大圣【OpenCV纯C++接口调用tensorflow生成的pb模型】【源码已开源】</title>
      <link href="/2022/04/04/codemo/"/>
      <url>/2022/04/04/codemo/</url>
      
        <content type="html"><![CDATA[<p><img src="https://images.wallpaperscraft.com/image/single/laptop_code_programming_212332_1280x720.jpg" alt="img"></p><p>CSDN原文:<a href="https://blog.csdn.net/weixin_41194129/article/details/123958572">AI模型C++部署:TensorFlow2图像分类模型之金钱豹大战齐天大圣【OpenCV纯C++接口调用tensorflow生成的pb模型】【源码已开源】</a></p><p><img src="https://img-blog.csdnimg.cn/182ed179c311496e9a0b7639efa852ae.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/85b2be77fbbd4802af96c43feb146817.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/d1732a50fdc54014b691f69e6156e547.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/1ce807f6fdb54a9b9cf3181d606465bb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h1 id="1、准备训练数据【金钱豹与齐天大圣】"><a href="#1、准备训练数据【金钱豹与齐天大圣】" class="headerlink" title="1、准备训练数据【金钱豹与齐天大圣】"></a>1、准备训练数据【金钱豹与齐天大圣】</h1><p>ubuntu16.04开发环境1：python3.6+cuda11+tensoflow-gpu 2.5<br>ubuntu16.04开发环境2：python3.6+cuda10+tensoflow-gpu 1.13<br><img src="https://img-blog.csdnimg.cn/d38c913111f9457bb41b42e6e0696536.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_10,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>这里的数据处理包括reshape、去噪、对比度增强、批量剪切、图片灰度、二值化、缩放、丰富数据等等。</p><h2 id="1-0、获取数据地址"><a href="#1-0、获取数据地址" class="headerlink" title="1.0、获取数据地址"></a>1.0、获取数据地址</h2><p><a href="https://github.com/KangChou/classeifier_models/blob/main/deal_datasets/down_datasets/golds_and_monkey.zip">https://github.com/KangChou/classeifier_models/blob/main/deal_datasets/down_datasets/golds_and_monkey.zip</a></p><h2 id="1-1、处理数据集100-100"><a href="#1-1、处理数据集100-100" class="headerlink" title="1.1、处理数据集100*100"></a>1.1、处理数据集100*100</h2><p>将数据处理成尺寸大小一样的jpg图片，处理的 代码</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># function:change the size of pictures in one folder&quot;</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取不规范图片数据，并规范为相同大小格式</span></span><br><span class="line"></span><br><span class="line">image_size = <span class="number">100</span>  <span class="comment"># 设定尺寸</span></span><br><span class="line">source_path = <span class="string">&quot;/home/zkpark/Documents/pcl2022/opencv_cc_tensorflow/golds_and_monkey/train/monkey_king/&quot;</span>  <span class="comment"># 源文件路径</span></span><br><span class="line"><span class="comment"># source_path = &quot;/home/zkpark/Documents/pcl2022/opencv_cc_tensorflow/golds_and_monkey/train/gold/&quot;  # 源文件路径</span></span><br><span class="line">target_path = <span class="string">&quot;./dataset2/monkey_king/&quot;</span>  <span class="comment"># 输出目标文件路径</span></span><br><span class="line"><span class="comment"># target_path = &quot;./dataset2/gold/&quot;  # 输出目标文件路径</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(target_path):</span><br><span class="line">    os.makedirs(target_path)</span><br><span class="line"></span><br><span class="line">image_list = os.listdir(source_path)  <span class="comment"># 获得文件名</span></span><br><span class="line"></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> image_list:</span><br><span class="line">    i = i + <span class="number">1</span></span><br><span class="line">    image_source = cv2.imread(source_path + file)  <span class="comment"># 读取图片</span></span><br><span class="line">    <span class="built_in">print</span>(image_source)</span><br><span class="line">    image = cv2.resize(image_source, (image_size, image_size), <span class="number">0</span>, <span class="number">0</span>, cv2.INTER_LINEAR)  <span class="comment"># 修改尺寸</span></span><br><span class="line">    cv2.imwrite(target_path + <span class="string">&#x27;1-&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;.jpg&quot;</span>, image)  <span class="comment"># 重命名并且保存</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;批量处理完成&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-2、数据批量处理"><a href="#1-2、数据批量处理" class="headerlink" title="1.2、数据批量处理"></a>1.2、数据批量处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_img</span>(<span class="params">path, w, h</span>):</span><br><span class="line">    cate = [path + x <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(path) <span class="keyword">if</span> os.path.isdir(path + x)]</span><br><span class="line">    <span class="comment"># print(cate)</span></span><br><span class="line"></span><br><span class="line">    imgs = []</span><br><span class="line">    labels = []</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Start read the image ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index, folder <span class="keyword">in</span> <span class="built_in">enumerate</span>(cate):</span><br><span class="line">        <span class="comment"># print(index, folder)</span></span><br><span class="line">        <span class="keyword">for</span> im <span class="keyword">in</span> glob.glob(folder + <span class="string">&#x27;/*.jpg&#x27;</span>):</span><br><span class="line">            <span class="comment"># print(&#x27;Reading The Image: %s&#x27; % im)</span></span><br><span class="line">            img = io.imread(im)</span><br><span class="line">            img = transform.resize(img, (w, h))</span><br><span class="line">            imgs.append(img)</span><br><span class="line">            <span class="comment"># print(index)</span></span><br><span class="line">            labels.append(index)                    <span class="comment"># 每个子文件夹是一个label</span></span><br><span class="line">            <span class="comment"># print(len(labels))</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished ...&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(imgs))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(labels))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.asarray(imgs, np.float32), np.asarray(labels, np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打乱顺序 为了均匀取数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">messUpOrder</span>(<span class="params">data, label</span>):</span><br><span class="line">    num_example = data.shape[<span class="number">0</span>]</span><br><span class="line">    arr = np.arange(num_example)</span><br><span class="line">    np.random.shuffle(arr)</span><br><span class="line">    data = data[arr]</span><br><span class="line">    label = label[arr]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将所有数据分为训练集和验证集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">segmentation</span>(<span class="params">data, label, ratio=<span class="number">0.8</span></span>):</span><br><span class="line">    num_example = data.shape[<span class="number">0</span>]</span><br><span class="line">    s = np.<span class="built_in">int</span>(num_example * ratio)</span><br><span class="line">    x_train = data[:s]</span><br><span class="line">    y_train = label[:s]</span><br><span class="line">    x_val = data[s:]</span><br><span class="line">    y_val = label[s:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_val, y_val</span><br></pre></td></tr></table></figure><h1 id="2、tensorflow2-x模型生成"><a href="#2、tensorflow2-x模型生成" class="headerlink" title="2、tensorflow2.x模型生成"></a>2、tensorflow2.x模型生成</h1><p>tensorflow1代码升级到2的方法请参考：<a href="https://blog.csdn.net/weixin_41194129/article/details/121499555">https://blog.csdn.net/weixin_41194129&#x2F;article&#x2F;details&#x2F;121499555</a></p><h2 id="2-0、TF2分类模型网络构建"><a href="#2-0、TF2分类模型网络构建" class="headerlink" title="2.0、TF2分类模型网络构建"></a>2.0、TF2分类模型网络构建</h2><h3 id="2-0-1、模型网络结构与训练"><a href="#2-0-1、模型网络结构与训练" class="headerlink" title="2.0.1、模型网络结构与训练"></a>2.0.1、模型网络结构与训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">buildCNN</span>(<span class="params">w, h, c</span>):</span><br><span class="line">    <span class="comment"># 占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, w, h, c], name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>, ], name=<span class="string">&#x27;y_&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一个卷积层 + 池化层（100——&gt;50)</span></span><br><span class="line">    conv1 = tf.layers.conv2d(</span><br><span class="line">        inputs=x,</span><br><span class="line">        filters=<span class="number">32</span>,</span><br><span class="line">        kernel_size=[<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">        padding=<span class="string">&quot;same&quot;</span>,</span><br><span class="line">        activation=tf.nn.relu,</span><br><span class="line">        kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二个卷积层 + 池化层 (50-&gt;25)</span></span><br><span class="line">    conv2 = tf.layers.conv2d(</span><br><span class="line">        inputs=pool1,</span><br><span class="line">        filters=<span class="number">64</span>,</span><br><span class="line">        kernel_size=[<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">        padding=<span class="string">&quot;same&quot;</span>,</span><br><span class="line">        activation=tf.nn.relu,</span><br><span class="line">        kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第三个卷积层 + 池化层 (25-&gt;12)</span></span><br><span class="line">    conv3 = tf.layers.conv2d(</span><br><span class="line">        inputs=pool2,</span><br><span class="line">        filters=<span class="number">128</span>,</span><br><span class="line">        kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        padding=<span class="string">&quot;same&quot;</span>,</span><br><span class="line">        activation=tf.nn.relu,</span><br><span class="line">        kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第四个卷积层 + 池化层 (12-&gt;6)</span></span><br><span class="line">    conv4 = tf.layers.conv2d(</span><br><span class="line">        inputs=pool3,</span><br><span class="line">        filters=<span class="number">128</span>,</span><br><span class="line">        kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        padding=<span class="string">&quot;same&quot;</span>,</span><br><span class="line">        activation=tf.nn.relu,</span><br><span class="line">        kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line">    pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[<span class="number">2</span>, <span class="number">2</span>], strides=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    re1 = tf.reshape(pool4, [-<span class="number">1</span>, <span class="number">6</span> * <span class="number">6</span> * <span class="number">128</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    dense1 = tf.layers.dense(inputs=re1,</span><br><span class="line">                             units=<span class="number">1024</span>,</span><br><span class="line">                             activation=tf.nn.relu,</span><br><span class="line">                             kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">                             kernel_regularizer=tf.contrib.layers.l2_regularizer(<span class="number">0.003</span>))</span><br><span class="line">    dense2 = tf.layers.dense(inputs=dense1,</span><br><span class="line">                             units=<span class="number">512</span>,</span><br><span class="line">                             activation=tf.nn.relu,</span><br><span class="line">                             kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">                             kernel_regularizer=tf.contrib.layers.l2_regularizer(<span class="number">0.003</span>))</span><br><span class="line">    logits = tf.layers.dense(inputs=dense2,</span><br><span class="line">                             units=<span class="number">32</span>,</span><br><span class="line">                             activation=<span class="literal">None</span>,</span><br><span class="line">                             kernel_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">                             kernel_regularizer=tf.contrib.layers.l2_regularizer(<span class="number">0.003</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, x, y_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回损失函数的值，准确值等参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accCNN</span>(<span class="params">logits, y_</span>):</span><br><span class="line">    loss = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=logits)</span><br><span class="line">    train_op = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">    correct_prediction = tf.equal(tf.cast(tf.argmax(logits, <span class="number">1</span>), tf.int32), y_)</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, train_op, correct_prediction, acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，按批次取数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minibatches</span>(<span class="params">inputs=<span class="literal">None</span>, targets=<span class="literal">None</span>, batch_size=<span class="literal">None</span>, shuffle=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(inputs) == <span class="built_in">len</span>(targets), <span class="string">&#x27;len(inputs) != len(targets)&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indices = np.arange(<span class="built_in">len</span>(inputs))   <span class="comment"># [0, 1, 2, ..., 422]</span></span><br><span class="line">        np.random.shuffle(indices)     <span class="comment"># 打乱下标顺序</span></span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(inputs) - batch_size + <span class="number">1</span>, batch_size):</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            excerpt = indices[start_idx:start_idx + batch_size]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            excerpt = <span class="built_in">slice</span>(start_idx, start_idx + batch_size)</span><br><span class="line">        <span class="keyword">yield</span> inputs[excerpt], targets[excerpt]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练和测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">runable</span>(<span class="params">x_train, y_train, train_op, loss, acc, x, y_, x_val, y_val</span>):</span><br><span class="line">    <span class="comment"># 训练和测试数据，可将n_epoch设置更大一些</span></span><br><span class="line">    n_epoch = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epoch):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, epoch)</span><br><span class="line">        <span class="comment"># training</span></span><br><span class="line">        train_loss, train_acc, n_batch = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x_train_a, y_train_a <span class="keyword">in</span> minibatches(x_train, y_train, batch_size, shuffle=<span class="literal">True</span>):</span><br><span class="line">            <span class="comment"># print(&#x27;x_val_a: &#x27;, x_val_a.shape())</span></span><br><span class="line">            <span class="comment"># print(&#x27;y_val_a: &#x27;, y_val_a.shape())</span></span><br><span class="line">            _, err, ac = sess.run([train_op, loss, acc], feed_dict=&#123;x: x_train_a, y_: y_train_a&#125;)</span><br><span class="line">            train_loss += err</span><br><span class="line">            train_acc += ac</span><br><span class="line">            n_batch += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train loss: %f&quot;</span> % (train_loss / n_batch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train acc: %f&quot;</span> % (train_acc / n_batch))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validation</span></span><br><span class="line">        val_loss, val_acc, n_batch = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x_val_a, y_val_a <span class="keyword">in</span> minibatches(x_val, y_val, batch_size, shuffle=<span class="literal">False</span>):</span><br><span class="line">            <span class="comment"># print(&#x27;x_val_a: &#x27;, x_val_a)</span></span><br><span class="line">            <span class="comment"># print(&#x27;y_val_a: &#x27;, y_val_a)</span></span><br><span class="line">            err, ac = sess.run([loss, acc], feed_dict=&#123;x: x_val_a, y_: y_val_a&#125;)</span><br><span class="line">            val_loss += err</span><br><span class="line">            val_acc += ac</span><br><span class="line">            n_batch += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;validation loss: %f&quot;</span> % (val_loss / n_batch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;validation acc: %f&quot;</span> % (val_acc / n_batch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    sess.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-0-2、训练输出"><a href="#2-0-2、训练输出" class="headerlink" title="2.0.2、训练输出"></a>2.0.2、训练输出</h3><p><img src="https://img-blog.csdnimg.cn/9f41e478e56d451c978426590538ab2d.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train loss: <span class="number">1.939822</span></span><br><span class="line">train acc: <span class="number">0.526667</span></span><br><span class="line">validation loss: <span class="number">1.844313</span></span><br><span class="line">validation acc: <span class="number">0.600000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">1</span></span><br><span class="line">train loss: <span class="number">1.482922</span></span><br><span class="line">train acc: <span class="number">0.513333</span></span><br><span class="line">validation loss: <span class="number">0.704169</span></span><br><span class="line">validation acc: <span class="number">0.600000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">2</span></span><br><span class="line">train loss: <span class="number">1.043171</span></span><br><span class="line">train acc: <span class="number">0.546667</span></span><br><span class="line">validation loss: <span class="number">1.278273</span></span><br><span class="line">validation acc: <span class="number">0.600000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">3</span></span><br><span class="line">train loss: <span class="number">1.165261</span></span><br><span class="line">train acc: <span class="number">0.546667</span></span><br><span class="line">validation loss: <span class="number">0.706824</span></span><br><span class="line">validation acc: <span class="number">0.600000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">4</span></span><br><span class="line">train loss: <span class="number">0.843956</span></span><br><span class="line">train acc: <span class="number">0.553333</span></span><br><span class="line">validation loss: <span class="number">0.733591</span></span><br><span class="line">validation acc: <span class="number">0.600000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">5</span></span><br><span class="line">train loss: <span class="number">0.754458</span></span><br><span class="line">train acc: <span class="number">0.546667</span></span><br><span class="line">validation loss: <span class="number">0.714196</span></span><br><span class="line">validation acc: <span class="number">0.400000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">6</span></span><br><span class="line">train loss: <span class="number">0.726300</span></span><br><span class="line">train acc: <span class="number">0.446667</span></span><br><span class="line">validation loss: <span class="number">0.754467</span></span><br><span class="line">validation acc: <span class="number">0.400000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">7</span></span><br><span class="line">train loss: <span class="number">0.730567</span></span><br><span class="line">train acc: <span class="number">0.460000</span></span><br><span class="line">validation loss: <span class="number">0.759676</span></span><br><span class="line">validation acc: <span class="number">0.400000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">8</span></span><br><span class="line">train loss: <span class="number">0.730799</span></span><br><span class="line">train acc: <span class="number">0.453333</span></span><br><span class="line">validation loss: <span class="number">0.739154</span></span><br><span class="line">validation acc: <span class="number">0.400000</span></span><br><span class="line">**************************************************</span><br><span class="line">epoch:  <span class="number">9</span></span><br><span class="line">train loss: <span class="number">0.712395</span></span><br><span class="line">train acc: <span class="number">0.440000</span></span><br><span class="line">validation loss: <span class="number">0.704720</span></span><br><span class="line">validation acc: <span class="number">0.400000</span></span><br><span class="line">**************************************************</span><br></pre></td></tr></table></figure><h3 id="2-0-3、vscode内浏览模型结构（tensorboard-–logdir）"><a href="#2-0-3、vscode内浏览模型结构（tensorboard-–logdir）" class="headerlink" title="2.0.3、vscode内浏览模型结构（tensorboard –logdir）"></a>2.0.3、vscode内浏览模型结构（tensorboard –logdir）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.train.import_meta_graph(<span class="string">&quot;./model/.meta&quot;</span>)</span><br><span class="line">tf.summary.FileWriter(<span class="string">&quot;./summary&quot;</span>, sess.graph)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看命令：tensorboard –logdir&#x3D;.&#x2F;summary&#x2F;</p><p><img src="https://img-blog.csdnimg.cn/cea65dba17d241cdbbbb82102fbffc1e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-0-4、生成pb模型"><a href="#2-0-4、生成pb模型" class="headerlink" title="2.0.4、生成pb模型"></a>2.0.4、生成pb模型</h3><p>转化的时候会出现如下问题：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;dealmodel_pb.py&quot;</span>, line <span class="number">11</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> tensorflow.framework <span class="keyword">import</span> graph_util</span><br><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;tensorflow.framework&#x27;</span></span><br></pre></td></tr></table></figure><p>解决方法：在代码开头补充如下几行</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#转换成pb文件</span></span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line">physical_devices = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(physical_devices) &gt; <span class="number">0</span>, <span class="string">&quot;Not enough GPU hardware devices available&quot;</span></span><br><span class="line">tf.config.experimental.set_memory_growth(physical_devices[<span class="number">0</span>], <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p> 完整代码：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#转换成pb文件</span></span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line">physical_devices = tf.compat.v1.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">len</span>(physical_devices) &gt; <span class="number">0</span>, <span class="string">&quot;Not enough GPU hardware devices available&quot;</span></span><br><span class="line">tf.compat.v1.config.experimental.set_memory_growth(physical_devices[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">freeze_graph</span>(<span class="params">output_node_names</span>):</span><br><span class="line">    <span class="comment">#checkpoint = tf.train.get_checkpoint_state(model_folder)</span></span><br><span class="line">    <span class="comment">#input_checkpoint = checkpoint.model_checkpoint_path</span></span><br><span class="line"></span><br><span class="line">    saver = tf.compat.v1.train.import_meta_graph(<span class="string">&#x27;./model/.meta&#x27;</span>, clear_devices=<span class="literal">True</span>)</span><br><span class="line">    graph = tf.compat.v1.get_default_graph()</span><br><span class="line">    input_graph_def = graph.as_graph_def()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        saver.restore(sess, <span class="string">&#x27;./model/&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> input_graph_def.node:</span><br><span class="line">            <span class="keyword">if</span> node.op == <span class="string">&#x27;RefSwitch&#x27;</span>:</span><br><span class="line">                node.op = <span class="string">&#x27;Switch&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(node.<span class="built_in">input</span>)):</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;moving_&#x27;</span> <span class="keyword">in</span> node.<span class="built_in">input</span>[index]:</span><br><span class="line">                        node.<span class="built_in">input</span>[index] = node.<span class="built_in">input</span>[index] + <span class="string">&#x27;/read&#x27;</span></span><br><span class="line">            <span class="keyword">elif</span> node.op == <span class="string">&#x27;AssignSub&#x27;</span>:</span><br><span class="line">                node.op = <span class="string">&#x27;Sub&#x27;</span></span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;use_locking&#x27;</span> <span class="keyword">in</span> node.attr: <span class="keyword">del</span> node.attr[<span class="string">&#x27;use_locking&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        output_graph_def = graph_util.convert_variables_to_constants(</span><br><span class="line">            sess,</span><br><span class="line">            input_graph_def,</span><br><span class="line">            output_node_names</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">with</span> tf.io.gfile.GFile(<span class="string">&#x27;classify_gold.pb&#x27;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(output_graph_def.SerializeToString())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    freeze_graph([<span class="string">&#x27;output&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-0-5、查看pb模型节点名称【事实上，上述的可视化就可以查看了，这里只是打印出来方便查看】"><a href="#2-0-5、查看pb模型节点名称【事实上，上述的可视化就可以查看了，这里只是打印出来方便查看】" class="headerlink" title="2.0.5、查看pb模型节点名称【事实上，上述的可视化就可以查看了，这里只是打印出来方便查看】"></a>2.0.5、查看pb模型节点名称【事实上，上述的可视化就可以查看了，这里只是打印出来方便查看】</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_dir = &#x27;./&#x27;</span></span><br><span class="line">out_path = <span class="string">&#x27;./&#x27;</span></span><br><span class="line">model_name = <span class="string">&#x27;classify_gold.pb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_graph</span>():</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.FastGFile(os.path.join(out_path + model_name), <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        tf.import_graph_def(graph_def, name=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">create_graph()</span><br><span class="line">tensor_name_list = [tensor.name <span class="keyword">for</span> tensor <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line"><span class="keyword">for</span> tensor_name <span class="keyword">in</span> tensor_name_list:</span><br><span class="line">    <span class="built_in">print</span>(tensor_name)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成内容：很明显，从输入到输出的节点都已经包括在其内了</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Use tf.gfile.GFile.</span><br><span class="line"><span class="built_in">input</span>/x-<span class="built_in">input</span></span><br><span class="line">conv2d/kernel</span><br><span class="line">conv2d/kernel/read</span><br><span class="line">conv2d/bias</span><br><span class="line">conv2d/bias/read</span><br><span class="line">conv2d-pool-<span class="number">1</span>/conv2d/Conv2D</span><br><span class="line">conv2d-pool-<span class="number">1</span>/conv2d/BiasAdd</span><br><span class="line">conv2d-pool-<span class="number">1</span>/conv2d/Relu</span><br><span class="line">conv2d-pool-<span class="number">1</span>/max_pooling2d/MaxPool</span><br><span class="line">conv2d_1/kernel</span><br><span class="line">conv2d_1/kernel/read</span><br><span class="line">conv2d_1/bias</span><br><span class="line">conv2d_1/bias/read</span><br><span class="line">conv2d-pool-<span class="number">2</span>/conv2d/Conv2D</span><br><span class="line">conv2d-pool-<span class="number">2</span>/conv2d/BiasAdd</span><br><span class="line">conv2d-pool-<span class="number">2</span>/conv2d/Relu</span><br><span class="line">conv2d-pool-<span class="number">2</span>/max_pooling2d/MaxPool</span><br><span class="line">conv2d_2/kernel</span><br><span class="line">conv2d_2/kernel/read</span><br><span class="line">conv2d_2/bias</span><br><span class="line">conv2d_2/bias/read</span><br><span class="line">conv2d-pool-<span class="number">3</span>/conv2d/Conv2D</span><br><span class="line">conv2d-pool-<span class="number">3</span>/conv2d/BiasAdd</span><br><span class="line">conv2d-pool-<span class="number">3</span>/conv2d/Relu</span><br><span class="line">conv2d-pool-<span class="number">3</span>/max_pooling2d/MaxPool</span><br><span class="line">conv2d_3/kernel</span><br><span class="line">conv2d_3/kernel/read</span><br><span class="line">conv2d_3/bias</span><br><span class="line">conv2d_3/bias/read</span><br><span class="line">conv2d-pool-<span class="number">4</span>/conv2d/Conv2D</span><br><span class="line">conv2d-pool-<span class="number">4</span>/conv2d/BiasAdd</span><br><span class="line">conv2d-pool-<span class="number">4</span>/conv2d/Relu</span><br><span class="line">conv2d-pool-<span class="number">4</span>/max_pooling2d/MaxPool</span><br><span class="line">Reshape/shape</span><br><span class="line">Reshape</span><br><span class="line">dense/kernel</span><br><span class="line">dense/kernel/read</span><br><span class="line">dense/bias</span><br><span class="line">dense/bias/read</span><br><span class="line">layer-<span class="number">1</span>/dense/MatMul</span><br><span class="line">layer-<span class="number">1</span>/dense/BiasAdd</span><br><span class="line">layer-<span class="number">1</span>/dense/Relu</span><br><span class="line">dense_1/kernel</span><br><span class="line">dense_1/kernel/read</span><br><span class="line">dense_1/bias</span><br><span class="line">dense_1/bias/read</span><br><span class="line">layer-<span class="number">2</span>/dense/MatMul</span><br><span class="line">layer-<span class="number">2</span>/dense/BiasAdd</span><br><span class="line">layer-<span class="number">2</span>/dense/Relu</span><br><span class="line">dense_2/kernel</span><br><span class="line">dense_2/kernel/read</span><br><span class="line">dense_2/bias</span><br><span class="line">dense_2/bias/read</span><br><span class="line">layer-<span class="number">3</span>/dense/MatMul</span><br><span class="line">layer-<span class="number">3</span>/dense/BiasAdd</span><br><span class="line">dense_3/kernel</span><br><span class="line">dense_3/kernel/read</span><br><span class="line">dense_3/bias</span><br><span class="line">dense_3/bias/read</span><br><span class="line">layer-<span class="number">4</span>/dense/MatMul</span><br><span class="line">layer-<span class="number">4</span>/dense/BiasAdd</span><br><span class="line">output</span><br></pre></td></tr></table></figure><h2 id="2-1、使用OpenCV接口调用TF2-PB模型"><a href="#2-1、使用OpenCV接口调用TF2-PB模型" class="headerlink" title="2.1、使用OpenCV接口调用TF2-PB模型"></a>2.1、使用OpenCV接口调用TF2-PB模型</h2><p>此处加载网络并未用到pbtxt文件，网上也有将pb文件导出为pbtxt文件的代码，但当将pb导出为pbtxt时，无法也将pbtxt加载到网络中，具体原因暂不明确。不过此处只加载pb文件就能够使用了。</p><h3 id="2-1-0、配置OpenCV-C-环境"><a href="#2-1-0、配置OpenCV-C-环境" class="headerlink" title="2.1.0、配置OpenCV(C++)环境"></a>2.1.0、配置OpenCV(C++)环境</h3><p>AI模型C++部署:【配置OpenCV4++环境】与【三种在 C++ 中部署 TensorFlow 模型的方式】【准备阶段】：<a href="https://blog.csdn.net/weixin_41194129/article/details/123951358">https://blog.csdn.net/weixin_41194129&#x2F;article&#x2F;details&#x2F;123951358</a></p><h3 id="2-1-1、定义OpenCV-C-接口去调用TF2-PB模型"><a href="#2-1-1、定义OpenCV-C-接口去调用TF2-PB模型" class="headerlink" title="2.1.1、定义OpenCV(C++)接口去调用TF2-PB模型"></a>2.1.1、定义OpenCV(C++)接口去调用TF2-PB模型</h3><p>main.cc</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/dnn.hpp&gt;</span><span class="comment">//包含dnn模块的头文件</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span><span class="comment">//文件流进行txt文件读取</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv::dnn;<span class="comment">//包含dnn的命名空间</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//车辆分类，输入模型的地址</span></span><br><span class="line">string bin_model = <span class="string">&quot;./classify_gold.pb&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//加载模型</span></span><br><span class="line">Net net = <span class="built_in">readNetFromTensorflow</span>(bin_model);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取各层信息</span></span><br><span class="line">vector&lt;string&gt; layer_names = net.<span class="built_in">getLayerNames</span>();<span class="comment">//此时我们就可以获取所有层的名称了，有了这些可以将其ID取出</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; layer_names.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line"><span class="type">int</span> id = net.<span class="built_in">getLayerId</span>(layer_names[i]);<span class="comment">//通过name获取其id</span></span><br><span class="line"><span class="keyword">auto</span> layer = net.<span class="built_in">getLayer</span>(id);<span class="comment">//通过id获取layer</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;layer id:%d,type:%s,name:%s\n&quot;</span>, id, layer-&gt;type.<span class="built_in">c_str</span>(), layer-&gt;name.<span class="built_in">c_str</span>());<span class="comment">//将每一层的id，类型，姓名打印出来（可以明白此网络有哪些结构信息了）</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">waitKey</span>(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// g++ --std=c++11 `pkg-config opencv --cflags` main.cc  -o demo `pkg-config opencv --libs` &amp;&amp; ./demo   </span></span><br></pre></td></tr></table></figure><p>执行命令：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g++ --std=c++<span class="number">11</span> `pkg-config opencv --cflags` main.cc  -o demo `pkg-config opencv --libs` &amp;&amp; ./demo </span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">g++ --std=c++<span class="number">11</span> `pkg-config opencv --cflags` main.cc  -o demo `pkg-config opencv --libs` &amp;&amp; ./demo </span><br><span class="line">[ INFO:<span class="number">0</span>@<span class="number">0.327</span>] global /data/Documents/pcl2022/opencv_cc_tensorflow/opencv<span class="number">-4.2</span><span class="number">.0</span>/opencv/modules/dnn/src/tensorflow/tf_importer.<span class="built_in">cpp</span> (<span class="number">2991</span>) populateNet DNN/TF: parsing model produced by TF <span class="built_in">v0</span> (min_consumer=<span class="number">0</span>). Number of nodes = <span class="number">62</span></span><br><span class="line">layer id:<span class="number">1</span>,type:Convolution,name:conv2d-pool<span class="number">-1</span>/conv2d/Conv2D</span><br><span class="line">layer id:<span class="number">2</span>,type:ReLU,name:conv2d-pool<span class="number">-1</span>/conv2d/Relu</span><br><span class="line">layer id:<span class="number">3</span>,type:Pooling,name:conv2d-pool<span class="number">-1</span>/max_pooling2d/MaxPool</span><br><span class="line">layer id:<span class="number">4</span>,type:Convolution,name:conv2d-pool<span class="number">-2</span>/conv2d/Conv2D</span><br><span class="line">layer id:<span class="number">5</span>,type:ReLU,name:conv2d-pool<span class="number">-2</span>/conv2d/Relu</span><br><span class="line">layer id:<span class="number">6</span>,type:Pooling,name:conv2d-pool<span class="number">-2</span>/max_pooling2d/MaxPool</span><br><span class="line">layer id:<span class="number">7</span>,type:Convolution,name:conv2d-pool<span class="number">-3</span>/conv2d/Conv2D</span><br><span class="line">layer id:<span class="number">8</span>,type:ReLU,name:conv2d-pool<span class="number">-3</span>/conv2d/Relu</span><br><span class="line">layer id:<span class="number">9</span>,type:Pooling,name:conv2d-pool<span class="number">-3</span>/max_pooling2d/MaxPool</span><br><span class="line">layer id:<span class="number">10</span>,type:Convolution,name:conv2d-pool<span class="number">-4</span>/conv2d/Conv2D</span><br><span class="line">layer id:<span class="number">11</span>,type:ReLU,name:conv2d-pool<span class="number">-4</span>/conv2d/Relu</span><br><span class="line">layer id:<span class="number">12</span>,type:Pooling,name:conv2d-pool<span class="number">-4</span>/max_pooling2d/MaxPool</span><br><span class="line">layer id:<span class="number">13</span>,type:Permute,name:Reshape/nhwc</span><br><span class="line">layer id:<span class="number">14</span>,type:Reshape,name:Reshape</span><br><span class="line">layer id:<span class="number">15</span>,type:InnerProduct,name:layer<span class="number">-1</span>/dense/MatMul</span><br><span class="line">layer id:<span class="number">16</span>,type:ReLU,name:layer<span class="number">-1</span>/dense/Relu</span><br><span class="line">layer id:<span class="number">17</span>,type:InnerProduct,name:layer<span class="number">-2</span>/dense/MatMul</span><br><span class="line">layer id:<span class="number">18</span>,type:ReLU,name:layer<span class="number">-2</span>/dense/Relu</span><br><span class="line">layer id:<span class="number">19</span>,type:InnerProduct,name:layer<span class="number">-3</span>/dense/MatMul</span><br><span class="line">layer id:<span class="number">20</span>,type:InnerProduct,name:layer<span class="number">-4</span>/dense/MatMul</span><br><span class="line">layer id:<span class="number">21</span>,type:Softmax,name:output</span><br><span class="line">[ INFO:<span class="number">0</span>@<span class="number">0.422</span>] global /data/Documents/pcl2022/opencv_cc_tensorflow/opencv<span class="number">-4.2</span><span class="number">.0</span>/opencv/modules/highgui/src/registry.impl.<span class="built_in">hpp</span> (<span class="number">114</span>) UIBackendRegistry UI: Enabled <span class="built_in">backends</span>(<span class="number">2</span>, sorted by priority): <span class="built_in">GTK</span>(<span class="number">1000</span>); <span class="built_in">GTK2</span>(<span class="number">990</span>) + <span class="built_in">BUILTIN</span>(GTK2)</span><br><span class="line">[ INFO:<span class="number">0</span>@<span class="number">0.422</span>] global /data/Documents/pcl2022/opencv_cc_tensorflow/opencv<span class="number">-4.2</span><span class="number">.0</span>/opencv/modules/highgui/src/backend.<span class="built_in">cpp</span> (<span class="number">90</span>) createUIBackend UI: <span class="keyword">using</span> backend: <span class="built_in">GTK</span> (priority=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><h3 id="2-1-2、OpenCV-C-调用TF2-PB模型并输入一张图片进行分类预测结果"><a href="#2-1-2、OpenCV-C-调用TF2-PB模型并输入一张图片进行分类预测结果" class="headerlink" title="2.1.2、OpenCV(C++)调用TF2-PB模型并输入一张图片进行分类预测结果"></a>2.1.2、OpenCV(C++)调用TF2-PB模型并输入一张图片进行分类预测结果</h3><p>测试图片也要保证尺寸一样100*100<br><img src="https://img-blog.csdnimg.cn/5bdd48a1c5e848b39ce6fe70fe403f22.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="OPENCV-C-推理代码"><a href="#OPENCV-C-推理代码" class="headerlink" title="OPENCV+C++ 推理代码"></a>OPENCV+C++ 推理代码</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/dnn.hpp&gt;</span><span class="comment">//包含dnn模块的头文件</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span><span class="comment">//文件流进行txt文件读取</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv::dnn;<span class="comment">//包含dnn的命名空间</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义名称，用于后面的显示操作</span></span><br><span class="line">String objNames[] = &#123; <span class="string">&quot;glod&quot;</span>,<span class="string">&quot;monkey_king&quot;</span>&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//模型的pb文件</span></span><br><span class="line">string bin_model = <span class="string">&quot;/data/Documents/pcl2022/opencv_cc_tensorflow/Neural_network/TFCNN/TFV2_MOLDEL/classify_gold.pb&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load DNN model</span></span><br><span class="line">Net net = <span class="built_in">readNetFromTensorflow</span>(bin_model);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取各层信息</span></span><br><span class="line"><span class="comment">// vector&lt;string&gt; layer_names = net.getLayerNames();//此时我们就可以获取所有层的名称了，有了这些可以将其ID取出</span></span><br><span class="line"><span class="comment">// for (int i = 0; i &lt; layer_names.size(); i++) &#123;</span></span><br><span class="line"><span class="comment">// int id = net.getLayerId(layer_names[i]);//通过name获取其id</span></span><br><span class="line"><span class="comment">// auto layer = net.getLayer(id);//通过id获取layer</span></span><br><span class="line"><span class="comment">// printf(&quot;layer id:%d,type:%s,name:%s\n&quot;, id, layer-&gt;type.c_str(), layer-&gt;name.c_str());//将每一层的id，类型，姓名打印出来（可以明白此网络有哪些结构信息了）</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">Mat src = <span class="built_in">imread</span>(<span class="string">&quot;/data/Documents/pcl2022/opencv_cc_tensorflow/Neural_network/TFCNN/dataset2/test/1-3.jpg&quot;</span>);</span><br><span class="line"><span class="comment">//2-20,2-21,2-11,2-1</span></span><br><span class="line"><span class="keyword">if</span> (src.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;could not load image..&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">getchar</span>();</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">imshow</span>(<span class="string">&quot;src&quot;</span>, src);</span><br><span class="line"></span><br><span class="line"><span class="comment">//构建输入(根据建立的网络模型时的输入)</span></span><br><span class="line">Mat inputBlob = <span class="built_in">blobFromImage</span>(src, <span class="number">1.0</span>, <span class="built_in">Size</span>(<span class="number">100</span>, <span class="number">100</span>), <span class="built_in">Scalar</span>(), <span class="literal">true</span>, <span class="literal">false</span>);<span class="comment">//我们要将图像resize成100*100的才是我们神经网络可以接受的宽高</span></span><br><span class="line"><span class="comment">//参数1：输入图像，参数2：默认1.0表示0-255范围的，参数3：设置输出的大小，参数4：均值对所有数据中心化预处理,参数5：是否进行通道转换(需要),参数6：，参数7：默认深度为浮点型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//上方得到的inputBlob是4维的（在变量窗口看dim），所以在imagewatch中无法查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置输入</span></span><br><span class="line"><span class="comment">//现在要将其输入到创建的网络中</span></span><br><span class="line">net.<span class="built_in">setInput</span>(inputBlob);</span><br><span class="line"></span><br><span class="line"><span class="comment">//进行推断得到输出</span></span><br><span class="line"><span class="comment">//让网络执行得到output,调用forward可以得到一个结果</span></span><br><span class="line"><span class="comment">//此处不给参数，得到的是最后一层的结果，也可以输入层数得到任何一层的输出结果</span></span><br><span class="line">Mat probMat = net.forward();<span class="comment">//通过前面的输出层看最后一层，可以知道输出7个分类</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//对数据进行序列化（变成1行n列的，可以在后面进行方便的知道是哪个index了）</span></span><br><span class="line">Mat prob = probMat.<span class="built_in">reshape</span>(<span class="number">1</span>, <span class="number">1</span>);<span class="comment">//reshape函数可以进行序列化，（输出为1通道1行的数据，参数1:1个通道，参数2:1行）将输出结果变成1行n列的，但前面probMat本身就是7*1*1</span></span><br><span class="line"><span class="comment">////实际结果probMat和prob相同</span></span><br><span class="line"><span class="comment">////当其他网络probMat需要序列化的时候，reshape就可以了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//此时找到最大的那个</span></span><br><span class="line">Point classNum;</span><br><span class="line"><span class="type">double</span> classProb;</span><br><span class="line"><span class="built_in">minMaxLoc</span>(prob, <span class="literal">NULL</span>, &amp;classProb, <span class="literal">NULL</span>, &amp;classNum);<span class="comment">//此时只获取最大值及最大值位置，最小值不管他</span></span><br><span class="line"><span class="type">int</span> index = classNum.x;<span class="comment">//此时得到的是最大值的列坐标。就是其类的索引值，就可以知道其类名了</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;\n current index=%d,possible:%2f,name=%s\n&quot;</span>, index, classProb, objNames[index].<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">//此时可以将名称打印到图片上去</span></span><br><span class="line"><span class="built_in">putText</span>(src, objNames[index].<span class="built_in">c_str</span>(), <span class="built_in">Point</span>(<span class="number">50</span>, <span class="number">50</span>), FONT_HERSHEY_SIMPLEX, <span class="number">0.75</span>, <span class="built_in">Scalar</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>, <span class="number">8</span>);</span><br><span class="line"><span class="built_in">imshow</span>(<span class="string">&quot;result&quot;</span>, src);</span><br><span class="line"></span><br><span class="line"><span class="built_in">waitKey</span>(<span class="number">50000</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// g++ --std=c++11 `pkg-config opencv --cflags` inference_cv.cc  -o result `pkg-config opencv --libs` &amp;&amp; ./result   </span></span><br></pre></td></tr></table></figure><h4 id="推理结果输出"><a href="#推理结果输出" class="headerlink" title="推理结果输出"></a>推理结果输出</h4><p>编译命令：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">g++ --std=c++<span class="number">11</span> `pkg-config opencv --cflags` inference_cv.cc  -o result `pkg-config opencv --libs` &amp;&amp; ./result   </span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/412fecffcfd8494fbae4d75d70232654.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rqQ5Luj56CB5p2A5omL,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/8ac15d83c80843e0a092990f2b88af5e.png" alt="在这里插入图片描述"><br>输出概率：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">current index=<span class="number">0</span>,possible:<span class="number">0.999974</span>,name=glod</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/22efb780a6f9491694438996045caaa4.png" alt="在这里插入图片描述"><br>输出概率：</p><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">current index=<span class="number">1</span>,possible:<span class="number">0.617289</span>,name=monkey_king</span><br></pre></td></tr></table></figure><h1 id="3、tensorflow1-x模型生成"><a href="#3、tensorflow1-x模型生成" class="headerlink" title="3、tensorflow1.x模型生成"></a>3、tensorflow1.x模型生成</h1><h2 id="tensorflow1-x训练生成的模型OPENCV-C-调用方法同上【代码已经开源】地址如下"><a href="#tensorflow1-x训练生成的模型OPENCV-C-调用方法同上【代码已经开源】地址如下" class="headerlink" title="tensorflow1.x训练生成的模型OPENCV(C++)调用方法同上【代码已经开源】地址如下"></a>tensorflow1.x训练生成的模型OPENCV(C++)调用方法同上【代码已经开源】地址如下</h2><p><a href="https://github.com/KangChou/gold_monkey_king">https://github.com/KangChou/gold_monkey_king</a></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://blog.csdn.net/qq_44870829/article/details/108592530?spm=1001.2014.3001.5506">https://blog.csdn.net/qq_44870829/article/details/108592530?spm=1001.2014.3001.5506</a><br><a href="https://blog.csdn.net/weixin_39928773/article/details/103910850?utm_medium=distribute.pc_relevant.none-task-blog-blogcommendfrommachinelearnpai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-blogcommendfrommachinelearnpai2-2.channel_param">https://blog.csdn.net/weixin_39928773&#x2F;article&#x2F;details&#x2F;103910850m</a></p>]]></content>
      
      
      <categories>
          
          <category> 图像分类模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源书籍与源码</title>
      <link href="/2021/04/05/books/"/>
      <url>/2021/04/05/books/</url>
      
        <content type="html"><![CDATA[<p>书籍下载链接：<a href="https://github.com/KangChou/AI-Technology-and-Algorithm-Programming">https://github.com/KangChou/AI-Technology-and-Algorithm-Programming</a></p><span id="more"></span><p><img src="https://s1.ax1x.com/2020/07/28/aVGUit.png" alt="image-20200728231437641"></p><p>数学分析上下册</p><p><img src="https://s1.ax1x.com/2020/07/28/aVGYdA.png"></p><p>python自动化操作</p><p><img src="https://s1.ax1x.com/2020/07/28/aVG3se.png"></p><p>python从入门让到实践</p><p><img src="https://s1.ax1x.com/2020/07/28/aVGdRf.png"></p><p>数字信号处理</p><p><img src="https://s1.ax1x.com/2020/07/28/aVG8qH.png"></p><p>《智能问答与深度学习》 <a href="https://github.com/l11x0m7/book-of-qna-code">https://github.com/l11x0m7/book-of-qna-code</a></p><p>人工智能实践：Tensorflow笔记 <br><a href="https://github.com/jlff/tf2_notes">https://github.com/jlff/tf2_notes</a> <br>源码下载链接：<a href="https://pan.baidu.com/s/19XC28Hz_TwnSQeuVifg1UQ">https://pan.baidu.com/s/19XC28Hz_TwnSQeuVifg1UQ</a> <br>提取码：mocm</p><p>数据科学&#x2F;人工智能比赛解决方案聚合:<a href="https://github.com/apachecn/awesome-data-comp-solution">https://github.com/apachecn/awesome-data-comp-solution</a></p><p>《学习学习与计算机视觉》配套代码: <a href="https://github.com/frombeijingwithlove/dlcv_for_beginners">https://github.com/frombeijingwithlove/dlcv_for_beginners</a></p><p>《算法导论》的C++实现”代码：<a href="https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms">https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms</a> <br><a href="http://www.huaxiaozhuan.com/">http://www.huaxiaozhuan.com/</a></p><p>《Unix 环境高级编程第三版》笔记：<a href="https://github.com/huaxz1986/APUE_notes">https://github.com/huaxz1986/APUE_notes</a></p><p>算法工程师(人工智能cv方向)面试问题及相关资料的网站收集:<a href="https://github.com/lcylmhlcy/Awesome-algorithm-interview">https://github.com/lcylmhlcy/Awesome-algorithm-interview</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 核心算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP技术汇总</title>
      <link href="/2021/04/05/nlp/"/>
      <url>/2021/04/05/nlp/</url>
      
        <content type="html"><![CDATA[<p>Deep learning speech learning library <br></p><span id="more"></span><p>一个轻量级、简单易用的 RNN 唤醒词监听器: <a href="https://github.com/MycroftAI/mycroft-precise">https://github.com/MycroftAI/mycroft-precise</a></p><p>zh:<a href="http://fancyerii.github.io/books/mycroft-precise/">http://fancyerii.github.io/books/mycroft-precise/</a></p><p>基于树莓派的人工智能小车，实现识别、提示、智能旅游线路、离线图像:<br><a href="https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi">https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi</a></p><p>中文NLP数据集:<a href="https://github.com/CLUEbenchmark/CLUEDatasetSearch">https://github.com/CLUEbenchmark/CLUEDatasetSearch</a> </p><p>模型：<a href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a> </p><p>中文 NLP 资源精选列表 中文自然语言处理相关资料:<br><a href="https://github.com/crownpku/Awesome-Chinese-NLP">https://github.com/crownpku/Awesome-Chinese-NLP</a></p><p>视觉聊天机器人:<a href="https://paperswithcode.com/paper/visual-dialog">https://paperswithcode.com/paper/visual-dialog</a></p><p>Bert&#x2F;Transformer模型压缩与优化加速: <a href="https://blog.csdn.net/nature553863/article/details/120292394%EF%BC%9A">https://blog.csdn.net/nature553863/article/details/120292394：</a></p><p>可以压缩 BERT 的所有方式：<a href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html">http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html</a><br><a href="https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html">https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html</a></p><p>BERT轻量化探索—模型剪枝（BERT Pruning）—Rasa维度剪枝:<a href="https://blog.csdn.net/ai_1046067944/article/details/103609152">https://blog.csdn.net/ai_1046067944/article/details/103609152</a> </p><p>压缩 BERT 以加快预测速度:<a href="https://rasa.com/blog/compressing-bert-for-faster-prediction-2/">https://rasa.com/blog/compressing-bert-for-faster-prediction-2/</a></p><p>论文综述与BERT相关最新论文:<a href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></p><p>中文自然语言排行榜及论文查询:<a href="https://www.cluebenchmarks.com/index.html">https://www.cluebenchmarks.com/index.html</a></p><p>计算语言学国际会议论文集:<a href="https://aclanthology.org/volumes/2020.coling-main/">https://aclanthology.org/volumes/2020.coling-main/</a></p><p>计算语言学协会第 58 届年会论文集:<a href="https://aclanthology.org/volumes/2020.acl-main/">https://aclanthology.org/volumes/2020.acl-main/</a></p><p>计算语言学2协会2021年会论文搜集：<a href="https://aclanthology.org/events/acl-2021/">https://aclanthology.org/events/acl-2021/</a></p><p>中文BERT全词掩蔽预训练（中文BERT-wwm系列模型）<a href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>一个大规模的中文跨领域面向任务的对话数据集:<a href="https://github.com/thu-coai/CrossWOZ">https://github.com/thu-coai/CrossWOZ</a></p><p>关于ConvLab-2：用于构建、评估和诊断对话系统的开源工具包（支持中文）：<a href="https://github.com/thu-coai/ConvLab-2">https://github.com/thu-coai/ConvLab-2</a></p><p>视觉和语言预训练模型 (VL-PTM) 的最新进展(语音视觉融合):<a href="https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers">https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers</a></p><p>深度学习和自然语言处理阅读清单:<a href="https://github.com/IsaacChanghau/DL-NLP-Readings">https://github.com/IsaacChanghau/DL-NLP-Readings</a></p><p>视觉问答 (VQA)（图像&#x2F;视频问答）、视觉问题生成、视觉对话、视觉常识推理和相关领域的精选列表：<a href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 核心算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>值得你阅读的Hexo个人博客搭建：不用购买服务器，不用购买域名，不要钱，不用敲代码等等，是的，你没有看错，快来转载学习吧！</title>
      <link href="/2020/04/05/blog/"/>
      <url>/2020/04/05/blog/</url>
      
        <content type="html"><![CDATA[<p>本文的原文在我的微信公众号，欢迎点击下面蓝色字体链接进入主页</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwNzUwOTY1Nw==&mid=2247484965&idx=1&sn=757e038fdc0eb968008ff73752b2456c&chksm=971007d3a0678ec5ae9fa1c1c871ff8cf15b67656f227327e3b507654a7cac28002f867ddb01&mpshare=1&scene=1&srcid=&sharer_sharetime=1571762807076&sharer_shareid=d33f5fb4b35f5c6867199f25b3752a3a&key=b97804376b1264af0c3ca188cf96e5b8d3b5990c40809c5c043e6da161476ef8c6e3cef3a9329dcc6d4eec6f605d83d617cd92d6f6883c7b9b632ca8aaf81c0f84c846463da87113881750250a56879b&ascene=1&uin=MjE2NzAwOTgyNA==&devicetype=Windows+7&version=62060844&lang=zh_CN&pass_ticket=fLo4Au8Vos0w/s3CVdzWInlkCBCduYag+mLa3dny9lkniGqUIl0wnHtlfaWbwpJJ">值得你阅读的Hexo个人博客搭建</a></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS01ZDJjYWQyNDY1NmQ2ZWM2?x-oss-process=image/format,png" alt="image"></p><p><strong>Hexo快速搭建个人博客（2019&#x2F;10&#x2F;22更新）</strong></p><hr><p><strong>使用到的工具</strong>  <strong>（本教程统一在Windows系统下搭建）</strong></p><p><strong>Node.js、Hexo、Git、Github账号、Sublime Text3</strong></p><p><strong>请自行注册一个Github账号</strong></p><p><strong>最后的部署到网上的博客展示</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0wNDNhNWFjNzIwZWNmNTBl?x-oss-process=image/format,png" alt="image"></p><p><strong>文章目录：</strong></p><p><em><strong>前言</strong></em></p><p><em><strong>值得你知道的话</strong></em></p><p><em><strong>一、从创建到部署博客</strong></em></p><p><em><strong><strong>二、博客的网页主题</strong></strong></em></p><p><em><strong><strong><strong>三、更换域名+博客测试成功</strong></strong></strong></em></p><span id="more"></span><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天一直在钻研这个博客，并搜索了大量有关hexo搭建博客的教程进行学习。我作为一个第一次使用Hexo搭建个人博客的菜鸟，<strong>我发现我踩了不少坑</strong>，哈哈，在这里我不得不吐槽一下某些撰写Hexo搭建个人博客的技术人员，用一个字来形容他们的博客就是<strong>“乱”</strong>，乱是因为我读完他们的博客写的内容发现逻辑顺序简直看得我<strong>一头雾水</strong>、细节内容对于他们来说就是一个摆设，难怪有好多人看不懂也是应该的。当然，可能是我的水平不够，也或许是在拜读他们的大作时候不够认真和严谨。</p><p>但我还是要告诫一下一些技术编辑者：</p><p><strong>如果是怕别人偷学你的内容，那就不要发在网上；<strong><strong>如果你发在网上，请考虑我们读者的感受，要对自己花费那么多时间撰写的内容负责，要</strong></strong>让别人看懂你的文章，让别人欣赏你的作品</strong>。其实，有时候还能看出一个人的品性。</p><p>吐槽到此结束~<strong>下面开始进入博客搭建环节</strong></p><hr><p><strong>值得你知道到话：</strong></p><p><strong>是的，你没有看错！</strong></p><p><strong>不用服务器，不用注册域名，不用花钱，不用敲大代码等等</strong></p><p><strong>一个博客就值得你拥有</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1lOTg2YzEzZTc4ODRiYTVl?x-oss-process=image/format,png" alt="image"></p><hr><h1 id="一、从创建到部署博客"><a href="#一、从创建到部署博客" class="headerlink" title="一、从创建到部署博客**"></a>一、从创建到部署博客**</h1><p><strong>1、安装好Node.js</strong></p><p><strong>别忘了用命令npm检验Node.js安装是否完成，</strong></p><p><strong>关于hexo的安装教程比较简单，</strong>网上有很多完整的教程，在这里就不再赘述。****</p><p><strong>安装Hexo 命令：</strong></p><p><strong>npm install -g hexo-cli</strong><br>补充:安装hexo-helper-live2d 看门动画插件 npm install –save hexo-helper-live2d \</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一定要在你博客目录的指定路径下（E:\hexo\KangChou）执行，否则在node_modules之中安装不了</span><br><span class="line">npm uninstall hexo-helper-live2d  </span><br><span class="line">npm install --save hexo-helper-live2d</span><br><span class="line">npm install live2d-widget-model-hibiki </span><br><span class="line">npm install npm install --save live2d-widget-model-xxx来安装你喜欢的模型</span><br><span class="line">参考：https://zhuanlan.zhihu.com/p/349278862</span><br></pre></td></tr></table></figure><p>参考文献:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://blog.csdn.net/qq_36239569/article/details/104104894</span><br><span class="line">https://zhuanlan.zhihu.com/p/350654582 </span><br><span class="line">解决办法：</span><br><span class="line">首先</span><br><span class="line">npm config set proxy null 代理置为空</span><br><span class="line"></span><br><span class="line">运行npm cache clean --force清理缓存</span><br><span class="line"></span><br><span class="line">然后尝试执行</span><br><span class="line">npm config set registry http://registry.npmjs.org/</span><br><span class="line">如果嫌安装依赖慢的话 可以使用国内淘宝镜像</span><br><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><p>国内镜像下载就是快：<br><img src="https://user-images.githubusercontent.com/36963108/163660165-9fbbd2f0-fbdd-40a5-ac7c-34a5cac56a45.png" alt="image"></p><p>做完了这一步之后，恭喜你，前期的准备工作已经完成，环境这一步结束了。</p><p><strong>2、安装好Git</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1hNGU4NjkxNGQ3ZmI5ZTkx?x-oss-process=image/format,png" alt="image"></p><p><strong>3、在C盘下创建hexo文件夹</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00MzA1ZmEyMzVlMjI4NjNj?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>4、打开Hexo文件夹下，右键点击Git bash 下执行命令</strong></p><p>工程文件目录：</p><p><img src="https://user-images.githubusercontent.com/36963108/163580819-8490257b-00cd-425d-9b9f-f2489aa1d3ae.png" alt="image"></p><p><strong>再使用一次这个命令：npm install hexo-cli -g在终端使用npm安装hexo</strong></p><p><strong>创建博客KangChou：hexo init KangChou</strong></p><p><strong>cd KangChou</strong></p><p><strong>npm install</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS04NjYyNmNhODY3NzQ4NTFm?x-oss-process=image/format,png" alt="image"></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1jYTFkZDI2NDNiMTc2M2Rm?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>5、命令hexo server启动github服务器</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0xZTk0ZDI2ODFlZGNiNGM3?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>6、浏览博客</strong></p><p>安照5中提示的网址<strong><a href="http://localhost:4000/">http://localhost:4000/</a></strong></p><p>复制该网址在浏览器中打开，如下图所示：这样一个博客的架子就出来了。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00MTNhNWJiNmYzYTAyNmQ1?x-oss-process=image/format,png" alt="image"></p><iframe js_editor_cpcad class="js_cpc_area res_iframe cpc_iframe" src="https://mp.weixin.qq.com/cgi-bin/readtemplate?t=tmpl/cpc_tmpl#1571758997949" data-category_id_list="1|11|16|17|22|24|26|27|28|29|3|31|32|35|36|37|39|41|42|43|45|46|47|48|49|5|50|51|52|53|54|55|6|7|8" data-id="1571758997949" style="margin: 14px 0px; padding: 0px; border: 0px; width: 636.997px; height: 378px; z-index: 9999; position: relative; display: block;"></iframe><hr><p><strong>7、部署前哨（一）：添加Github仓库地址</strong></p><p>在部署之前，我们需要先知道博客的部署地址，它需要对应 GitHub 的一个 Repository 的地址，这个信息需要我们来配置一下。(这里我就省略了，自己去布置)，这是我的这个Github仓库</p><p><strong><a href="https://github.com/KangChou/KangChou.github.io.git">https://github.com/KangChou/KangChou.github.io.git</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0zN2EyODQ2NzBiOTIyOWU5?x-oss-process=image/format,png" alt="image"></p><p>打开文件Hexo下的KangChou文件根目录下的 _config.yml 文件，我使用编辑器Sublime Text3打开的（或者你使用其他代码编辑器打开，<strong>千万别用文本编辑器打开</strong>），找到 Deployment 这个地方(提示：文件最后)，把刚才新建的 Repository 的地址贴过来</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1mMTNiODgyYjdjYWEyYWEx?x-oss-process=image/format,png" alt="image"></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1lZDVlMDdlOTRhZjE3NGQ2?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>8、部署前哨（二）：****部署插件</strong></p><p>需要安装一个支持 Git 的部署插件： <strong>hexo-deployer-git</strong>，有了它我们才可以顺利将其部署到 GitHub 上面(如果不安装的话，在执行部署命令时会报错).</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kMzJhYmJmZWZlYjg5OTNj?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>9、下面开始部署到Github</strong></p><p>如果8的插件部署没有问题就开始进行部署，首先输入部署命令如下：<strong>hexo deploy</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1hNzUyMjBjZTQ3YWI3ZmJi?x-oss-process=image/format,png" alt="image"></p><p>结尾….</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00OTNlNmVhYzIzOGYzMDE1?x-oss-process=image/format,png" alt="image"></p><p>可以发现出现了上面的报错提示：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Error: Spawn failed</span><br></pre></td></tr></table></figure><p><strong>解决方法第一次：</strong></p><p>经搜索大量资料发现了下面的这个博客出现类似上述的一样问题，看了这个解决办法，我就斗胆试一试：</p><p><strong><a href="https://blog.csdn.net/njc_sec/article/details/89021083">https://blog.csdn.net/njc_sec/article/details/89021083</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1hYjhlZDY5ZTc0N2UzMDYz?x-oss-process=image/format,png" alt="image"></p><p>可惜我再重新安装上述的三个命令输入之后还是出先一样的错误。</p><p><strong>解决方法第二次：</strong></p><p>因此我怀疑可能是仓库的地址出错，因此去看看了看地址</p><p>这是原来的地址：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br></pre></td></tr></table></figure><p>我按照出现错误提示中的网址去打开它：</p><p> <strong><a href="https://hexo.io/docs/troubleshooting.html">https://hexo.io/docs/troubleshooting.html</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS04YzUxZDQ3Y2M3ZWYyNzcx?x-oss-process=image/format,png" alt="image"></p><p>并找到了部署到Github目前的语法规定的网页下</p><p><strong><a href="https://hexo.io/docs/github-pages">https://hexo.io/docs/github-pages</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kNmIxMjllMTQxYTgxNDVk?x-oss-process=image/format,png" alt="image"></p><p>按照上面对部署仓库的地址，我将上述的Deploy的源码修改为</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br></pre></td></tr></table></figure><p>于是我再试了上述的三个命令：</p><p><strong>hexo clean</strong></p><p><strong>hexo g</strong></p><p><strong>hexo d</strong></p><p>最终出现下面的结果:说明出现的问题解决了</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0xZjcwNTdkYTk5NGZjNzdh?x-oss-process=image/format,png" alt="image"></p><p>由于我起初没有部署仓库的密钥，所以要去仓库部署。</p><hr><p><strong>10、创建的 ssh 密钥的密码</strong></p><p>(1)、我打开了我得仓库，并找到了设置</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1iOGJmOTlmMzQxNjViYzVh?x-oss-process=image/format,png" alt="image"></p><p>（2）查看部署密钥指南以了解更多信息</p><p><strong><a href="https://developer.github.com/v3/guides/managing-deploy-keys/#deploy-keys">https://developer.github.com/v3/guides/managing-deploy-keys/#deploy-keys</a></strong></p><p><strong>找到了设置密码的步骤（经过翻译以后，目前我们进行到下面的5）</strong> </p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS04ZTdiODQxZmNmMDQyMTVk?x-oss-process=image/format,png" alt="image"></p><p>（3）怎么创建SSH密钥?</p><p>步骤：</p><ul><li><p>找到本地环境：C:\Users\admin.ssh   这个路径下的用户&#x2F;名称&#x2F;.ssh</p></li><li><p>在这路径下，打开gitbub的命令控制台</p></li><li><p>(I): git  init  &#x2F;&#x2F;初始化一下，看看路径对不对</p><p>   (II):ssh-keygen -t rsa -C “你的邮箱”</p></li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1iMjgxYmNlMzI1ZDc0NWNm?x-oss-process=image/format,png" alt="image"></p><ul><li>到本地环境.ssh路径下查看，是否生成id_rsa,id_rsa.pub这个两个文件</li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0xMzE1MDViZmY5MzlkMGMz?x-oss-process=image/format,png" alt="image"></p><ul><li>生成后，现在把id_rsa.pub里面的内容复制到githubd 的add github key 的key里面（也就是刚刚仓库的密钥添加的地方）</li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0zYzgxZjM1Njc4OWQ3OWQ1?x-oss-process=image/format,png" alt="image"></p><p><strong>点击Add SSH key获得下面结果</strong></p><p><strong>注意：第一次提交，配置密钥，需要输入github的密码，如下就是添加秘钥成功</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS04OWUyMDMzYmU5M2VkZjMy?x-oss-process=image/format,png" alt="image"></p><ul><li><p>密钥配置成功后，要验证一下是否配置成功</p><p>命令：ssh -T <a href="mailto:&#103;&#105;&#116;&#x40;&#103;&#x69;&#x74;&#104;&#117;&#x62;&#x2e;&#99;&#x6f;&#x6d;">&#103;&#105;&#116;&#x40;&#103;&#x69;&#x74;&#104;&#117;&#x62;&#x2e;&#99;&#x6f;&#x6d;</a></p></li></ul><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1jYzZhMzM1ZjIxOGIzMzIw?x-oss-process=image/format,png" alt="image"></p><p>出现下面提示，说明配置成功。</p><p>Hi KangChou&#x2F;KangChou.github.io! You’ve successfully authenticated, but GitHub does not provide shell access</p><hr><p><strong>11、再次使用密钥部署</strong></p><p>仍然使用命令：</p><p><strong>hexo clean</strong></p><p><strong>hexo g</strong></p><p><strong>hexo d</strong></p><p>如果都没有问题就会出现下面的结果，输入你刚刚设置的<strong>名</strong>和<strong>SSH的密码</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kZWY1M2FjNzQ3OGE5NWE0?x-oss-process=image/format,png" alt="image"></p><p>但是这里又出错了</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kMWJjMTc2ZWM0MmM2Y2Yw?x-oss-process=image/format,png" alt="image"></p><p>追根溯源，我怀疑还是部署的仓库出现了问题：因此我再次打开，并做了修改</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS03NGQ5NWMyMGUyODRlMjNi?x-oss-process=image/format,png" alt="image"></p><p>命令hexo d执行又出错</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kOTJhODBmYjdjMGRhZWRl?x-oss-process=image/format,png" alt="image"></p><p>下面终于找到了答案。这里要特别感谢**@李典金 @崔庆才<strong>两位网络开发大佬的</strong>细节**点拨才通过了上面的一个小环节，从而我力挽狂澜，一气呵成，搭建了后面的所有框架。</p><p>备注：ssh-keygen -t rsa -C “<a href="mailto:&#x6b;&#x61;&#x6e;&#103;&#99;&#x68;&#111;&#x75;&#54;&#54;&#x36;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;">&#x6b;&#x61;&#x6e;&#103;&#99;&#x68;&#111;&#x75;&#54;&#54;&#x36;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;</a>“不用输入密码，回车就可以生成。如果需要严密一点，可以进行加密功能的部署。</p><p><img src="https://user-images.githubusercontent.com/36963108/163664932-10ba6060-bfd4-4538-8cbf-200ad8d34472.png" alt="image"></p><hr><p><strong>12、终于部署成功</strong></p><p><strong>最后终于找到了错误的原因，这是因为我创建的仓库下Github的SSH错误</strong></p><p>因此，我去仓库找到了</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1mM2VlNjk2YzRjYjg3Y2Qx?x-oss-process=image/format,png" alt="image"></p><p>将红色的部分复制到hexo文件目录下也就是你的博客文件末尾，打开修改如下</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00NDY0ZjhmNmE5M2Y0ODA5?x-oss-process=image/format,png" alt="image"></p><p><strong>再次输入命令hexo d执行以后出现下面的结果，</strong></p><p><strong>出现Deploy done :git说明已经部署成功</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS05YzViOTc2Y2U3ZDc2NDk5?x-oss-process=image/format,png" alt="image"></p><p>这时候我们访问一下 GitHub Repository 同名的链接，</p><p>比如我的 KangChou 博客的 Repository 名称取的是 KangChou.github.io，</p><p>那我就访问 <strong><a href="http://kangchou.github.io/">http://KangChou.github.io</a></strong></p><p>这时候我们就可以看到跟本地一模一样的博客内容了。</p><p>（此时你用手机同样可以打开该网站）</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS03MGVkMjY3OWY0MTk5YTUx?x-oss-process=image/format,png" alt="image"></p><h1 id="二、博客的网页主题"><a href="#二、博客的网页主题" class="headerlink" title="二、博客的网页主题**"></a>二、博客的网页主题**</h1><p>主题的设置包括中文页面、整个页面的样式、页面风格等等，</p><p>目前 Hexo 里面应用最多的主题基本就是 Next 主题，</p><p>这个主题还是挺好看的，并且它支持的一些插件和功能都极为丰富，</p><p>配置了这个主题，我们的博客可以支持更多的扩展功能，比如阅览进度条、中英文空格排版、图片懒加载等等。</p><hr><p><strong>13、下载主题</strong></p><p>打开我的电脑创建的Hexo文件夹下的KangChou目录，</p><p>单击右键Git bush输入下面的命令，执行结果如下：</p><p><strong>git clone <a href="https://github.com/theme-next/hexo-theme-next">https://github.com/theme-next/hexo-theme-next</a> themes&#x2F;next</strong></p><p>将下载后的themes主题替换原文件landscape中里所有的文件，并输入启动服务器命令</p><p><strong>hexo server</strong></p><p>执行结果如下</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS01MjE0ZTY1MGQ1NmI1MmU4?x-oss-process=image/format,png" alt="image"></p><hr><p>备注：使用数学公式需要安装这个工具：npm install hexo-math</p><p><strong>14、配置中文环境</strong></p><p>在博客kangchou目录下打开_config.yml修改语言为中文汉语<strong>zh-Hans</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS05N2E4NzU3ZWRlODBkODgz?x-oss-process=image/format,png" alt="image"></p><p><strong>执行的结果如下</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1lNzAxOTk4N2ExOTBlMzdi?x-oss-process=image/format,png" alt="image"></p><p><strong>由于这只是部分为中文，而我的目的是大部分是中文的，</strong></p><p><strong>为了方便还要在网页上手动添加更多中文描述</strong></p><hr><p><strong><strong>15、配置中文菜单栏</strong></strong></p><p>打开C:\Hexo\KangChou\themes\landscape\languages</p><p><strong>发有三种汉语:简体中文、香港繁体、台湾繁体</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1mOWVhYjFkNDNlNGM4OGIw?x-oss-process=image/format,png" alt="image"></p><p>然后点开<code>zh-Hans.yml</code>其中的配置项就是已经翻译的文本</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1mZWIyNTVmNWUzMWM3MWM4?x-oss-process=image/format,png" alt="image"></p><p>网站会根据你<code>站点配置``yml</code>中的语言配置来去读取对应的语言文件</p><p>打开你<code>languages``皮肤配置``yml</code>你会看到菜单栏基础配置：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS05N2QyOGM1ZGE1MTRjZWQ4?x-oss-process=image/format,png" alt="image"></p><p><strong>发现home和archives菜单是开启的，</strong></p><p>现在我们<strong>全部开****启</strong>，只需要去掉前面的#，刷新浏览器</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1lN2QyNzI2N2UzNTEzYzAw?x-oss-process=image/format,png" alt="image"></p><p><strong>尝试修改站点配置yml语言，重启服务后刷新浏览器</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1iOThlM2UxMjI0NzdhMWYy?x-oss-process=image/format,png" alt="image"></p><p><strong>显然结果很成功，****下面关闭git,将结果上传到Github页面：</strong></p><p><strong>重新打开输入部署的三个命令：</strong></p><p><strong>hexo clean</strong> </p><p><strong>hexo g</strong> </p><p><strong>hexo d</strong></p><p><strong>结果和上面一样，此时就可以访问了.</strong></p><p><strong>访问网站****<a href="https://kangchou.github.io/">https://kangchou.github.io/</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS02YTFlZmE2MjBlOTVlNTMw?x-oss-process=image/format,png" alt="image"></p><hr><p><strong>实际上文章到这里就已经结束博客的搭建了，至于其他的</strong></p><p><strong>比如上传文章、上传图片，添加logo等这些我这里就不说了，</strong></p><p><strong>hexo官网以及其他网站都能搜索到具体的教程，</strong></p><p><strong>想继续完善博客网站部署的朋友可以去搜索相关文献学习。</strong></p><h1 id="三、更换域名"><a href="#三、更换域名" class="headerlink" title="三、更换域名**"></a>三、更换域名**</h1><p><strong>相信所有做互联网开发的科技工作者都知道，如果拥有属于自己的网站一定得看起来很专业、很官方、很大气。<strong><strong>因此，有些科技工作者就想更换自己网站的域名，让自己的域名看起来官方标准。也还有另一个原因，因为Github毕竟是外国网站，国内用户访问相对较慢，因此，如果有国内的域名作为辅助会事半功倍。</strong></strong>事实上，我个人觉得只要可以搭建网站，即便是不换域名也没什么区别。<strong><strong>不过，既然我给大家写这个教程，我还是有必要说一下，毕竟有很多人还是愿意换域名的。</strong></strong>如果不想花钱买域名的，这一小节可以跳过。******</strong>16、购买域名+注册阿里网+实名认证*<em><strong><strong><strong>自行注册，如果你是在校大学生，包括硕士、博士购买域名都是有学生价优惠的，但是</strong></strong>一定要使用自己在学校注册的电子邮箱</strong></em><em>，因为阿里云官网数据库可以识别你的学生信息的学年期限。此外，注册以后一定要进行学生认证、实名认证。</em><em><strong><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1mM2M4ZTdkMTcyODNmNDNk?x-oss-process=image/format,png" alt="image"></strong> <strong>然后去买域名，域名的形式有很多，按照自己的需求进行设置域名名称和域名后缀。（实名认证最快是两天的时间）</strong></em><em><a href="https://www.aliyun.com/">https://www.aliyun.com/</a></em>*</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1hMDI4MzE3ZDg0MmQzNGI4?x-oss-process=image/format,png" alt="image"></p><p><strong><strong><strong><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1iY2FmMDc1MjMyYWNkYmRk?x-oss-process=image/format,png" alt="image"></strong></strong></strong> </p><p> <strong><strong><strong>17、在阿里云添加域名解析</strong></strong></strong></p><p>cmd+ping你的<a href="http://github.io域名,得到一个ip/">http://github.io域名，得到一个IP</a></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1kMThhMTFlNjhlMjRlMDI3?x-oss-process=image/format,png" alt="image"></p><p>修改你的域名解析记录</p><p><strong>添加两个A记录，用得到的IP，一个主机记录为：“www”，一个为“@”，</strong></p><p><strong>这样通过<a href="https://coomatrix.com/%E5%B0%B1%E8%83%BD%E8%AE%BF%E9%97%AE%E5%88%B0%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E4%BA%86">https://coomatrix.com/就能访问到你的博客了</a></strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS05Y2QxZjFjMjc5NWM1YjM0?x-oss-process=image/format,png" alt="image"></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1iZjYxNGFkZWQ3NTRhYTA3?x-oss-process=image/format,png" alt="image"></p><p><strong><strong><strong>18、</strong></strong></strong>填写绑定的域名在你的本地文件下也就是hexo—&gt;你的博客（我的是KangChou）本地目录下找到 文件夹<code>source</code> ，并在该文件目录下面新建一个文件CNAME文件，那么一定要注意创建的CNAME文件<strong>没有任何扩展名</strong>（切记）</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00NTQwYTE0ZTBmNjk1OGQ1?x-oss-process=image/format,png" alt="image"></p><p>再一次使用部署三命令<strong>hexo clean<strong><strong>hexo g</strong></strong>hexo d****完成以后，</strong>进入Github设置，找到 Custom domain添加域名后保存即可</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS1lMTlhMDI3OTk3YTEyZTA3?x-oss-process=image/format,png" alt="image"></p><p><strong><strong><strong>19、刷新网页+更改域名成功</strong></strong></strong></p><p><strong>如果上面的17没有出错的话，那么你填完域名保存以后会出现下面的结果</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS0zYTc5ZmFhYjc1ZmVlMjk5?x-oss-process=image/format,png" alt="image"></p><p>那么就是更改域名成功了，此时你只需要点击上图的域名就可以访问啦。</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS03MmM2YmEyY2RjM2RkZGU2?x-oss-process=image/format,png" alt="image"></p><p><strong>……到此完成了本博客的搭建……</strong></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS00ZWYyZWFhYzNiOWJjY2Q2?x-oss-process=image/format,png" alt="image"></p><p>投稿—&gt;展示你的才华</p><p>请发邮件到</p><p><strong><a href="mailto:&#x6b;&#97;&#x6e;&#103;&#x73;&#x69;&#x6e;&#x78;&#64;&#x79;&#101;&#x61;&#104;&#46;&#x6e;&#x65;&#116;">&#x6b;&#97;&#x6e;&#103;&#x73;&#x69;&#x6e;&#x78;&#64;&#x79;&#101;&#x61;&#104;&#46;&#x6e;&#x65;&#116;</a></strong></p><p>标题注明【投稿】</p><p>告诉我们</p><p>你是谁，从哪来，投什么</p><p>我们会及时回复你</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTg2MzE3MS04MDhkMTI5Y2VhMDM3MGVi?x-oss-process=image/format,png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> 教程学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机科学与人工智能论文汇集</title>
      <link href="/2019/07/05/papers/"/>
      <url>/2019/07/05/papers/</url>
      
        <content type="html"><![CDATA[<h1 id="计算机科学技术书籍"><a href="#计算机科学技术书籍" class="headerlink" title="计算机科学技术书籍"></a>计算机科学技术书籍</h1><p>GO、黑客、Android、计算机原理、人工智能、大数据、机器学习、数据库、PHP、java、架构、消息队列、算法、python、爬虫、操作系统、linux、C语言：</p><p><a href="https://github.com/TIM168/technical_books">https://github.com/TIM168/technical_books</a></p><p>计算机科学，软件技术，创业，思想类，数学类，人物传记书籍：<a href="https://github.com/0voice/expert_readed_books">https://github.com/0voice/expert_readed_books</a></p><p>国内几所大学专业课程资料整理：<a href="https://github.com/lib-pku/libpku">https://github.com/lib-pku/libpku</a></p><p>NLP自然语言处理资料汇总：</p><pre><code>中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术：</code></pre><p><a href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p><h1 id="AI书籍与算法源码"><a href="#AI书籍与算法源码" class="headerlink" title="AI书籍与算法源码"></a>AI书籍与算法源码</h1><p>神经网络与深度学习:<a href="https://nndl.github.io/">https://nndl.github.io/</a></p><p>统计学方法：<a href="https://github.com/SmirkCao/Lihang">https://github.com/SmirkCao/Lihang</a></p><p>统计学方法习题答案:<a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/">https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/</a></p><p><a href="https://github.com/SuperCV/Book">https://github.com/SuperCV/Book</a></p><p>机器学习：</p><ul><li><a href="https://github.com/MorvanZhou/tutorials">https://github.com/MorvanZhou/tutorials</a></li><li><a href="https://github.com/lawlite19/MachineLearning_Python">https://github.com/lawlite19/MachineLearning_Python</a></li><li>西瓜书 <a href="https://github.com/datawhalechina/pumpkin-book">https://github.com/datawhalechina/pumpkin-book</a></li><li>机器视觉:<a href="https://github.com/Ewenwan/MVision">https://github.com/Ewenwan/MVision</a></li><li>视觉算法排名:<a href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark</a></li><li>机器学习资源大全中文版:<a href="https://github.com/jobbole/awesome-machine-learning-cn">https://github.com/jobbole/awesome-machine-learning-cn</a></li><li>机器数学与代码实现:<a href="https://ailearning.apachecn.org/#/">https://ailearning.apachecn.org/#/</a></li></ul><p><a href="https://github.com/fengdu78/lihang-code">https://github.com/fengdu78/lihang-code</a></p><p><a href="https://github.com/Dod-o/Statistical-Learning-Method_Code">https://github.com/Dod-o/Statistical-Learning-Method_Code</a></p><p><a href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p><p><a href="https://github.com/zslucky/awesome-AI-books">https://github.com/zslucky/awesome-AI-books</a></p><p><a href="https://github.com/dsgiitr/d2l-pytorch">https://github.com/dsgiitr/d2l-pytorch</a></p><p><a href="https://github.com/1033020837/Basic4AI">https://github.com/1033020837/Basic4AI</a></p><p><a href="https://github.com/xmj-ai/deeplearning_ai_books">https://github.com/xmj-ai/deeplearning_ai_books</a></p><p><a href="https://github.com/lllhhh/BooksKeeper">https://github.com/lllhhh/BooksKeeper</a></p><p><a href="https://github.com/cosen1024/awesome-cs-books">https://github.com/cosen1024/awesome-cs-books</a></p><p><a href="https://github.com/wugenqiang/NoteBook">https://github.com/wugenqiang/NoteBook</a></p><p><a href="https://github.com/bat67/awesome-ai-books-and-code">https://github.com/bat67/awesome-ai-books-and-code</a></p><p><a href="https://github.com/china-testing/python-api-tesing">https://github.com/china-testing/python-api-tesing</a></p><p><a href="https://github.com/iamshuaidi/CS-Book">https://github.com/iamshuaidi/CS-Book</a></p><p><a href="https://github.com/itdevbooks/pdf">https://github.com/itdevbooks/pdf</a></p><p><a href="https://github.com/getsources/CS-Growing-book">https://github.com/getsources/CS-Growing-book</a></p><p><a href="https://github.com/861664308/Tensorflow-Keras--">https://github.com/861664308/Tensorflow-Keras--</a></p><p><a href="https://github.com/jlgulu/PythonAIPath-Geek">https://github.com/jlgulu/PythonAIPath-Geek</a></p><p><a href="https://github.com/zhangziliang04/aipm">https://github.com/zhangziliang04/aipm</a></p><p><a href="https://github.com/Baiyuetribe/paper2gui">https://github.com/Baiyuetribe/paper2gui</a></p><p><a href="https://github.com/Robinwho/Deep-Learning">https://github.com/Robinwho/Deep-Learning</a></p><p>构建开源对话机器人:<a href="https://github.com/Chinese-NLP-book/rasa_chinese_book_code">https://github.com/Chinese-NLP-book/rasa_chinese_book_code</a></p><p><a href="http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf">http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf</a></p><p><a href="http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf">http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf</a></p><p>《Navin Sabharwal - Hands-on Question Answering Systems with BERT_ Applications in Neural Networks and Natural Language Processing-Apress (2021)》</p><p>《Sudharsan Ravichandiran - Getting Started with Google BERT_ Build and train state-of-the-art natural language processing models using BERT-Packt Publishing Ltd (2021)》</p><ul><li><a href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></li><li><a href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></li><li><a href="https://github.com/Microstrong0305/Python2AI">https://github.com/Microstrong0305/Python2AI</a></li><li><a href="https://github.com/chatopera/Synonyms">https://github.com/chatopera/Synonyms</a></li><li><a href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes">https://github.com/cj0012/AI-Practice-Tensorflow-Notes</a></li><li><a href="https://github.com/YeonwooSung/ai_book">https://github.com/YeonwooSung/ai_book</a></li><li><a href="https://github.com/jikexueyuanwiki/tensorflow-zh">https://github.com/jikexueyuanwiki/tensorflow-zh</a></li><li><a href="https://github.com/JDHHH/AI-Books">https://github.com/JDHHH/AI-Books</a></li><li><a href="https://github.com/lihanghang/Deep-learning-And-Paper">https://github.com/lihanghang/Deep-learning-And-Paper</a></li><li><a href="https://github.com/koryako/FundamentalsOfAI_book_code">https://github.com/koryako/FundamentalsOfAI_book_code</a></li><li><a href="https://github.com/zhangbincheng1997/chatbot-aiml-webqa">https://github.com/zhangbincheng1997/chatbot-aiml-webqa</a></li><li><a href="https://github.com/qqqil/books">https://github.com/qqqil/books</a></li><li><a href="https://github.com/KeKe-Li/books">https://github.com/KeKe-Li/books</a></li><li><a href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li><li><a href="https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95">https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95</a></li><li><a href="https://github.com/Dujltqzv/Some-Many-Books">https://github.com/Dujltqzv/Some-Many-Books</a></li></ul><h1 id="计算机视觉实时动态"><a href="#计算机视觉实时动态" class="headerlink" title="计算机视觉实时动态"></a>计算机视觉实时动态</h1><p><a href="https://openaccess.thecvf.com/menu">https://openaccess.thecvf.com/menu</a></p><p><img src="https://user-images.githubusercontent.com/36963108/193174986-63d2ae54-ee0f-4507-8f32-b75d325d78a9.png" alt="image"></p><h1 id="3D-对象检测"><a href="#3D-对象检测" class="headerlink" title="3D 对象检测"></a>3D 对象检测</h1><p><img src="https://miro.medium.com/max/1400/0*JDqH7_kKaGpkoUmv.png"></p><p>参考来自： <a href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection">https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection</a></p><p><a href="https://github.com/TianhaoFu/Awesome-3D-Object-Detection">https://github.com/TianhaoFu/Awesome-3D-Object-Detection</a></p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><ul><li><a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI 数据集</a></li><li>3,712 个训练样本</li><li>3,769 个验证样本</li><li>7,518个测试样本</li><li><a href="https://www.nuscenes.org/">nuScenes 数据集</a></li><li>28k 训练样本</li><li>6k 验证样本</li><li>6k 测试样本</li><li><a href="https://level-5.global/data/perception/">Lyft 数据集</a></li><li><a href="https://waymo.com/open/download/">Waymo 开放数据集</a></li><li>798 个训练序列，大约 158、361 个 LiDAR 样本</li><li>202 个验证序列，包含 40、077 个 LiDAR 样本。</li></ul><h1 id="顶级会议和研讨会"><a href="#顶级会议和研讨会" class="headerlink" title="顶级会议和研讨会"></a>顶级会议和研讨会</h1><h1 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h1><ul><li>计算机视觉与模式识别会议（CVPR）</li><li>计算机视觉国际会议（ICCV）</li><li>欧洲计算机视觉会议（ECCV）</li></ul><h1 id="作坊"><a href="#作坊" class="headerlink" title="作坊"></a>作坊</h1><ul><li>CVPR 2019 自动驾驶研讨会（<a href="http://cvpr2019.wad.vision/">nuScenes 3D detection</a>）</li><li>CVPR 2020 自动驾驶研讨会（<a href="http://cvpr2020.wad.vision/">BDD1k 3D tracking</a>）</li><li>CVPR 2021 自动驾驶研讨会（<a href="http://cvpr2021.wad.vision/">waymo 3D检测</a>）</li><li>CVPR 2022 自动驾驶研讨会（<a href="http://cvpr2022.wad.vision/">waymo 3D检测</a>）</li><li><a href="https://sites.google.com/view/cvpr2021-3d-vision-robotics">CVPR 2021 3D 视觉和机器人研讨会</a></li><li><a href="https://scene-understanding.com/">CVPR 2021 视觉、图形和机器人 3D 场景理解研讨会</a></li><li><a href="http://wad.ai/">ICCV 2019 自动驾驶研讨会</a></li><li><a href="https://avvision.xyz/iccv21/">ICCV 2021 自动驾驶汽车视觉研讨会（AVVision）</a>，<a href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Fan_Autonomous_Vehicle_Vision_2021_ICCV_Workshop_Summary_ICCVW_2021_paper.pdf">注</a></li><li><a href="https://competitions.codalab.org/competitions/33236#learn_the_details">ICCV 2021 研讨会 SSLAD Track 2–3D 对象检测</a></li><li><a href="https://c4av-2020.github.io/">ECCV 2020 自动驾驶汽车指令研讨会</a></li><li><a href="https://sites.google.com/view/pad2020">ECCV 2020 自动驾驶感知研讨会</a></li></ul><h1 id="论文（基于激光雷达的方法）"><a href="#论文（基于激光雷达的方法）" class="headerlink" title="论文（基于激光雷达的方法）"></a>论文（基于激光雷达的方法）</h1><ul><li>用于 LiDAR 点云中 3D 对象检测的端到端多视图融合<a href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">论文</a></li><li>使用全卷积网络（百度）<a href="https://arxiv.org/abs/1608.07916">论文从 3D 激光雷达进行车辆检测</a></li><li><a href="https://arxiv.org/pdf/1711.06396.pdf">VoxelNet：基于点云的 3D 对象检测论文</a>的端到端学习<a href="https://arxiv.org/pdf/1711.06396.pdf"></a></li><li><a href="https://arxiv.org/pdf/1805.08689.pdf">使用深度卷积网络论文</a>在占用网格地图中进行对象检测和分类<a href="https://arxiv.org/pdf/1805.08689.pdf"></a></li><li>RT3D：用于自动驾驶的 LiDAR 点云中的实时 3-D 车辆检测<a href="https://www.onacademic.com/detail/journal_1000040467923610_4dfe.html">论文</a></li><li>BirdNet：来自 LiDAR 信息<a href="https://arxiv.org/pdf/1805.01195.pdf">论文的 3D 对象检测框架</a></li><li><a href="https://arxiv.org/pdf/1805.04902.pdf">LMNet：使用 3D LiDAR论文</a>在 CPU 上进行实时多类目标检测<a href="https://arxiv.org/pdf/1805.04902.pdf"></a></li><li>HDNET: Exploit HD Maps for 3D Object Detection<a href="https://link.zhihu.com/?target=http://proceedings.mlr.press/v87/yang18b/yang18b.pdf">论文</a></li><li>PointNet：用于 3D 分类和分割的点集的深度学习<a href="https://arxiv.org/pdf/1612.00593.pdf">论文</a></li><li>PointNet++：度量空间中点集的深度分层特征学习<a href="https://arxiv.org/abs/1706.02413">论文</a></li><li>IPOD: Intensive Point-based Object Detector for Point Cloud<a href="https://arxiv.org/abs/1812.05276v1">论文</a></li><li>PIXOR：来自点云的实时 3D 对象检测<a href="http://www.cs.toronto.edu/~wenjie/papers/cvpr18/pixor.pdf">论文</a></li><li>DepthCN：车辆检测使用 3D-LIDAR 和 ConvNet<a href="https://www.baidu.com/link?url=EaE2zYjHkWvF33nsET2eNvbFGFu8-D3wWPia04uyKm95jMetHsSv3Zk-tODPGm5clsgCUgtVULsZ6IQqv0EYS_Z8El7Zzh57XzlJroSkaOuC8yv7r1XXL4bUrM2tWrTgjwqzfMV2tMTnFNbMOmHLTkUobgMg7HKoS6WW6PfQzkG&wd=&eqid=8f320cfa0005b878000000055e528b6d">论文</a></li><li>Voxel-FPN：点云 3D 对象检测中的多尺度体素特征聚合<a href="https://arxiv.org/ftp/arxiv/papers/1907/1907.05286.pdf">论文</a></li><li>STD：点云<a href="https://arxiv.org/abs/1907.10471">纸的稀疏到密集 3D 对象检测器</a></li><li>快速点 R-CNN<a href="https://arxiv.org/abs/1908.02990">论文</a></li><li>StarNet：点云中目标检测的目标计算<a href="https://arxiv.org/abs/1908.11069">论文</a></li><li>点云 3D 对象检测<a href="https://arxiv.org/abs/1908.09492v1">论文的类平衡分组和采样</a></li><li>LaserNet：一种用于自动驾驶<a href="https://arxiv.org/abs/1903.08701v1">论文的高效概率 3D 对象检测器</a></li><li>FVNet：3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds<a href="https://arxiv.org/abs/1903.10750v1">论文</a></li><li>Part-A² Net：3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud<a href="https://arxiv.org/abs/1907.03670v1">论文</a></li><li>PointRCNN：3D Object Proposal Generation and Detection from Point Cloud<a href="https://arxiv.org/abs/1812.04244">论文</a></li><li>Complex-YOLO：点云上的实时 3D 对象检测<a href="https://arxiv.org/abs/1803.06199">论文</a></li><li>YOLO4D: A ST Approach for RT Multi-object Detection and Classification from LiDAR Point Clouds<a href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">论文</a></li><li><a href="https://arxiv.org/abs/1808.02350">YOLO3D：来自 LiDAR 点云论文</a>的端到端实时 3D 面向对象边界框检测<a href="https://arxiv.org/abs/1808.02350"></a></li><li>使用 Pseudo-LiDAR 点云<a href="https://arxiv.org/pdf/1903.09847.pdf">论文进行单目 3D 对象检测</a></li><li>Structure Aware Single-stage 3D Object Detection from Point Cloud（CVPR2020）<a href="http://openaccess.thecvf.com/content_CVPR_2020/html/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.html">论文</a> <a href="https://github.com/skyhehe123/SA-SSD">代码</a></li><li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detection（CVPR2020）<a href="https://arxiv.org/abs/2004.05679">论文</a> <a href="https://github.com/NUAAXQ/MLCVNet">代码</a></li><li>3DSSD: Point-based 3D Single Stage Object Detector（CVPR2020）<a href="https://arxiv.org/abs/2002.10187">论文</a> <a href="https://github.com/tomztyang/3DSSD">代码</a></li><li>LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention（CVPR2020）<a href="https://arxiv.org/abs/2004.01389">论文</a> <a href="https://github.com/yinjunbo/3DVID">代码</a></li><li>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection(CVPR2020)<a href="https://arxiv.org/abs/1912.13192">论文</a> <a href="https://github.com/sshaoshuai/PV-RCNN">代码</a></li><li>Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud（CVPR2020）<a href="https://arxiv.org/abs/2003.01251">论文</a> <a href="https://github.com/WeijingShi/Point-GNN">代码</a></li><li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detection（CVPR2020）<a href="https://arxiv.org/pdf/2004.05679">论文</a></li><li>Density Based Clustering for 3D Object Detection in Point Clouds（CVPR2020）<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Ahmed_Density-Based_Clustering_for_3D_Object_Detection_in_Point_Clouds_CVPR_2020_paper.pdf">论文</a></li><li>所见即所得：Exploiting Visibility for 3D Object Detection（CVPR2020）<a href="https://arxiv.org/pdf/1912.04986.pdf">论文</a></li><li>PointPainting: Sequential Fusion for 3D Object Detection (CVPR2020)<a href="https://arxiv.org/pdf/1911.10150.pdf">论文</a></li><li>HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection（CVPR2020）<a href="https://arxiv.org/pdf/2003.00186">论文</a></li><li>LiDAR R-CNN: An Efficient and Universal 3D Object Detector（CVPR2021）<a href="https://arxiv.org/abs/2103.15297">论文</a></li><li>Center-based 3D Object Detection and Tracking (CVPR2021)<a href="https://arxiv.org/abs/2006.11275">论文</a></li><li>3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection (CVPR2021)<a href="https://arxiv.org/pdf/2012.04355.pdf">论文</a></li><li>Embracing Single Stride 3D Object Detector with Sparse Transformer（CVPR2022）<a href="https://arxiv.org/pdf/2112.06375.pdf">论文</a>，<a href="https://github.com/TuSimple/SST">代码</a></li><li>Point Density-Aware Voxels for LiDAR 3D Object Detection (CVPR2022)<a href="https://arxiv.org/abs/2203.05662">论文</a>，<a href="https://github.com/TRAILab/PDV">代码</a></li><li>A Unified Query-based Paradigm for Point Cloud Understanding (CVPR2022)<a href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">论文</a></li><li>Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds (CVPR2022)<a href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">论文</a>，<a href="https://github.com/Ghostish/Open3DSOT">代码</a></li><li>并非所有的点都是平等的：Learning High Efficient Point-based Detectors for 3D LiDAR Point Clouds (CVPR2022)<a href="https://arxiv.org/abs/2203.11139">论文</a>，<a href="https://github.com/yifanzhang713/IA-SSD">代码</a></li><li>回到现实：Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement（CVPR2022）<a href="http://arxiv.org/abs/2203.05238">论文</a>，<a href="https://github.com/xuxw98/BackToReality">代码</a></li><li>Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds (CVPR2022)<a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/VoxSeT_cvpr22.pdf">论文</a>，<a href="https://github.com/skyhehe123/VoxSeT">代码</a></li><li>BoxeR: Box-Attention for 2D and 3D Transformers(CVPR2022)<a href="https://arxiv.org/abs/2111.13087">论文</a>,<a href="https://github.com/kienduynguyen/boxer">代码</a>,<a href="https://mp.weixin.qq.com/s/UnUJJBwcAsRgz6TnQf_b7w">中文介绍</a></li><li>规范投票：Towards Robust Oriented Bounding Box Detection in 3D Scenes (CVPR2022)<a href="https://arxiv.org/abs/2011.12001">论文</a>，<a href="https://github.com/qq456cvb/CanonicalVoting">代码</a></li><li>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection(CVPR2022)<a href="https://arxiv.org/abs/2203.08195">论文</a>，<a href="https://github.com/tensorflow/lingvo">代码</a></li><li>TransFusion：使用 Transformers 进行 3D 对象检测的稳健 LiDAR-Camera Fusion。(CVPR2022)<a href="https://arxiv.org/abs/2203.11496">论文</a>，<a href="https://github.com/xuyangbai/transfusion">代码</a></li><li>Point2Seq：将 3D 对象检测为序列。(CVPR2022)<a href="https://arxiv.org/abs/2203.13394">论文</a>，<a href="https://github.com/ocnflag/point2seq">代码</a></li><li>CAT-Det：用于多模态 3D 对象检测的对比增强变压器（CVPR2022）<a href="https://arxiv.org/abs/2204.00325">论文</a></li><li>LiDAR Snowfall Simulation for Robust 3D Object Detection (CVPR2022)<a href="https://arxiv.org/abs/2203.15118">论文</a>，<a href="https://github.com/syscv/lidar_snow_sim">代码</a></li><li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a href="https://arxiv.org/abs/2203.15175">论文</a>，<a href="https://github.com/visionml/pytracking">代码</a></li><li>Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion (CVPR2022)<a href="https://arxiv.org/abs/2203.09780">论文</a></li><li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a href="https://arxiv.org/abs/2203.15175">论文</a>，<a href="https://github.com/visionml/pytracking">代码</a></li></ul><h1 id="竞赛解决方案"><a href="#竞赛解决方案" class="headerlink" title="竞赛解决方案"></a>竞赛解决方案</h1><h1 id="工程"><a href="#工程" class="headerlink" title="工程"></a>工程</h1><h1 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h1><ul><li>2021.04 用于自动驾驶应用的基于点云的 3D 对象检测和分类方法：调查和分类<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520304097">论文</a></li><li>2021.07 用于自动驾驶的 3D 对象检测：调查<a href="https://arxiv.org/abs/2106.10823">论文</a></li><li>2021.07 自动驾驶中的多模态 3D 对象检测：调查<a href="https://arxiv.org/abs/2106.12735">论文</a></li><li>2021.10 基于激光雷达的 3D 物体检测方法与深度学习的自动驾驶<a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849321001321">论文综合调查</a></li><li>2021.12 3D 点云的深度学习：调查<a href="https://ieeexplore.ieee.org/abstract/document/9127813">论文</a></li></ul><h1 id="书"><a href="#书" class="headerlink" title="书"></a>书</h1><ul><li>基于激光雷达和摄像头的 3D 对象检测算法：设计与仿真<a href="https://www.amazon.com/Object-Detection-Algorithms-Based-Camera/dp/6200536538">书</a></li></ul><h1 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h1><ul><li>Aivia 在线研讨会：3D 对象检测和跟踪<a href="https://www.youtube.com/watch?v=P0TrkwAdFYQ">视频</a></li><li>3D 对象检索 2021 研讨会<a href="https://3dor2021.github.io/programme.html">视频</a></li><li><a href="https://www.youtube.com/watch?v=vfL6uJYFrp4">来自 UCSD视频</a>的 SU 实验室的 3D 深度学习教程<a href="https://www.youtube.com/watch?v=vfL6uJYFrp4"></a></li><li>讲座：自动驾驶汽车（图宾根大学 Andreas Geiger 教授）<a href="https://www.youtube.com/watch?v=vfL6uJYFrp4">视频</a></li><li>点云对象的当前方法和未来方向 (2021.04)<a href="https://www.youtube.com/watch?v=xFFCQVwYeec">视频</a></li><li>CPU 上 30+ FPS 的最新 3D 对象检测 — MediaPipe 和 OpenCV Python (2021.05)<a href="https://www.youtube.com/watch?v=f-Ibri14KMY">视频</a></li><li>MIT自动驾驶研讨会（2019.11）<a href="https://space.bilibili.com/174493426/channel/series">视频</a></li><li>sensetime 研讨会1<a href="https://www.bilibili.com/video/BV1Bf4y1b7PF?spm_id_from=333.999.0.0">视频</a></li><li>sensetime 研讨会 2<a href="https://docs.google.com/presentation/d/11CoKCxRFgzbIujMXxTZjHDo_hV0arEQ7sUFWFXWaX8o/edit#slide=id.p1">张幻灯片</a></li></ul><h1 id="课程"><a href="#课程" class="headerlink" title="课程"></a>课程</h1><ul><li><a href="http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf">多伦多大学，csc2541</a></li><li><a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/">图宾根大学，自动驾驶汽车</a> <em>（强烈推荐）</em></li><li><a href="https://apollo.auto/devcenter/devcenter.html">百度-Udacity</a></li><li><a href="http://bit.baidu.com/Subject/index/id/16.html">百度-阿波罗</a></li><li><a href="https://www.coursera.org/specializations/self-driving-cars?ranMID=40328&ranEAID=9IqCvd3EEQc&ranSiteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&siteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=9IqCvd3EEQc">多伦多大学，课程</a></li></ul><h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><ul><li><a href="https://blog.waymo.com/">Waymo 博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/142401769">apollo介绍之感知模块</a></li><li><a href="https://github.com/daohu527/Dig-into-Apollo#ledger-%E7%9B%AE%E5%BD%95">Apollo 笔记（Apollo 学习笔记）— Apollo 初学者学习笔记。</a></li><li><a href="https://zhuanlan.zhihu.com/p/44809266">PointNet系列论文解读</a></li><li><a href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/deep3dbox.html">Deep3dBox：使用深度学习和几何进行 3D 边界框估计</a></li><li><a href="https://zhuanlan.zhihu.com/p/356892010">SECOND算法解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/361973979">PointRCNN深度解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/363926237">Fast PointRCNN论文解读</a></li><li><a href="https://zhuanlan.zhihu.com/p/357626425">PointPillars论文和代码解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/352419316">VoxelNet论文和代码解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/444447881">CenterPoint分析</a></li><li><a href="https://zhuanlan.zhihu.com/p/148942116">PV-RCNN：3D目标检测Waymo模态挑战赛+KITTI榜单模态第一模挑战赛</a></li><li><a href="https://zhuanlan.zhihu.com/p/359800738">LiDAR R-CNN：一种快速、通用的二类3D检测器</a></li><li><a href="https://zhuanlan.zhihu.com/p/122426949">混合体素网络（HVNet）</a></li><li><a href="https://zhuanlan.zhihu.com/p/420708905">自动驾驶汽车| 范围图像纸分享</a></li><li><a href="https://zhuanlan.zhihu.com/p/476056546">SST：单步放大装置Transformer 3D探测仪</a></li></ul><h1 id="著名研究组-x2F-学者"><a href="#著名研究组-x2F-学者" class="headerlink" title="著名研究组&#x2F;学者"></a>著名研究组&#x2F;学者</h1><ul><li><a href="https://scholar.google.com/citations?user=yAWtq6QAAAAJ&hl=en">王乃燕@Tusimple</a></li><li><a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en">李洪生@CUHK</a></li><li><a href="https://scholar.google.com/citations?user=Fe7NTe0AAAAJ&hl=en">一次 Tuzel@Apple</a></li><li><a href="https://scholar.google.com/citations?user=XP_Hxm4AAAAJ&hl=en">奥斯卡Beijbom@nuTonomy</a></li><li><a href="https://scholar.google.com/citations?user=jyxO2akAAAAJ&hl=en">Raquel Urtasun@多伦多大学</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=dzOd2hgAAAAJ&view_op=list_works&sortby=pubdate">Philipp Krähenbühl@UT Austin</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=9B8PoXUAAAAJ&view_op=list_works&sortby=pubdate">德瓦拉马南@CMU</a></li><li><a href="https://jiaya.me/">贾家亚@CUHK</a></li><li><a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser@princeton</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=5JlEyTAAAAAJ&view_op=list_works&sortby=pubdate">列奥尼达斯·吉巴斯@斯坦福</a></li><li><a href="https://www.trailab.utias.utoronto.ca/">史蒂文·瓦斯兰德@多伦多大学</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=nFefEI8AAAAJ&view_op=list_works&sortby=pubdate">Ouais Alsharif@Google 大脑</a></li><li><a href="https://scholar.google.com/citations?hl=en&user=i7U4YogAAAAJ&view_op=list_works&sortby=pubdate">柴育宁（前）@waymo</a></li><li><a href="http://yulanguo.me/">郭玉兰@NUDT</a></li><li><a href="https://www4.comp.polyu.edu.hk/~cslzhang/">张磊@香港理工大学</a></li><li><a href="https://lihongyang.info/">李洪洋@sensetime</a></li></ul><h1 id="著名的代码库"><a href="#著名的代码库" class="headerlink" title="著名的代码库"></a>著名的代码库</h1><ul><li><a href="https://github.com/PointCloudLibrary/pcl">点云库 (PCL)</a></li><li><a href="https://github.com/traveller59/spconv">Spconv</a></li><li><a href="https://github.com/poodarchu/Det3D">Det3D</a></li><li><a href="https://github.com/open-mmlab/mmdetection3d">毫米检测3d</a></li><li><a href="https://github.com/open-mmlab/OpenPCDet">开放PCDet</a></li><li><a href="https://github.com/tianweiy/CenterPoint">中心点</a></li><li><a href="https://github.com/ApolloAuto">Apollo Auto——百度开放自动驾驶平台</a></li><li><a href="https://www.autoware.org/">AutoWare——东京大学自动驾驶平台</a></li><li><a href="https://comma.ai/">Openpilot — 一种开源软件，旨在改进当今道路上大多数新车的现有驾驶员辅助</a></li></ul><h1 id="深度学习点云参考论文来源"><a href="#深度学习点云参考论文来源" class="headerlink" title="深度学习点云参考论文来源"></a>深度学习点云参考论文来源</h1><h1><figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">- Recent papers (from 2017)</span></span><br></pre></td></tr></table></figure></h1><h3> Keywords </h3><p><strong><code>dat.</code></strong>: dataset &amp;emsp; | &amp;emsp; <strong><code>cls.</code></strong>: classification &amp;emsp; | &amp;emsp; <strong><code>rel.</code></strong>: retrieval &amp;emsp; | &amp;emsp; <strong><code>seg.</code></strong>: segmentation<br><strong><code>det.</code></strong>: detection &amp;emsp; | &amp;emsp; <strong><code>tra.</code></strong>: tracking &amp;emsp; | &amp;emsp; <strong><code>pos.</code></strong>: pose &amp;emsp; | &amp;emsp; <strong><code>dep.</code></strong>: depth<br><strong><code>reg.</code></strong>: registration &amp;emsp; | &amp;emsp; <strong><code>rec.</code></strong>: reconstruction &amp;emsp; | &amp;emsp; <strong><code>aut.</code></strong>: autonomous driving<br><strong><code>oth.</code></strong>: other, including normal-related, correspondence, mapping, matching, alignment, compression, generative model…</p><p>Statistics: 🔥 code is available &amp; stars &gt;&#x3D; 100 &amp;emsp;|&amp;emsp; ⭐️ citation &gt;&#x3D; 50</p><hr><h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><ul><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf">CVPR</a>] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. [<a href="https://github.com/charlesq34/pointnet">tensorflow</a>][<a href="https://github.com/fxia22/pointnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.pdf">CVPR</a>] Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs. [<strong><code>cls.</code></strong>] ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yi_SyncSpecCNN_Synchronized_Spectral_CVPR_2017_paper.pdf">CVPR</a>] SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation. [<a href="https://github.com/ericyi/SyncSpecCNN">torch</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf">CVPR</a>] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. [<a href="http://www.scan-net.org/">project</a>][<a href="http://www.scan-net.org/">git</a>] [<strong><code>dat.</code></strong> <strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Mostegel_Scalable_Surface_Reconstruction_CVPR_2017_paper.pdf">CVPR</a>] Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Straub_Efficient_Global_Point_CVPR_2017_paper.pdf">CVPR</a>] Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures. [<a href="http://people.csail.mit.edu/jstraub/">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Vongkulbhisal_Discriminative_Optimization_Theory_CVPR_2017_paper.pdf">CVPR</a>] Discriminative Optimization: Theory and Applications to Point Cloud Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Elbaz_3D_Point_Cloud_CVPR_2017_paper.pdf">CVPR</a>] 3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder. [<a href="https://github.com/gilbaz/LORAX">git</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf">CVPR</a>] Multi-View 3D Object Detection Network for Autonomous Driving. [<a href="https://github.com/bostondiditeam/MV3D">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf">CVPR</a>] 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions. [<a href="https://github.com/andyzeng/3dmatch-toolbox">code</a>] [<strong><code>dat.</code></strong> <strong><code>pos.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.pdf">CVPR</a>] OctNet: Learning Deep 3D Representations at High Resolutions. [<a href="https://github.com/griegler/octnet">torch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li><li></li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Klokov_Escape_From_Cells_ICCV_2017_paper.pdf">ICCV</a>] Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models. [<a href="https://github.com/fxia22/kdnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>] ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper.pdf">ICCV</a>] 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds. [<a href="https://github.com/CKchaos/scn2pointcloud_tool">code</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Park_Colored_Point_Cloud_ICCV_2017_paper.pdf">ICCV</a>] Colored Point Cloud Registration Revisited. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Nan_PolyFit_Polygonal_Surface_ICCV_2017_paper.pdf">ICCV</a>] PolyFit: Polygonal Surface Reconstruction from Point Clouds. [<a href="https://github.com/LiangliangNan/PolyFit">code</a>] [<strong><code>rec.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ladicky_From_Point_Clouds_ICCV_2017_paper.pdf">ICCV</a>] From Point Clouds to Mesh using Regression. [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf">ICCV</a>] 3D Graph Neural Networks for RGBD Semantic Segmentation. [<a href="https://github.com/yanx27/3DGNN_pytorch">pytorch</a>] [<strong><code>seg.</code></strong>]</li><li></li><li>[<a href="https://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space">NeurIPS</a>] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. [<a href="https://github.com/charlesq34/pointnet2">tensorflow</a>][<a href="https://github.com/erikwijmans/Pointnet2_PyTorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li><li>[<a href="https://papers.nips.cc/paper/6931-deep-sets">NeurIPS</a>] Deep Sets. [<a href="https://github.com/manzilzaheer/DeepSets">pytorch</a>] [<strong><code>cls.</code></strong>] ⭐️</li><li></li><li>[<a href="https://ieeexplore.ieee.org/document/7989161">ICRA</a>] Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks. [<a href="https://github.com/lijiannuist/Vote3Deep_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ⭐️</li><li>[<a href="https://ieeexplore.ieee.org/document/7989591">ICRA</a>] Fast segmentation of 3D point clouds: A paradigm on LiDAR data for autonomous vehicle applications. [<a href="https://github.com/VincentCheungM/Run_based_segmentation">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/document/7989618">ICRA</a>] SegMatch: Segment based place recognition in 3D point clouds. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/document/7989664">ICRA</a>] Using 2 point+normal sets for fast registration of point clouds with small overlap. [<strong><code>reg.</code></strong>]</li><li></li><li>[<a href="https://ieeexplore.ieee.org/document/8202234">IROS</a>] Car detection for autonomous vehicle: LIDAR and vision fusion approach through deep learning framework. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/document/8202239">IROS</a>] 3D object classification with point convolution network. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/document/8205955">IROS</a>] 3D fully convolutional network for vehicle detection in point cloud. [<a href="https://github.com/yukitsuji/3D_CNN_tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li><li>[<a href="https://ieeexplore.ieee.org/document/8206488">IROS</a>] Deep learning of directional truncated signed distance function for robust 3D object recognition. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/document/8206584">IROS</a>] Analyzing the quality of matched 3D point clouds of objects. [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="http://segcloud.stanford.edu/segcloud_2017.pdf">3DV</a>] SEGCloud: Semantic Segmentation of 3D Point Clouds. [<a href="http://segcloud.stanford.edu/">project</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>] ⭐️</li><li></li><li>[<a href="https://ieeexplore.ieee.org/ielx7/34/8454009/08046026.pdf?tp=&arnumber=8046026&isnumber=8454009&ref=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8=">TPAMI</a>] Structure-aware Data Consolidation. [<strong><code>oth.</code></strong>]</li></ul><hr><h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><ul><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf">CVPR</a>] SPLATNet: Sparse Lattice Networks for Point Cloud Processing. [<a href="https://github.com/NVlabs/splatnet">caffe</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.pdf">CVPR</a>] Attentional ShapeContextNet for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Mining_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling. [<a href="http://www.merl.com/research/license#KCNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. [<a href="http://www.merl.com/research/license#FoldingNet">code</a>] [<strong><code>cls.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.pdf">CVPR</a>] Pointwise Convolutional Neural Networks. [<a href="https://github.com/scenenn/pointwise">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] PU-Net: Point Cloud Upsampling Network. [<a href="https://github.com/yulequan/PU-Net">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf">CVPR</a>] SO-Net: Self-Organizing Network for Point Cloud Analysis. [<a href="https://github.com/lijx10/SO-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.pdf">CVPR</a>] Recurrent Slice Networks for 3D Segmentation of Point Clouds. [<a href="https://github.com/qianguih/RSNet">pytorch</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf">CVPR</a>] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. [<a href="https://github.com/facebookresearch/SparseConvNet">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf">CVPR</a>] Deep Parametric Continuous Convolutional Neural Networks. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf">CVPR</a>] PIXOR: Real-time 3D Object Detection from Point Clouds. [<a href="https://github.com/ankita-kalra/PIXOR">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf">CVPR</a>] SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. [<a href="https://github.com/laughtervv/SGPN">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. [<a href="https://github.com/loicland/superpoint_graph">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf">CVPR</a>] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. [<a href="https://github.com/tsinghua-rll/VoxelNet-tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yun_Reflection_Removal_for_CVPR_2018_paper.pdf">CVPR</a>] Reflection Removal for Large-Scale 3D Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">CVPR</a>] Hand PointNet: 3D Hand Pose Estimation using Point Sets. [<a href="https://github.com/3huo/Hand-Pointnet">pytorch</a>] [<strong><code>pos.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.pdf">CVPR</a>] PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition. [<a href="https://github.com/mikacuy/pointnetvlad.git">tensorflow</a>] [<strong><code>rel.</code></strong>] 🔥</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf">CVPR</a>] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation. [<strong><code>cls.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf">CVPR</a>] Density Adaptive Point Set Registration. [<a href="https://github.com/felja633/DARE">code</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Birdal_A_Minimalist_Approach_CVPR_2018_paper.pdf">CVPR</a>] A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf">CVPR</a>] Inverse Composition Discriminative Optimization for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf">CVPR</a>] CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles. [<strong><code>tra.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf">CVPR</a>] PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf">CVPR</a>] PointGrid: A Deep Network for 3D Shape Understanding. [<a href="https://github.com/trucleduc/PointGrid">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf">CVPR</a>] PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation. [<a href="https://github.com/malavikabindhi/CS230-PointFusion">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf">CVPR</a>] Frustum PointNets for 3D Object Detection from RGB-D Data. [<a href="https://github.com/charlesq34/frustum-pointnets">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li><li>[<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf">CVPR</a>] Tangent Convolutions for Dense Prediction in 3D. [<a href="https://github.com/tatarchm/tangent_conv">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Matheus_Gadelha_Multiresolution_Tree_Networks_ECCV_2018_paper.pdf">ECCV</a>] Multiresolution Tree Networks for 3D Point Cloud Processing. [<a href="https://github.com/matheusgadelha/MRTNet">pytorch</a>] [<strong><code>cls.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper.pdf">ECCV</a>] EC-Net: an Edge-aware Point set Consolidation Network. [<a href="https://github.com/yulequan/EC-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper.pdf">ECCV</a>] 3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhou_Learning_and_Matching_ECCV_2018_paper.pdf">ECCV</a>] Learning and Matching Multi-View Descriptors for Registration of Point Clouds. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf">ECCV</a>] 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration. [<a href="https://github.com/yewzijian/3DFeatNet">tensorflow</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.pdf">ECCV</a>] Local Spectral Graph Convolution for Point Set Feature Learning. [<a href="https://github.com/fate3439/LocalSpecGCN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper.pdf">ECCV</a>] SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. [<a href="https://github.com/xyf513/SpiderCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yinlong_Liu_Efficient_Global_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Global Point Cloud Registration by Matching Rotation Invariant Features Through Translation Search. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Kejie_Li_Efficient_Dense_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Dense Point Cloud Object Reconstruction using Deformation Vector Fields. [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Dario_Rethage_Fully-Convolutional_Point_Networks_ECCV_2018_paper.pdf">ECCV</a>] Fully-Convolutional Point Networks for Large-Scale Point Clouds. [<a href="https://github.com/drethage/fully-convolutional-point-network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf">ECCV</a>] Deep Continuous Fusion for Multi-Sensor 3D Object Detection. [<strong><code>det.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Benjamin_Eckart_Fast_and_Accurate_ECCV_2018_paper.pdf">ECCV</a>] HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf">ECCV</a>] Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [<strong><code>pos.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tolga_Birdal_PPF-FoldNet_Unsupervised_Learning_ECCV_2018_paper.pdf">ECCV</a>] PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Zeng_3DContextNet_K-d_Tree_Guided_Hierarchical_Learning_of_Point_Clouds_Using_ECCVW_2018_paper.pdf">ECCVW</a>] 3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.pdf">ECCVW</a>] YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16530/16302">AAAI</a>] Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction. [<a href="https://github.com/chenhsuanlin/3D-point-cloud-generation">tensorflow</a>] [<strong><code>rec.</code></strong>] 🔥</li><li>[<a href="https://ai.tencent.com/ailab/media/publications/aaai/junzhou_-AAAI-Adaptive_Graph_Convolutional_Neural_NetworksI.pdf">AAAI</a>] Adaptive Graph Convolutional Neural Networks. [<strong><code>cls.</code></strong>]</li><li></li><li>[<a href="https://papers.nips.cc/paper/7545-unsupervised-learning-of-shape-and-pose-with-differentiable-point-clouds">NeurIPS</a>] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds. [<a href="https://github.com/eldar/differentiable-point-clouds">tensorflow</a>] [<strong><code>pos.</code></strong>]</li><li>[<a href="https://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points">NeurIPS</a>] PointCNN: Convolution On X-Transformed Points. [<a href="https://github.com/yangyanli/PointCNN">tensorflow</a>][<a href="https://github.com/hxdengBerkeley/PointCNN.Pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li><li></li><li>[<a href="https://arxiv.org/abs/1707.02392">ICML</a>] Learning Representations and Generative Models for 3D Point Clouds. [<a href="https://github.com/optas/latent_3d_points">code</a>] [<strong><code>oth.</code></strong>] 🔥</li><li></li><li>[<a href="https://dl.acm.org/ft_gateway.cfm?id=3201301&ftid=1991771&dwn=1&CFID=155708095&CFTOKEN=598df826a5b545a7-3E7CE91C-DE12-F588-FAEEF2551115E64E">TOG</a>] Point Convolutional Neural Networks by Extension Operators. [<a href="https://github.com/matanatz/pcnn">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1803.09263">SIGGRAPH</a>] P2P-NET: Bidirectional Point Displacement Net for Shape Transform. [<a href="https://github.com/kangxue/P2P-NET">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1806.01759">SIGGRAPH Asia</a>] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds. [<a href="https://github.com/viscom-ulm/MCCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1706.04496">SIGGRAPH</a>] Learning local shape descriptors from part correspondences with multi-view convolutional networks. [<a href="https://people.cs.umass.edu/~hbhuang/local_mvcnn/index.html">project</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1808.07659">MM</a>] PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1806.02952">MM</a>] RGCNN: Regularized Graph CNN for Point Cloud Segmentation. [<a href="https://github.com/tegusi/RGCNN">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1804.10783">MM</a>] Hybrid Point Cloud Attribute Compression Using Slice-based Layered Structure and Block-based Intra Prediction. [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462884">ICRA</a>] End-to-end Learning of Multi-sensor 3D Tracking by Detection. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460837">ICRA</a>] Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462926">ICRA</a>] SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud. [<a href="https://github.com/priyankanagaraj1494/Squeezseg">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461257">ICRA</a>] Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications. [<strong><code>det.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461048">ICRA</a>] High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion. [<strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461095">ICRA</a>] Sampled-Point Network for Classification of Deformed Building Element Point Clouds. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460532">ICRA</a>] Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460605">ICRA</a>] Signature of Topologically Persistent Points for 3D Point Cloud Description. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461232">ICRA</a>] A General Pipeline for 3D Detection of Vehicles. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460716">ICRA</a>] Robust and Fast 3D Scan Alignment Using Mutual Information. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460940">ICRA</a>] Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460862">ICRA</a>] Surface-Based Exploration for Autonomous 3D Modeling. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460554">ICRA</a>] Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460887">ICRA</a>] Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460639">ICRA</a>] Real-Time Object Tracking in Sparse Point Clouds Based on 3D Interpolation. [<strong><code>tra.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460825">ICRA</a>] Robust Generalized Point Cloud Registration Using Hybrid Mixture Model. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461049">ICRA</a>] A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461000">ICRA</a>] Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461102">ICRA</a>] Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460910">ICRA</a>] Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460204">ICRA</a>] Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460834">ICRA</a>] Complex Urban LiDAR Data Set. [<a href="https://www.youtube.com/watch?v=IguZjmLf5V0&feature=youtu.be">video</a>] [<strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593693">IROS</a>] CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.[<a href="https://github.com/epiception/CalibNet">tensorflow</a>] [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593839">IROS</a>] Dynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593733">IROS</a>] A 3D Laparoscopic Imaging System Based on Stereo-Photogrammetry with Random Patterns. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593558">IROS</a>] Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594024">IROS</a>] Octree map based on sparse point cloud and heuristic probability distribution for labeled images. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593854">IROS</a>] PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593953">IROS</a>] Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594299">IROS</a>] LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.[<a href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM">code</a>] [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>] 🔥</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593741">IROS</a>] Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594362">IROS</a>] Stereo Camera Localization in 3D LiDAR Maps. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594049">IROS</a>] Joint 3D Proposal Generation and Object Detection from View Aggregation. [<strong><code>det.</code></strong>] ⭐️</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594318">IROS</a>] Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593910">IROS</a>] Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594175">IROS</a>] NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor. [<a href="https://github.com/CTTCGeoLab/VI_ROS">code</a>][<strong><code>oth.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593837">IROS</a>] A 3D Convolutional Neural Network Towards Real-Time Amodal 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594356">IROS</a>] Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models. [<strong><code>seg.</code></strong> <strong><code>rec.</code></strong>]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594514">IROS</a>] PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1711.08241">IROS</a>] [<a href="https://github.com/sitzikbs/3DmFV-Net">Tensorflow</a>]3DmFV: Point Cloud Classification and segmentation for unstructured 3D point clouds. [<strong><code>cls.</code></strong> ]</li><li>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594042">IROS</a>] Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments. [<strong><code>oth.</code></strong> ]</li><li></li><li>[<a href="https://www.mdpi.com/1424-8220/18/10/3337">SENSORS</a>] SECOND: Sparsely Embedded Convolutional Detection. [<a href="https://github.com/traveller59/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li><li></li><li>[<a href="https://arxiv.org/abs/1803.07289">ACCV</a>] Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds). [<a href="https://github.com/cgtuebingen/Flex-Convolution">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1808.00671">3DV</a>] PCN: Point Completion Network. [<a href="https://github.com/TonythePlaneswalker/pcn">tensorflow</a>] [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>] 🔥</li><li></li><li>[<a href="https://arxiv.org/abs/1812.01711">ICASSP</a>] A Graph-CNN for 3D Point Cloud Classification. [<a href="https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification">tensorflow</a>] [<strong><code>cls.</code></strong>] 🔥</li><li></li><li>[<a href="https://arxiv.org/pdf/1805.01195.pdf">ITSC</a>] BirdNet: a 3D Object Detection Framework from LiDAR information. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1807.00652">arXiv</a>] PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation. [<a href="https://github.com/MVIG-SJTU/pointSIFT">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1805.07872">arXiv</a>] Spherical Convolutional Neural Network for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.07605">arXiv</a>] Adversarial Autoencoders for Generating 3D Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.11209">arXiv</a>] Iterative Transformer Network for 3D Point Cloud. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>pos.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.12543">arXiv</a>] Topology-Aware Surface Reconstruction for Point Clouds. [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.01402">arXiv</a>] Inferring Point Clouds from Single Monocular Images by Depth Intermediation. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.04302">arXiv</a>] Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.05276">arXiv</a>] IPOD: Intensive Point-based Object Detector for Point Cloud. [<strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.11383">arXiv</a>] Feature Preserving and Uniformity-controllable Point Cloud Simplification on Graph. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1901.01060">arXiv</a>] POINTCLEANNET: Learning to Denoise and Remove Outliers from Dense Point Clouds. [<a href="https://github.com/mrakotosaon/pointcleannet">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1803.06199">arXiv</a>] Complex-YOLO: Real-time 3D Object Detection on Point Clouds. [<a href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1811.03818">arxiv</a>] RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement. [<a href="https://github.com/Kiwoo/RoarNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.11029">arXiv</a>] Multi-column Point-CNN for Sketch Segmentation. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1810.05591">arXiv</a>] PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention. [<a href="https://liuziwei7.github.io/projects/PointGrow">project</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1810.05795.pdf">arXiv</a>] Point Cloud GAN. [<a href="https://github.com/chunliangli/Point-Cloud-GAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li></ul><hr><h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><ul><li>[<a href="http://export.arxiv.org/abs/1904.07601">CVPR</a>] Relation-Shape Convolutional Neural Network for Point Cloud Analysis. [<a href="https://github.com/Yochengliu/Relation-Shape-CNN">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥</li><li>[<a href="https://raoyongming.github.io/files/SFCNN.pdf">CVPR</a>] Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.11397">CVPR</a>] DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds. [<a href="https://ai4ce.github.io/DeepMapping/">code</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.07179">CVPR</a>] Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving. [<a href="https://github.com/mileyan/pseudo_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.04244">CVPR</a>] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. [<a href="https://github.com/sshaoshuai/PointRCNN">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1809.07016">CVPR</a>] Generating 3D Adversarial Point Clouds. [<a href="https://github.com/xiangchong1/3d-adv-pc">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.03375v1">CVPR</a>] Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://export.arxiv.org/abs/1904.08017">CVPR</a>] A-CNN: Annularly Convolutional Neural Networks on Point Clouds. [<a href="https://github.com/artemkomarichev/a-cnn">tensorflow</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.07246">CVPR</a>] PointConv: Deep Convolutional Networks on 3D Point Clouds. [<a href="https://github.com/DylanWusee/pointconv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1812.11647">CVPR</a>] Path-Invariant Map Networks. [<a href="https://github.com/zaiweizhang/path_invariance_map_network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.02713">CVPR</a>] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding. [<a href="https://github.com/daerduoCarey/partnet_dataset">code</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://export.arxiv.org/abs/1901.00680">CVPR</a>] GeoNet: Deep Geodesic Networks for Point Cloud Analysis. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1902.09852">CVPR</a>] Associatively Segmenting Instances and Semantics in Point Clouds. [<a href="https://github.com/WXinlong/ASIS">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1811.08988">CVPR</a>] Supervised Fitting of Geometric Primitives to 3D Point Clouds. [<a href="https://github.com/csimstu2/SPFN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.00343">CVPR</a>] Octree guided CNN with Spherical Kernels for 3D Point Clouds. [<a href="https://arxiv.org/pdf/1909.09287.pdf">extension</a>] [<a href="https://github.com/hlei-ziyan/SPH3D-GCN">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Point Cloud Registration using PointNet. [<a href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.00699v1">CVPR</a>] JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields. [<a href="https://github.com/pqhieu/JSIS3D">pytorch</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.02113">CVPR</a>] Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.05784">CVPR</a>] PointPillars: Fast Encoders for Object Detection from Point Clouds. [<a href="https://github.com/nutonomy/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1811.11286">CVPR</a>] Patch-based Progressive 3D Point Set Upsampling. [<a href="https://github.com/yifita/3PU">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.09793">CVPR</a>] PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval. [<a href="https://github.com/XLechter/PCAN">code</a>] [<strong><code>rel.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.00709">CVPR</a>] PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. [<a href="https://github.com/FoggYu/PartNet">pytorch</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1806.02170">CVPR</a>] PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds. [<a href="https://github.com/aseembehl/pointflownet">code</a>] [<strong><code>det.</code></strong> <strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.03483">CVPR</a>] SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences. [<a href="https://github.com/intellhave/SDRSAC">matlab</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.04019">CVPR</a>] Deep Reinforcement Learning of Volume-guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.03461">CVPR</a>] Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.10775v1">CVPR</a>] 3D Point-Capsule Networks. [<a href="https://github.com/yongheng1991/3D-point-capsule-networks">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://export.arxiv.org/abs/1904.08755">CVPR</a>] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. [<a href="https://github.com/StanfordVL/MinkowskiEngine">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1811.06879v2">CVPR</a>] The Perfect Match: 3D Point Cloud Matching with Smoothed Densities. [<a href="https://github.com/zgojcic/3DSmoothNet">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.10136">CVPR</a>] FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization. [<a href="https://bitbucket.org/gaowei19951004/poser/src/master/">code</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1806.01411">CVPR</a>] FlowNet3D: Learning Scene Flow in 3D Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.07782">CVPR</a>] Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="http://www.linliang.net/wp-content/uploads/2019/04/CVPR2019_PointClound.pdf">CVPR</a>] ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis. [<strong><code>cls.</code></strong>]</li><li>[<a href="http://jiaya.me/papers/pointweb_cvpr19.pdf">CVPR</a>] PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. [<a href="https://github.com/hszhao/PointWeb">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.12304">CVPR</a>] RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion. [<a href="https://github.com/iSarmad/RL-GAN-Net">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Robust &amp; Efficient Point Cloud Registration using PointNet. [<a href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://www.researchgate.net/publication/332240602_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes">CVPR</a>] Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes. [<a href="https://github.com/ziquan111/RobustPCLReconstruction">code</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.00709">CVPR</a>] Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks. [<a href="https://github.com/sitzikbs/Nesti-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.03320">CVPR</a>] GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf">CVPR</a>] Graph Attention Convolution for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.02050">CVPR</a>] Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [<strong><code>pos.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.08701v1">CVPR</a>] LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1904.03498.pdf">CVPR</a>] LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks. [<a href="https://sites.google.com/view/lp-3dcnn/home">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.pdf">CVPR</a>] Structural Relational Reasoning of Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.03322">CVPR</a>] 3DN: 3D Deformation Network. [<a href="https://github.com/laughtervv/3DN">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.pdf">CVPR</a>] Privacy Preserving Image-Based Localization. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR</a>] Argoverse: 3D Tracking and Forecasting With Rich Maps.[<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Giancola_Leveraging_Shape_Completion_for_3D_Siamese_Tracking_CVPR_2019_paper.pdf">CVPR</a>] Leveraging Shape Completion for 3D Siamese Tracking. [<a href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking">pytorch</a>] [<strong><code>tra.</code></strong> ]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf">CVPRW</a>] Attentional PointNet for 3D-Object Detection in Point Clouds. [<a href="https://github.com/anshulpaigwar/Attentional-PointNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_3D_Local_Features_for_Direct_Pairwise_Registration_CVPR_2019_paper.pdf">CVPR</a>] 3D Local Features for Direct Pairwise Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dovrat_Learning_to_Sample_CVPR_2019_paper.pdf">CVPR</a>] Learning to Sample. [<a href="https://github.com/orendv/learning_to_sample">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pittaluga_Revealing_Scenes_by_Inverting_Structure_From_Motion_Reconstructions_CVPR_2019_paper.pdf">CVPR</a>] Revealing Scenes by Inverting Structure from Motion Reconstructions. [<a href="https://github.com/francescopittaluga/invsfm">code</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf">CVPR</a>] DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image. [<a href="https://github.com/JiaxiongQ/DeepLiDAR">pytorch</a>] [<strong><code>dep.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_HPLFlowNet_Hierarchical_Permutohedral_Lattice_FlowNet_for_Scene_Flow_Estimation_on_CVPR_2019_paper.pdf">CVPR</a>] HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds. [<a href="https://github.com/laoreja/HPLFlowNet">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1904.09664v1">ICCV</a>] Deep Hough Voting for 3D Object Detection in Point Clouds. [<a href="https://github.com/facebookresearch/votenet">pytorch</a>] [<a href="https://github.com/qq456cvb/VoteNet">tensorflow</a>] [<strong><code>det.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1904.03751">ICCV</a>] DeepGCNs: Can GCNs Go as Deep as CNNs? [<a href="https://github.com/lightaime/deep_gcns">tensorflow</a>] <a href="https://github.com/lightaime/deep_gcns_torch">[pytorch]</a> [<strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/pdf/1907.10844.pdf">ICCV</a>] PU-GAN: a Point Cloud Upsampling Adversarial Network. [<a href="https://github.com/liruihui/PU-GAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition. [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1906.12320.pdf">ICCV</a>] PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows. [<a href="https://github.com/stevenygd/PointFlow">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.12704.pdf">ICCV</a>] Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://drive.google.com/file/d/11GJzouV6jt_aOpvrJ8l3J5x_R_-m-Lg8/view">ICCV</a>] SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [<a href="https://github.com/TerenceCYJ/SO-HandNet">code</a>] [<strong><code>pos.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.11017">ICCV</a>] DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1908.04616">ICCV</a>] Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data. [<strong><code>cls.</code></strong> <strong><code>dat.</code></strong>] [<a href="https://github.com/hkust-vgd/scanobjectnn">code</a>] [<a href="https://hkust-vgd.github.io/scanobjectnn/">dataset</a>]</li><li>[<a href="https://arxiv.org/abs/1904.08889">ICCV</a>] KPConv: Flexible and Deformable Convolution for Point Clouds. [<a href="https://github.com/HuguesTHOMAS/KPConv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/pdf/1908.06295.pdf">ICCV</a>] ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics. [<a href="https://hkust-vgd.github.io/shellnet/">project</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1908.04422.pdf">ICCV</a>] Point-Based Multi-View Stereo Network. [<a href="https://github.com/callmeray/PointMVSNet">pytorch</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1909.03669">ICCV</a>] DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing. [<a href="https://github.com/Yochengliu/DensePoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1905.04153v2">ICCV</a>] DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1905.06292.pdf">ICCV</a>] 3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions. [<a href="https://github.com/seowok/TreeGAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1909.10469.pdf">ICCV</a>] Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.html">ICCV</a>] Fully Convolutional Geometric Features. [<a href="https://github.com/chrischoy/FCGF">pytorch</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.pdf">ICCV</a>] Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning. [<a href="https://github.com/phermosilla/TotalDenoising">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.00229">ICCV</a>] USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds. [<a href="https://github.com/lijx10/USIP">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.pdf">ICCV</a>] Interpolated Convolutional Networks for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf">ICCV</a>] PointCloud Saliency Maps. [<a href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.10471.pdf">ICCV</a>] STD: Sparse-to-Dense 3D Object Detector for Point Cloud. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.pdf">ICCV</a>] Accelerated Gravitational Point Set Alignment with Altered Physical Laws. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf">ICCV</a>] Deep Closest Point: Learning Representations for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.pdf">ICCV</a>] Efficient Learning on Point Clouds with Basis Point Sets. [<a href="https://github.com/sergeyprokudin/bps">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.pdf">ICCV</a>] PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling. [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds. [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.pdf">ICCV</a>] Dynamic Points Agglomeration for Hierarchical Point Sets Learning. [<a href="https://github.com/yuyi1005/DPAM">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Unsupervised Multi-Task Feature Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.pdf">ICCV</a>] VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation. [<a href="https://github.com/xianyuMeng/VV-Net-Voxel-VAE-Net-with-Group-Convolutions-for-Point-Cloud-Segmentation">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.pdf">ICCV</a>] GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion. [<a href="https://github.com/justanhduc/graphx-conv">pytorch</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MeteorNet_Deep_Learning_on_Dynamic_3D_Point_Cloud_Sequences_ICCV_2019_paper.pdf">ICCV</a>] MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences. [<a href="https://github.com/xingyul/meteornet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1908.02990">ICCV</a>] Fast Point R-CNN. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.pdf">ICCV</a>] Robust Variational Bayesian Point Set Registration. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf">ICCV</a>] DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.pdf">ICCV</a>] 3D Instance Segmentation via Multi-Task Metric Learning. [<a href="https://sites.google.com/view/3d-instance-mtml">code</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.pdf">ICCV</a>] 3D Face Modeling From Diverse Raw Scan Data. [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1909.12249">ICCVW</a>] Range Adaptation for 3D Object Detection in LiDAR. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/1901.08396.pdf">NeurIPS</a>] Self-Supervised Deep Learning on Point Clouds by Reconstructing Space. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1906.01140">NeurIPS</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://papers.nips.cc/paper/8706-exploiting-local-and-global-structure-for-point-cloud-semantic-segmentation-with-contextual-point-representations.pdf">NeurIPS</a>] Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations. [<a href="https://github.com/fly519/ELGS">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.03739.pdf">NeurIPS</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://papers.nips.cc/paper/8940-pointdan-a-multi-scale-3d-domain-adaption-network-for-point-cloud-representation.pdf">NeurIPS</a>] PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation. [<a href="https://github.com/canqin001/PointDAN">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://openreview.net/forum?id=SJeXSo09FQ">ICLR</a>] Learning Localized Generative Models for 3D Point Clouds via Graph Convolution. [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1905.07290">ICMLW</a>] LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1811.11731">AAAI</a>] CAPNet: Continuous Approximation Projection For 3D Point Cloud Reconstruction Using 2D Supervision. [<a href="https://github.com/val-iisc/capnet">code</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.02565">AAAI</a>] Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network. [<a href="https://github.com/liuxinhai/Point2Sequence">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://par.nsf.gov/biblio/10086163">AAAI</a>] Point Cloud Processing via Recurrent Set Encoding. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.00333">AAAI</a>] PVRNet: Point-View Relation Neural Network for 3D Shape Recognition. [<a href="https://github.com/Hxyou/PVRNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li>[<a href="http://gaoyue.org/paper/HGNN.pdf">AAAI</a>] Hypergraph Neural Networks. [<a href="https://github.com/iMoonLab/HGNN">pytorch</a>] [<strong><code>cls.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1801.07829">TOG</a>] Dynamic Graph CNN for Learning on Point Clouds. [<a href="https://github.com/WangYueFt/dgcnn">tensorflow</a>][<a href="https://github.com/WangYueFt/dgcnn">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li><li>[<a href="https://arxiv.org/pdf/1903.10170.pdf">TOG</a>] LOGAN: Unpaired Shape Transform in Latent Overcomplete Space. [<a href="https://github.com/kangxue/LOGAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://dl.acm.org/doi/10.1145/3355089.3356573">SIGGRAPH Asia</a>] RPM-Net: recurrent prediction of motion and parts from point cloud. [<a href="https://github.com/Salingo/RPM-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1908.00575v1">SIGGRAPH Asia</a>] StructureNet: Hierarchical Graph Networks for 3D Shape Generation. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://dl.acm.org/citation.cfm?id=3343031.3351009">MM</a>] MMJN: Multi-Modal Joint Networks for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li>[<a href="https://dl.acm.org/citation.cfm?id=3351061">MM</a>] 3D Point Cloud Geometry Compression on Deep Learning. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://dl.acm.org/citation.cfm?id=3351042">MM</a>] SRINet: Learning Strictly Rotation-Invariant Representations for Point Cloud Classification and Segmentation. [<a href="https://github.com/tasx0823/SRINet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://dl.acm.org/citation.cfm?id=3350960">MM</a>] L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li>[<a href="https://dl.acm.org/citation.cfm?id=3351076">MM</a>] Ground-Aware Point Cloud Semantic Segmentation for Autonomous Driving. [<a href="https://github.com/Jaiy/Ground-aware-Seg">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1908.08996">ICME</a>] Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By Lookup Tables. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1904.04427">ICASSP</a>] 3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation. [<a href="https://github.com/chaojingduan/Neural-Projection">code</a>] [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1907.06371">BMVC</a>] Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects. [<strong><code>cls.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1904.00319">ICRA</a>] Discrete Rotation Equivariance for Point Cloud Recognition. [<a href="https://github.com/lijx10/rot-equ-net">pytorch</a>] [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1809.08495">ICRA</a>] SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud. [<a href="https://github.com/xuanyuzhou98/SqueezeSegV2">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://www.ais.uni-bonn.de/papers/ICRA_2019_Razlaw.pdf">ICRA</a>] Detection and Tracking of Small Objects in Sparse 3D Laser Range Data. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1905.02553">ICRA</a>] Oriented Point Sampling for Plane Detection in Unorganized Point Clouds. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_1.html">ICRA</a>] Point Cloud Compression for 3D LiDAR Sensor Using Recurrent Neural Network with Residual Blocks. [<a href="https://github.com/ChenxiTU/Point-cloud-compression-by-RNN">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1809.06065">ICRA</a>] Focal Loss in 3D Object Detection. [<a href="https://github.com/pyun-ram/FL3D">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1809.06267">ICRA</a>] PointNetGPD: Detecting Grasp Configurations from Point Sets. [<a href="https://github.com/lianghongzhuo/PointNetGPD">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.09742">ICRA</a>] 2D3D-MatchNet: Learning to Match Keypoints across 2D Image and 3D Point Cloud. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Speeding up Iterative Closest Point Using Stochastic Gradient Descent. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Uncertainty Estimation for Projecting Lidar Points Onto Camera Images for Moving Platforms. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.06405v1">ICRA</a>] BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving. [<a href="https://github.com/VCCIV/BLVD">project</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] A Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications. [<strong><code>det.</code></strong>]</li><li>[<a href="https://arpg.colorado.edu/papers/hmrf_icp.pdf">ICRA</a>] Robust low-overlap 3-D point cloud registration for outlier rejection. [<a href="https://github.com/JStech/ICP">matlab</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust Generalized Point Set Registration Using Inhomogeneous Hybrid Mixture Models Via Expectation. [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1902.07511">ICRA</a>] Dense 3D Visual Mapping via Semantic Simplification. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.01649">ICRA</a>] MVX-Net: Multimodal VoxelNet for 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://export.arxiv.org/abs/1810.01470">ICRA</a>] CELLO-3D: Estimating the Covariance of ICP in the Real World. [<strong><code>reg.</code></strong>]</li><li></li><li>[<a href="https://www.researchgate.net/publication/334720713_EPN_Edge-Aware_PointNet_for_Object_Recognition_from_Multi-View_25D_Point_Clouds">IROS</a>] EPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds. [<a href="https://github.com/Merium88/Edge-Aware-PointNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1904.13030.pdf">IROS</a>] SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles. [<strong><code>oth.</code></strong>] [<strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1909.01643v1.pdf">IROS</a>] PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1906.10964">IV</a>] End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving. [<strong><code>seg.</code></strong>] [<strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1904.02375">Eurographics Workshop</a>] Generalizing Discrete Convolutions for Unstructured Point Clouds. [<a href="https://github.com/aboulch/ConvPoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1811.02191">WACV</a>] 3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds. [<strong><code>cls.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/1908.06297.pdf">3DV</a>] Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. [<a href="https://hkust-vgd.github.io/riconv/">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1906.11555">3DV</a>] Effective Rotation-invariant Point CNN with Spherical Harmonics kernels. [<a href="https://github.com/adrienPoulenard/SPHnet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/1907.13538.pdf">TVCG</a>] LassoNet: Deep Lasso-Selection of 3D Point Clouds. [<a href="https://lassonet.github.io/">project</a>] [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/1901.02532">arXiv</a>] Fast 3D Line Segment Detection From Unorganized Point Cloud. [<strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1812.01687">arXiv</a>] Point-Cloud Saliency Maps. [<a href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://export.arxiv.org/abs/1901.03006">arXiv</a>] Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers. [<a href="https://github.com/Daniel-Liu-c0deb0t/3D-Neural-Network-Adversarial-Attacks">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1901.08396">arxiv</a>] Context Prediction for Unsupervised Deep Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="http://export.arxiv.org/abs/1901.09280">arXiv</a>] Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks. [<strong><code>oth.</code></strong>]</li><li>[<a href="http://export.arxiv.org/abs/1901.09394">arXiv</a>] NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1902.05247">arXiv</a>] 3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1902.10272">arXiv</a>] Zero-shot Learning of 3D Point Cloud Objects. [<a href="https://github.com/alichr/Zero-shot-Learning-of-3D-Point-Cloud-Objects">code</a>] [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.09847">arXiv</a>] Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.01695">arXiv</a>] Real-time Multiple People Hand Localization in 4D Point Clouds. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.02858">arXiv</a>] Variational Graph Methods for Efficient Point Cloud Sparsification. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.05807">arXiv</a>] Neural Style Transfer for Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.07918">arXiv</a>] OREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.10750">arXiv</a>] FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds. [<a href="https://github.com/LordLiang/FVNet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.00069">arXiv</a>] Unpaired Point Cloud Completion on Real Scans using Adversarial Training. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.00230">arXiv</a>] MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.00817">arXiv</a>] DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric Learning on 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.07537">arXiv</a>] Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. [<a href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>] 🔥</li><li>[<a href="https://arxiv.org/abs/1904.10795">arXiv</a>] Graph-based Inpainting for 3D Dynamic Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1903.11027">arXiv</a>] nuScenes: A multimodal dataset for autonomous driving. [<a href="https://www.nuscenes.org/overview">link</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1901.08373">arXiv</a>] 3D Backbone Network for 3D Object Detection. [<a href="https://github.com/Benzlxs/tDBN">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.07605v3">arXiv</a>] Adversarial Autoencoders for Compact Representations of 3D Point Clouds. [<a href="https://github.com/MaciejZamorski/3d-AAE">pytorch</a>] [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1904.10014.pdf">arXiv</a>] Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1905.08705">arXiv</a>] GAPNet: Graph Attention based Point Neural Network for Exploiting Local Feature of Point Cloud. [<a href="https://github.com/FrankCAN/GAPNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1906.01140">arXiv</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://export.arxiv.org/abs/1906.04173">arXiv</a>] Differentiable Surface Splatting for Point-based Geometry Processing. [<a href="https://github.com/yifita/DSS">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1906.10887">arXiv</a>] Spatial Transformer for 3D Points. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1907.03739">arXiv</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1906.08240">arXiv</a>] Neural Point-Based Graphics. [<a href="https://dmitryulyanov.github.io/neural_point_based_graphics">project</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1908.02111.pdf">arXiv</a>] Point Cloud Super Resolution with Adversarial Residual Graph Networks. [<strong><code>oth.</code></strong>] [<a href="https://github.com/wuhuikai/PointCloudSuperResolution">tensorflow</a>]</li><li>[<a href="https://arxiv.org/pdf/1908.10209.pdf">arXiv</a>] Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1908.11069v1.pdf">arXiv</a>] StarNet: Targeted Computation for Object Detection in Point Clouds. [<a href="https://github.com/tensorflow/lingvo">tensorflow</a>] [<strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1903.10168.pdf">arXiv</a>] Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDAR. [<strong><code>tra.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1905.07650v1.pdf">arXiv</a>] SAWNet: A Spatially Aware Deep Neural Network for 3D Point Cloud Processing. [<a href="https://github.com/balwantraikekutte/SAWNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1907.03670">arXiv</a>] Part-A^2 Net: 3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1906.03299.pdf">arXiv</a>] PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding Module for Classification and Segmentation. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1910.08287.pdf">arXiv</a>] PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. [<a href="https://github.com/hehefan/PointRNN">tensorflow</a>] [<strong><code>tra.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1907.09798">arXiv</a>] PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points. [<a href="https://github.com/paul007pl/PointAtrousGraph">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.05279.pdf">arXiv</a>] Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1911.09040.pdf">arXiv</a>] 3D-Rotation-Equivariant Quaternion Neural Networks. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1908.11026.pdf">arXiv</a>] Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1911.12885">arXiv</a>] Geometric Feedback Network for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1912.00202">arXiv</a>] Relation Graph Network for 3D Object Detection in Point Clouds. [<strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.13079.pdf">arXiv</a>] Deformable Filter Convolution for Point Cloud Reasoning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.03264.pdf">arXiv</a>] PU-GCN: Point Cloud Upsampling via Graph Convolutional Network. [<a href="https://sites.google.com/kaust.edu.sa/pugcn">project</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1911.11098.pdf">arXiv</a>] StructEdit: Learning Structural Shape Variations. [<a href="https://github.com/daerduoCarey/structedit">project</a>] [<strong><code>rec.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.02984v1.pdf">arXiv</a>] Grid-GCN for Fast and Scalable Point Cloud Learning. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1911.10150.pdf">arXiv</a>] PointPainting: Sequential Fusion for 3D Object Detection. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.07161.pdf">arXiv</a>] Transductive Zero-Shot Learning for 3D Point Cloud Classification. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.10644.pdf">arXiv</a>] Geometry Sharing Network for 3D Point Cloud Classification and Segmentation. [<a href="https://github.com/MingyeXu/GS-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1912.12033">arvix</a>] Deep Learning for 3D Point Clouds: A Survey. [<a href="https://github.com/QingyongHu/SoTA-Point-Cloud">code</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.01800v1.pdf">arXiv</a>] Spectral-GANs for High-Resolution 3D Point-cloud Generation. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1909.12663.pdf">arXiv</a>] Point Attention Network for Semantic Segmentation of 3D Point Clouds. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1909.07137v1.pdf">arXiv</a>] PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation. [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1904.08159">arXiv</a>] 3D Object Recognition with Ensemble Learning — A Study of Point Cloud-Based Deep Learning Models. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li></ul><hr><h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><ul><li>[<a href="https://arxiv.org/abs/1912.00280">AAAI</a>] Morphing and Sampling Network for Dense Point Cloud Completion. [<a href="https://github.com/Colin97/MSN-Point-Cloud-Completion">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1912.05163.pdf">AAAI</a>] TANet: Robust 3D Object Detection from Point Clouds with Triple Attention. [<a href="https://github.com/happinesslz/TANet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1912.10775">AAAI</a>] Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1811.09361">AAAI</a>] PRIN: Pointwise Rotation-Invariant Network. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1912.00497">CVPR</a>] Just Go with the Flow: Self-Supervised Scene Flow Estimation. [<a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">code</a>][<strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1912.00195">CVPR</a>] SGAS: Sequential Greedy Architecture Search. [<a href="https://github.com/lightaime/sgas">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1911.11236.pdf">CVPR</a>] RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. [<a href="https://github.com/QingyongHu/RandLA-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/2001.05119">CVPR</a>] Learning multiview 3D point cloud registration. [<a href="https://github.com/zgojcic/3D_multiview_reg">code</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2003.00410.pdf">CVPR</a>] PF-Net: Point Fractal Network for 3D Point Cloud Completion. [<a href="https://github.com/zztianzz/PF-Net-Point-Fractal-Network.git">pytorch</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2004.05679.pdf">CVPR</a>] MLCVNet: Multi-Level Context VoteNet for 3D Object Detection. [<a href="https://github.com/NUAAXQ/MLCVNet">code</a>] [<strong><code>det.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_SampleNet_Differentiable_Point_Cloud_Sampling_CVPR_2020_paper.pdf">CVPR</a>] SampleNet: Differentiable Point Cloud Sampling. [<a href="https://github.com/itailang/SampleNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.html">CVPR</a>] MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment.  [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/2005.01014">CVPR</a>] Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences. [<a href="https://github.com/XiaoshuiHuang/fmr">code</a>] [<strong><code>reg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/1907.02545.pdf">CVPR</a>] Attentive Context Normalization for Robust Permutation-Equivariant Learning. [<a href="https://github.com/vcg-uvic/acne">code</a>] [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2003.01456.pdf">CVPR</a>] Implicit Functions in Feature Space for Shape Reconstruction and Completion. [<a href="https://github.com/jchibane/if-net">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2002.10876.pdf">CVPR</a>] PointAugment: an Auto-Augmentation Framework for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/1912.08487.pdf">WACV</a>] FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/2001.10692">arXiv</a>] ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes. [<strong><code>det.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/1912.12098.pdf">ECCV</a>] Quaternion Equivariant Capsule Networks for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2007.10985.pdf">ECCV</a>] PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/2003.10826">ECCV</a>] DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares. [<a href="https://github.com/sitzikbs/DeepFit">code</a>] [<strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/2004.11784v2">ECCV</a>] DPDist: Comparing Point Clouds Using Deep Point Cloud Distance. [<a href="https://github.com/dahliau/DPDist">code</a>] [<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://hal.inria.fr/hal-02927350/document">IROS</a>] GndNet: Fast Ground Plane Estimation and Point Cloud Segmentation for Autonomous Vehicles. [<a href="https://github.com/anshulpaigwar/GndNet">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/pdf/2002.00118.pdf">ICLR</a>] AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing. [<a href="https://github.com/xingzhehe/AdvectiveNet-An-Eulerian-Lagrangian-Fluidic-Reservoir-for-Point-Cloud-Processing">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/2006.04569">arXiv</a>] Parameter-Efficient Person Re-identification in the 3D Space. <a href="https://github.com/layumi/person-reid-3d">[code]</a>[<strong><code>rel.</code></strong>] 🔥</li></ul><h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><ul><li>[<a href="https://openreview.net/pdf?id=O3bqkf_Puys">ICLR</a>] PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li></li><li>[<a href="https://hehefan.github.io/pdfs/p4transformer.pdf">CVPR</a>] Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos. [<a href="https://github.com/hehefan/P4Transformer">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://arxiv.org/pdf/2012.00987">CVPR</a>] PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds. [<a href="https://github.com/weiyithu/PV-RAFT">code</a>][<strong><code>oth.</code></strong>]</li><li></li><li>[<a href="https://arxiv.org/abs/2105.07647">ICRA</a>] FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection. [<a href="https://github.com/weiyithu/FGR">code</a>][<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li></li><li>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hamdi_MVTN_Multi-View_Transformation_Network_for_3D_Shape_Recognition_ICCV_2021_paper.pdf">ICCV</a>] MVTN: Multi-View Transformation Network for 3D Shape Recognition. [<a href="https://github.com/ajhamdi/MVTN">code</a>][<strong><code>det.</code></strong> <strong><code>rel.</code></strong>]</li></ul><h1><figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">- Datasets</span></span><br></pre></td></tr></table></figure></h1><ul><li>[<a href="http://www.cvlibs.net/datasets/kitti/">KITTI</a>] The KITTI Vision Benchmark Suite. [<strong><code>det.</code></strong>]</li><li>[<a href="http://modelnet.cs.princeton.edu/">ModelNet</a>] The Princeton ModelNet . [<strong><code>cls.</code></strong>]</li><li>[<a href="https://www.shapenet.org/">ShapeNet</a>]  A collaborative dataset between researchers at Princeton, Stanford and TTIC. [<strong><code>seg.</code></strong>]</li><li>[<a href="https://shapenet.org/download/parts">PartNet</a>] The PartNet dataset provides fine grained part annotation of objects in ShapeNetCore. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://kevinkaixu.net/projects/partnet.html">PartNet</a>] PartNet benchmark from Nanjing University and National University of Defense Technology. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://buildingparser.stanford.edu/dataset.html#Download">S3DIS</a>] The Stanford Large-Scale 3D Indoor Spaces Dataset. [<strong><code>seg.</code></strong>]</li><li>[<a href="http://www.scan-net.org/">ScanNet</a>] Richly-annotated 3D Reconstructions of Indoor Scenes. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D</a>] The Stanford 3D Scanning Repository. [<strong><code>reg.</code></strong>]</li><li>[<a href="http://staffhome.ecm.uwa.edu.au/~00053650/databases.html">UWA Dataset</a>] . [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>reg.</code></strong>]</li><li>[<a href="http://shape.cs.princeton.edu/benchmark/">Princeton Shape Benchmark</a>] The Princeton Shape Benchmark.</li><li>[<a href="http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml">SYDNEY URBAN OBJECTS DATASET</a>] This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong>]</li><li>[<a href="https://projects.asl.ethz.ch/datasets/doku.php?id=home">ASL Datasets Repository(ETH)</a>] This site is dedicated to provide datasets for the Robotics community with the aim to facilitate result evaluations and comparisons. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong> <strong><code>reg.</code></strong> <strong><code>det</code></strong>]</li><li>[<a href="http://www.semantic3d.net/">Large-Scale Point Cloud Classification Benchmark(ETH)</a>] This benchmark closes the gap and provides a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total. [<strong><code>cls.</code></strong>]</li><li>[<a href="http://asrl.utias.utoronto.ca/datasets/3dmap/">Robotic 3D Scan Repository</a>] The Canadian Planetary Emulation Terrain 3D Mapping Dataset is a collection of three-dimensional laser scans gathered at two unique planetary analogue rover test facilities in Canada.</li><li>[<a href="http://radish.sourceforge.net/">Radish</a>] The Robotics Data Set Repository (Radish for short) provides a collection of standard robotics data sets.</li><li>[<a href="http://data.ign.fr/benchmarks/UrbanAnalysis/#">IQmulus &amp; TerraMobilita Contest</a>] The database contains 3D MLS data from a dense urban environment in Paris (France), composed of 300 million points. The acquisition was made in January 2013. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/">Oakland 3-D Point Cloud Dataset</a>] This repository contains labeled 3-D point cloud laser data collected from a moving platform in a urban environment.</li><li>[<a href="http://kos.informatik.uni-osnabrueck.de/3Dscans/">Robotic 3D Scan Repository</a>] This repository provides 3D point clouds from robotic experiments，log files of robot runs and standard 3D data sets for the robotics community.</li><li>[<a href="http://robots.engin.umich.edu/SoftwareData/Ford">Ford Campus Vision and Lidar Data Set</a>] The dataset is collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck.</li><li>[<a href="https://cs.stanford.edu/people/teichman/stc/">The Stanford Track Collection</a>] This dataset contains about 14,000 labeled tracks of objects as observed in natural street scenes by a Velodyne HDL-64E S2 LIDAR.</li><li>[<a href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL3D+</a>] Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild. [<strong><code>pos.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="https://www.kaggle.com/daavoo/3d-mnist">3D MNIST</a>] The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition. [<strong><code>cls.</code></strong>]</li><li>[<a href="http://wad.ai/2019/challenge.html">WAD</a>] [<a href="http://apolloscape.auto/tracking.html">ApolloScape</a>] The datasets are provided by Baidu Inc. [<strong><code>tra.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li><li>[<a href="https://d3u7q4379vrm7e.cloudfront.net/object-detection">nuScenes</a>] The nuScenes dataset is a large-scale autonomous driving dataset.</li><li>[<a href="https://uwaterloo.ca/waterloo-intelligent-systems-engineering-lab/projects/precise-synthetic-image-and-lidar-presil-dataset-autonomous">PreSIL</a>] Depth information, semantic segmentation (images), point-wise segmentation (point clouds), ground point labels (point clouds), and detailed annotations for all vehicles and people. [<a href="https://arxiv.org/abs/1905.00160">paper</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://3dmatch.cs.princeton.edu/">3D Match</a>] Keypoint Matching Benchmark, Geometric Registration Benchmark, RGB-D Reconstruction Datasets. [<strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://github.com/VCCIV/BLVD">BLVD</a>] (a) 3D detection, (b) 4D tracking, (c) 5D interactive event recognition and (d) 5D intention prediction. [<a href="https://arxiv.org/abs/1903.06405v1">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li><li>[<a href="https://arxiv.org/abs/1809.03605">PedX</a>] 3D Pose Estimation of Pedestrians, more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. [<a href="https://arxiv.org/abs/1809.03605">ICRA 2019 paper</a>] [<strong><code>pos.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://usa.honda-ri.com/H3D">H3D</a>] Full-surround 3D multi-object detection and tracking dataset. [<a href="https://arxiv.org/abs/1903.01568">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li><a href="https://www.argoverse.org/">[Argoverse BY ARGO AI]</a> Two public datasets (3D Tracking and Motion Forecasting) supported by highly detailed maps to test, experiment, and teach self-driving vehicles how to understand the world around them.[<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR 2019 paper</a>][<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://niessner.github.io/Matterport/">Matterport3D</a>] RGB-D: 10,800 panoramic views from 194,400 RGB-D images. Annotations: surface reconstructions, camera poses, and 2D and 3D semantic segmentations. Keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and scene classification. [<a href="https://arxiv.org/abs/1709.06158">3DV 2017 paper</a>] [<a href="https://github.com/niessner/Matterport">code</a>] [<a href="https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/">blog</a>]</li><li>[<a href="https://arxiv.org/abs/1907.04758">SynthCity</a>] SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Nine categories. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="https://level5.lyft.com/dataset/?source=post_page">Lyft Level 5</a>] Include high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://semantic-kitti.org/">SemanticKITTI</a>] Sequential Semantic Segmentation, 28 classes, for autonomous driving. All sequences of KITTI odometry labeled. [<a href="https://arxiv.org/abs/1904.01416">ICCV 2019 paper</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li><li>[<a href="http://npm3d.fr/paris-lille-3d">NPM3D</a>] The Paris-Lille-3D  has been produced by a Mobile Laser System (MLS) in two different cities in France (Paris and Lille). [<strong><code>seg.</code></strong>]</li><li>[<a href="https://waymo.com/open/">The Waymo Open Dataset</a>] The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. [<strong><code>det.</code></strong>]</li><li>[<a href="https://github.com/I2RDL2/ASTAR-3D">A*3D: An Autonomous Driving Dataset in Challeging Environments</a>] A*3D: An Autonomous Driving Dataset in Challeging Environments. [<strong><code>det.</code></strong>]</li><li>[<a href="https://github.com/canqin001/PointDAN">PointDA-10 Dataset</a>] Domain Adaptation for point clouds.</li><li>[<a href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford Robotcar</a>] The dataset captures many different combinations of weather, traffic and pedestrians. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li><li>[<a href="https://scale.com/open-datasets/pandaset">PandaSet</a>] Public large-scale dataset for autonomous driving provided by Hesai &amp; Scale. It enables researchers to study challenging urban driving situations using the full sensor suit of a real self-driving-car. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li><li>[<a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset">3D-FRONT</a> <a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future">3D-FUTURE</a>] [Alibaba] 3D-FRONT contains 10,000 houses (or apartments) and ~70,000 rooms with layout information. 3D-FUTURE contains 20,000+ clean and realistic synthetic scenes in 5,000+ diverse rooms which contain 10,000+ unique high quality 3D instances of furniture.</li><li>[<a href="https://3d.dataset.site/">Campus3D</a>] The Campus3D contains a photogrametry point cloud which has 931.7 million points, covering 1.58 km2 of 6 connected campus regions of NUS. The dataset are point-wisely annotated with a hierarchical structure of 24 semantic labels and contains 2,530 instances based on the labels. [<a href="https://arxiv.org/pdf/2008.04968.pdf">MM 2020 paper</a>][<a href="https://github.com/shinke-li/Campus3D">code</a>][ <strong><code>det.</code></strong> <strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li></ul><h1 id="参考来源"><a href="#参考来源" class="headerlink" title="参考来源"></a>参考来源</h1><p><a href="https://cvpr2021.thecvf.com/">https://cvpr2021.thecvf.com/</a><br><a href="https://cvpr2022.thecvf.com/">https://cvpr2022.thecvf.com/</a></p><p>论文与code查询网站：<a href="https://paperswithcode.com/">https://paperswithcode.com/</a><br>AI论文查询地址：<a href="https://arxiv.org/list/cs.AI/recent">https://arxiv.org/list/cs.AI/recent</a></p><p>论文：<a href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation">https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation</a>综述：<a href="https://github.com/extreme-assistant/survey-computer-vision-2020">https://github.com/extreme-assistant/survey-computer-vision-2020</a>\</p><ul><li>推荐阅读：<code>&lt;br&gt;</code><ul><li><a href="https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation">ICCV2021&#x2F;2019&#x2F;2017 论文&#x2F;代码&#x2F;解读&#x2F;直播合集</a></li><li><a href="https://github.com/extreme-assistant/survey-computer-vision">2020-2021年计算机视觉综述论文汇总</a><code>&lt;br&gt;</code></li><li><a href="https://github.com/extreme-assistant/Awesome-CV-Team">国内外优秀的计算机视觉团队汇总</a></li></ul></li></ul><hr><h1 id="cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017（Papers-x2F-Codes-x2F-Project-x2F-Paper-reading）"><a href="#cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017（Papers-x2F-Codes-x2F-Project-x2F-Paper-reading）" class="headerlink" title="cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017（Papers&#x2F;Codes&#x2F;Project&#x2F;Paper reading）"></a>cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017（Papers&#x2F;Codes&#x2F;Project&#x2F;Paper reading）</h1><p>论文解读汇总：<a href="https://bbs.cvmart.net/articles/3031">https://bbs.cvmart.net/articles/3031</a> <code>&lt;br&gt;</code><br>论文分类汇总：<a href="https://bbs.cvmart.net/articles/4267%60">https://bbs.cvmart.net/articles/4267`</a><br><br><code> 2000~2020年历届CVPR最佳论文代码，解读等汇总：http://bbs.cvmart.net/topics/665/CVPR-Best-Paper</code><br><code> </code><br>&#96;</p><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p><a href="#8">8. CVPR2021最新信息及论文下载</a><code>&lt;br&gt;</code><br><a href="#7">7. CVPR2021论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#6">6. CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播</a><code>&lt;br&gt;</code><br><a href="#5">5. CVPR2020论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#4">4. CVPR2019全部论文下载&#x2F;开源代码</a><code>&lt;br&gt;</code><br><a href="#3">3. CVPR2019论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#2">2. CVPR2019论文直播分享</a><code>&lt;br&gt;</code><br><a href="#1">1. CVPR2018&#x2F;CVPR2017</a><code>&lt;br&gt;</code></p><br><a name="8"><h1 id="8-CVPR2021最新论文分类汇总-持续更新"><a href="#8-CVPR2021最新论文分类汇总-持续更新" class="headerlink" title="8.CVPR2021最新论文分类汇总(持续更新)"></a><a href>8.CVPR2021最新论文分类汇总(持续更新)</a></h1><ul><li><a href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation/blob/master/CVPR2021.md">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReading／Demos&#x2F;直播分享／论文分享会等</a><code>&lt;br&gt;</code><ul><li><a href="https://pan.baidu.com/s/1TWPkRukz9JC4Br-g_Ws5OA">CVPR2021全部论文下载（共1661篇）</a> 提取码：su7e</li></ul></li><li><a href="https://bbs.cvmart.net/articles/4368">CVPR2021 论文解读汇总 + 技术直播汇总</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/articles/4366">CVPR2021 Oral论文汇总&#x2F;解读</a><code>&lt;br&gt;</code></li></ul><br><a name="7"><h1 id="7-CVPR2021论文分方向盘点-lt-br-gt"><a href="#7-CVPR2021论文分方向盘点-lt-br-gt" class="headerlink" title="7.CVPR2021论文分方向盘点&lt;br&gt;"></a><a href>7.CVPR2021论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul><li><a href="https://mp.weixin.qq.com/s/Ho7qtrpF9FhHGaamkQo6Lw">一文看尽CVPR2021 2D 目标检测论文（27篇）</a></li><li><a href="https://mp.weixin.qq.com/s/ysfwYQ3sVvXINPzBR91S7A">一文看尽CVPR2021 图像异常检测论文（6篇）</a></li><li><a href="https://mp.weixin.qq.com/s/w1jPD2AbxnENUBgfdFLFSg">一文看尽CVPR2021 伪装目标检测+旋转目标检测论文（6篇）</a></li><li><a href="https://mp.weixin.qq.com/s/_a0UmZSSxvVUFUGOnMrMhw">CVPR2021 论文大盘点：全景分割论文汇总（共15篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5832">CVPR2021 论文大盘点：人员重识别汇总（共26篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5831">CVPR2021 论文大盘点：行人技术汇总（共7篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5829">CVPR2021 论文大盘点：医学影像汇总（共22篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5560">CVPR2021 论文大盘点：超分辨率汇总（共32篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5824">CVPR2021 论文大盘点：图像修复汇总（共20篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5828">CVPR2021 论文大盘点：图像去噪汇总（共14篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5827">CVPR2021 论文大盘点：去雾去模糊汇总（共14篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5826">CVPR2021 论文大盘点：图像视频去雨汇总（共10篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5562">CVPR2021 论文大盘点：文本图像汇总（共17篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5811">CVPR2021 论文大盘点：人脸识别汇总（共15篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5810">CVPR2021 论文大盘点：人脸造假检测汇总（共9篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5809">CVPR2021 论文大盘点：图像压缩汇总（共5篇）</a></li><li><a href="https://bbs.cvmart.net/articles/5830">CVPR2021 论文大盘点：遥感与航拍影像汇总（共7篇）</a></li></ul><br><a name="6"><h1 id="6-CVPR2020论文下载-x2F-代码-x2F-解读-x2F-直播"><a href="#6-CVPR2020论文下载-x2F-代码-x2F-解读-x2F-直播" class="headerlink" title="6.CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播"></a><a href>6.CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播</a></h1><ul><li><a href="https://github.com/extreme-assistant/cvpr2020/blob/master/CVPR2020.md#cvpr2020%E6%9C%80%E6%96%B0%E4%BF%A1%E6%81%AF%E5%8F%8A%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E8%B4%B4paperscodesprojectpaperreadingdemos%E7%9B%B4%E6%92%AD%E5%88%86%E4%BA%AB%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AD%89">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReading／Demos&#x2F;直播分享／论文分享会等</a><code>&lt;br&gt;</code></li><li><a href="https://pan.baidu.com/s/1UXW6iviZ_d3wpdujNgWJSQ">CVPR2020全部论文下载（共1467篇）</a><code>&lt;br&gt;</code><br>提取码：pun7<code>&lt;br&gt;&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/articles/3031">CVPR2020 论文解读汇总 + 技术直播汇总</a><code>&lt;br&gt;</code></li></ul><br><a name="5"><h1 id="5-CVPR2020论文分方向盘点-lt-br-gt"><a href="#5-CVPR2020论文分方向盘点-lt-br-gt" class="headerlink" title="5.CVPR2020论文分方向盘点&lt;br&gt;"></a><a href>5.CVPR2020论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul><li><a href="https://bbs.cvmart.net/topics/3028">20.CVPR 2020 论文大盘点-动作检测与动作分割（13篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/3000">19.CVPR 2020 论文大盘点-动作识别（21篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2992">18.CVPR 2020 论文大盘点-光流（12篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2964">17.CVPR 2020 论文大盘点-图像与视频检索（16篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2953">16.CVPR 2020 论文大盘点-遥感与航拍影像处理识别（18篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2923">15.CVPR 2020 论文大盘点-图像质量评价（7篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2903">14.CVPR 2020 论文大盘点-图像修复 Inpainting （7篇）</a> <code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2902">13.CVPR 2020 论文大盘点-图像增强与图像恢复（22篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2876">12.CVPR 2020 论文大盘点-去雨去雾去模糊（8篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2855">11.CVPR 2020 论文大盘点-医学影像处理识别（19篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2854">10.CVPR 2020 论文大盘点-抠图 Matting （3篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2829">9.CVPR 2020 论文大盘点-图像分割（25篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2818">8.CVPR 2020 论文大盘点-全景分割与视频目标分割（8篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2725">7.CVPR 2020 论文大盘点-超分辨（21篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2732">6.CVPR 2020 论文大盘点-目标检测（64篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2720">5.CVPR 2020 论文大盘点-人脸技术（64篇</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2733">4.CVPR 2020 论文大盘点-目标跟踪（33篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2778">3.CVPR 2020 论文大盘点-文本图像（16篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2751">2.CVPR 2020 论文大盘点-行人检测与重识别（33篇）</a><code>&lt;br&gt;</code></li><li><a href="https://bbs.cvmart.net/topics/2806">1.CVPR 2020 论文大盘点-实例分割（18篇）</a><code>&lt;br&gt;&lt;br&gt;</code></li></ul><p><code>&lt;br&gt;&lt;br&gt;</code></p><br><a name="4"><h1 id="4-CVPR2019全部论下载-x2F-开源代码-lt-br-gt"><a href="#4-CVPR2019全部论下载-x2F-开源代码-lt-br-gt" class="headerlink" title="4.CVPR2019全部论下载&#x2F;开源代码&lt;br&gt;"></a><a href>4.CVPR2019全部论下载&#x2F;开源代码<code>&lt;br&gt;</code></a></h1><p><a href>全部1294篇<code>&lt;br&gt;</code></a></p><ul><li><a href>全部链接：http://openaccess.thecvf.com/CVPR2019.py <code>&lt;br&gt;</code></a></li><li><a href>下载链接:<code>&lt;br&gt;</code><br>链接:https://pan.baidu.com/s/1dhXrWFHeKeJ1kFsKBxQzVg  密码:f53l</a></li><li><a href="https://github.com/extreme-assistant/cvpr2019/blob/master/cvpr_2019_githublinks.csv">CVPR 2019全部论文开源源码汇总Excel点这里</a></li></ul><p><code>&lt;br&gt;&lt;br&gt;</code></p><br><a name="3"><h1 id="3-CVPR2019论文分方向盘点-lt-br-gt"><a href="#3-CVPR2019论文分方向盘点-lt-br-gt" class="headerlink" title="3.CVPR2019论文分方向盘点&lt;br&gt;"></a><a href>3.CVPR2019论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul><li><a href="http://bbs.cvmart.net/articles/523/cvpr-2019-lun-wen-da-pan-dian-mu-biao-gen-zong-pian">CVPR 2019 论文大盘点-目标跟踪篇</a><code>&lt;br&gt;</code></li><li><a href="http://bbs.cvmart.net/topics/452/cvpr-2019-lun-wen-da-pan-dian-chao-fen-bian-lv-pian">CVPR 2019 论文大盘点-超分辨率篇</a><code>&lt;br&gt;</code></li><li><a href="http://bbs.cvmart.net/topics/451/cvpr-2019-lun-wen-da-pan-dian-ren-lian-ji-shu-pian">CVPR 2019 论文大盘点-人脸技术篇</a><code>&lt;br&gt;</code></li><li><a href="https://mp.weixin.qq.com/s/l8Cfi3CIt2gqVC9i3LV6hw">CVPR 2019 论文大盘点—目标检测篇</a><code>&lt;br&gt;</code></li><li><a href="http://bbs.cvmart.net/topics/535/CVPR2019-Text">CVPR 2019 论文大盘点—文本图像篇</a><code>&lt;br&gt;</code></li><li><a href="http://bbs.cvmart.net/topics/464/cvpr-2019-gong-bu-mo-xing-jian-zhi-lun-wen-hui-zong">CVPR2019模型剪枝论文汇总</a><code>&lt;br&gt;&lt;br&gt;</code></li></ul><br><a name="2"><h1 id="2-CVPR2019论文直播分享-lt-br-gt"><a href="#2-CVPR2019论文直播分享-lt-br-gt" class="headerlink" title="2.CVPR2019论文直播分享&lt;br&gt;"></a><a href>2.CVPR2019论文直播分享<code>&lt;br&gt;</code></a></h1><ul><li><a href="http://bbs.cvmart.net/topics/609/CVPR-2019">微软亚研院CVPR2019线下分享会视频回放及PPT下载</a></li><li>3&#x2F;28晚点云分割分享回放<code>&lt;br&gt;</code><a href="http://bbs.cvmart.net/topics/351/%E8%81%94%E5%90%88%E5%88%86%E5%89%B2%E7%82%B9%E4%BA%91%E4%B8%AD%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%92%8C%E8%AF%AD%E4%B9%89">王鑫龙：联合分割点云中的实例和语义（开源，列表id 27)</a><code>&lt;br&gt;</code></li><li>4月18日晚目标检测分享回放<code>&lt;br&gt;</code><br><a href="https://mp.weixin.qq.com/s/CvzFG63c1bTuWFSIzNSxBA">CMU诸宸辰:基于Anchor-free特征选择模块的单阶目标检测(CVPR2019，列表id 88)</a> <code>&lt;br&gt;</code></li><li>5月9日晚单目标跟踪分享回放<code>&lt;br&gt;</code><a href="https://mp.weixin.qq.com/s/3vlVXQDh6ou8Gdhg4xY2Tg">张志鹏:基于siamese网络的单目标跟踪(CVPR2019 Oral，列表id 65)</a><code>&lt;br&gt;</code></li><li>[5月30日晚人脸识别分享回放<code>&lt;br&gt;</code><a href="https://mp.weixin.qq.com/s/SIHFTbDc_XjbfYfpgwNYeQ">邓健康-CVPR2019:ArcFace 构建高效的人脸识别系统(CVPR2019，列表id 243)</a>：<code>&lt;br&gt;</code></li><li>6月13日晚三维多人多视角姿态识别分享回放<code>&lt;br&gt;</code><br><a href="https://mp.weixin.qq.com/s/Td510LMs3UWV_8d5kDgFYw">董峻廷：多视角下多人三维姿态估计 CVPR2019，列表id 106</a><code>&lt;br&gt;</code></li></ul><br><a name="1"><h1 id="1-CVPR2018-x2F-CVPR2017-lt-br-gt"><a href="#1-CVPR2018-x2F-CVPR2017-lt-br-gt" class="headerlink" title="1.CVPR2018&#x2F;CVPR2017&lt;br&gt;"></a><a href>1.CVPR2018&#x2F;CVPR2017<code>&lt;br&gt;</code></a></h1><ul><li><a href>CVPR 2018全部论文下载百度云链接：https://pan.baidu.com/s/1bhYzNz2TGijUdfPIdyEGtg <code>&lt;br&gt;</code> 密码:gyk2</a></li><li><a href="http://bbs.cvmart.net/articles/56/cvpr-2018-lun-wen-jie-du-ji-jin-190326-geng-xin"><strong>CVPR 2018论文解读汇总</strong></a></li><li>CVPR 2017全部论文下载百度云链接：<a href="https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w">https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w</a>  <code>&lt;br&gt;</code> 密码:o6tu</li><li><a href="https://zhuanlan.zhihu.com/p/27651707"><strong>CVPR 2017论文解读集锦</strong></a></li></ul><h3 id="参考链接-lt-br-gt"><a href="#参考链接-lt-br-gt" class="headerlink" title="参考链接&lt;br&gt;"></a>参考链接<code>&lt;br&gt;</code></h3><ul><li><a href="https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A">https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A</a> <code>&lt;br&gt;</code></li><li><a href="https://github.com/hoya012/CVPR-2019-Paper-Statistics">https://github.com/hoya012/CVPR-2019-Paper-Statistics</a> <code>&lt;br&gt;</code></li><li><a href="https://github.com/jonahthelion/cvpr_with_code">https://github.com/jonahthelion/cvpr_with_code</a> <code>&lt;br&gt;</code></li><li><a href="https://github.com/amusi/daily-paper-computer-vision">https://github.com/amusi/daily-paper-computer-vision</a> <code>&lt;br&gt;&lt;br&gt;</code></li></ul><h1 id="NLPs"><a href="#NLPs" class="headerlink" title="NLPs"></a>NLPs</h1><p>Deep learning speech learning library</p><p>Py2neo 手册<br>Py2neo是一个客户端库和工具包，用于从Python应用程序和命令行中使用Neo4j 。该库支持 Bolt 和 HTTP，并提供高级 API、OGM、管理工具、交互式控制台、Pygments 的 Cypher 词法分析器以及许多其他花里胡哨。从版本 2021.1 开始，Py2neo 包含对路由的完全支持，正如 Neo4j 集群所公开的那样。这可以使用neo4j:&#x2F;&#x2F;…URI 或传递routing&#x3D;True给Graph构造函数来启用。<a href="https://py2neo.org/2021.1/">https://py2neo.org/2021.1/</a></p><h1 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h1><p>中文、英文NER、英汉机器翻译数据集。中英文实体识别数据集，中英文机器翻译数据集，中文分词数据集：<a href="https://github.com/quincyliang/nlp-public-dataset">https://github.com/quincyliang/nlp-public-dataset</a></p><p>CTB词性标注集</p><p><img src="https://user-images.githubusercontent.com/36963108/170655243-6267fbc4-246d-40f0-8303-7c97a934918a.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/36963108/170656452-738fbb77-03bc-4315-a3fb-d14a50a9b5f6.png" alt="image"></p><p>ck: <a href="https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173">https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173</a></p><p>标注标签说明：<a href="https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf">https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf</a></p><p><a href="https://blog.csdn.net/qq_40332976/article/details/120331450">https://blog.csdn.net/qq_40332976/article/details/120331450</a></p><h1 id="资料汇总"><a href="#资料汇总" class="headerlink" title="资料汇总"></a>资料汇总</h1><p>一个轻量级、简单易用的 RNN 唤醒词监听器: <a href="https://github.com/MycroftAI/mycroft-precise">https://github.com/MycroftAI/mycroft-precise</a></p><p>zh:<a href="http://fancyerii.github.io/books/mycroft-precise/">http://fancyerii.github.io/books/mycroft-precise/</a></p><p>基于树莓派的人工智能小车，实现识别、提示、智能旅游线路、离线图像:<br><a href="https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi">https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi</a></p><p>中文NLP数据集:<a href="https://github.com/CLUEbenchmark/CLUEDatasetSearch">https://github.com/CLUEbenchmark/CLUEDatasetSearch</a></p><p>模型：<a href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p><p>中文 NLP 资源精选列表 中文自然语言处理相关资料:<br><a href="https://github.com/crownpku/Awesome-Chinese-NLP">https://github.com/crownpku/Awesome-Chinese-NLP</a></p><p>视觉聊天机器人:<a href="https://paperswithcode.com/paper/visual-dialog">https://paperswithcode.com/paper/visual-dialog</a></p><p>Bert&#x2F;Transformer模型压缩与优化加速: <a href="https://blog.csdn.net/nature553863/article/details/120292394%EF%BC%9A">https://blog.csdn.net/nature553863/article/details/120292394：</a></p><p>可以压缩 BERT 的所有方式：<a href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html">http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html</a><br><a href="https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html">https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html</a></p><p>BERT轻量化探索—模型剪枝（BERT Pruning）—Rasa维度剪枝:<a href="https://blog.csdn.net/ai_1046067944/article/details/103609152">https://blog.csdn.net/ai_1046067944/article/details/103609152</a></p><p>压缩 BERT 以加快预测速度:<a href="https://rasa.com/blog/compressing-bert-for-faster-prediction-2/">https://rasa.com/blog/compressing-bert-for-faster-prediction-2/</a></p><p>论文综述与BERT相关最新论文:<a href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></p><p>中文自然语言排行榜及论文查询:<a href="https://www.cluebenchmarks.com/index.html">https://www.cluebenchmarks.com/index.html</a></p><p>计算语言学国际会议论文集:<a href="https://aclanthology.org/volumes/2020.coling-main/">https://aclanthology.org/volumes/2020.coling-main/</a></p><p>计算语言学协会第 58 届年会论文集:<a href="https://aclanthology.org/volumes/2020.acl-main/">https://aclanthology.org/volumes/2020.acl-main/</a></p><p>计算语言学2协会2021年会论文搜集：<a href="https://aclanthology.org/events/acl-2021/">https://aclanthology.org/events/acl-2021/</a></p><p>中文BERT全词掩蔽预训练（中文BERT-wwm系列模型）<a href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>一个大规模的中文跨领域面向任务的对话数据集:<a href="https://github.com/thu-coai/CrossWOZ">https://github.com/thu-coai/CrossWOZ</a></p><p>关于ConvLab-2：用于构建、评估和诊断对话系统的开源工具包（支持中文）：<a href="https://github.com/thu-coai/ConvLab-2">https://github.com/thu-coai/ConvLab-2</a></p><p>视觉和语言预训练模型 (VL-PTM) 的最新进展(语音视觉融合):<a href="https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers">https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers</a></p><p>深度学习和自然语言处理阅读清单:<a href="https://github.com/IsaacChanghau/DL-NLP-Readings">https://github.com/IsaacChanghau/DL-NLP-Readings</a></p><p>视觉问答 (VQA)（图像&#x2F;视频问答）、视觉问题生成、视觉对话、视觉常识推理和相关领域的精选列表：<a href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></p><p>汇总得不错的nlp学习资料:<a href="https://jackkuo666.github.io/">https://jackkuo666.github.io/</a></p><p>dl4nlp自然语言处理深度学习课程材料:<a href="https://github.com/liu-nlp/dl4nlp">https://github.com/liu-nlp/dl4nlp</a></p><p>论文与数据集网站：<a href="https://www.ai2news.com/area/">https://www.ai2news.com/area/</a></p><p>HanLP的Python接口，支持自动下载与升级HanLP，兼容py2、py3。内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版，欢迎查阅随书代码:<a href="https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1">https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1</a></p><h1 id="pyhanlp文本训练与预测API接口"><a href="#pyhanlp文本训练与预测API接口" class="headerlink" title="pyhanlp文本训练与预测API接口"></a>pyhanlp文本训练与预测API接口</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tests.test_utility <span class="keyword">import</span> ensure_data</span><br><span class="line"></span><br><span class="line">IClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.IClassifier&#x27;</span>)</span><br><span class="line">NaiveBayesClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier&#x27;</span>)</span><br><span class="line"><span class="comment"># 中文情感挖掘语料-ChnSentiCorp 数据来自：</span></span><br><span class="line">chn_senti_corp = ensure_data(<span class="string">&quot;ChnSentiCorp情感分析酒店评论&quot;</span>, <span class="string">&quot;http://file.hankcs.com/corpus/ChnSentiCorp.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">classifier, text</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;《%s》 情感极性是 【%s】&quot;</span> % (text, classifier.classify(text)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = NaiveBayesClassifier()</span><br><span class="line">    <span class="comment">#  创建分类器，更高级的功能请参考IClassifier的接口定义</span></span><br><span class="line">    classifier.train(chn_senti_corp)</span><br><span class="line">    <span class="comment">#  训练后的模型支持持久化，下次就不必训练了</span></span><br><span class="line">    predict(classifier, <span class="string">&quot;前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;结果大失所望，灯光昏暗，空间极其狭小，床垫质量恶劣，房间还伴着一股霉味。&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;可利用文本分类实现情感分析，效果不是不行&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>原创来自这里:<a href="https://www.jianshu.com/p/0a131e042238">https://www.jianshu.com/p/0a131e042238</a></p><p><img src="https://user-images.githubusercontent.com/36963108/188380358-f15635f6-7ff8-4467-9aeb-88ecfef0acaa.png" alt="image"></p><p>好东西：<a href="https://github.com/Kyubyong/nlp_tasks">https://github.com/Kyubyong/nlp_tasks</a></p><p><a href="https://github.com/songyingxin/NLPer-Interview">https://github.com/songyingxin/NLPer-Interview</a></p><p>总结梳理自然语言处理工程师(NLP)需要积累的各方面知识，包括面试题，各种基础知识，工程能力等等，提升核心竞争力 <a href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p><p>史上最全Transformer面试题<br>答案解析(1)-史上最全Transformer面试题<br>Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点<br>解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数(附Pytorch代码详细解读)<br>3分钟从零解读Transformer的Encoder<br>原版Transformer的位置编码究竟有没有包含相对位置信息<br>BN踩坑记–谈一下Batch Normalization的优缺点和适用场景<br>谈一下相对位置编码<br>NLP任务中-layer-norm比BatchNorm好在哪里<br>谈一谈Decoder模块<br>Transformer的并行化<br>Transformer全部文章合辑<br>RNN的梯度消失有什么与众不同的地方.md<br>VIT-如何将Transformer更好的应用到CV领域</p><p>好书：<a href="https://github.com/FudanNLP/nlp-beginner">https://github.com/FudanNLP/nlp-beginner</a></p><p>论文与code:<a href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p><p>跟踪自然语言处理的进展:<a href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p><p>元研究：<a href="https://research.facebook.com/research-areas/">https://research.facebook.com/research-areas/</a></p><p>此项目是机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现，也是作为一个算法工程师必会的理论基础知识。<a href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p><p><a href="https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb">https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb</a></p><p>NLP-Models-Tensorflow  <a href="https://github.com/huseinzol05/NLP-Models-Tensorflow">https://github.com/huseinzol05/NLP-Models-Tensorflow</a></p><p>相似度 <a href="https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md">https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md</a></p><p><a href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p><p><a href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p><p><a href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p><p><a href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p><p><a href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p><p><a href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></p><p><a href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p><p>NLP 领域经典书籍《Speech and Language Processing》第三版   <a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p><p>项目是机器学习（Machine Learning）、深度学习（Deep Learning）、NLP面试中常考到的知识点和代码实现，也是一个算法工程师会必选的理论基础知识 <a href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p><p>NLP以及相关的学习实践 <a href="https://github.com/jarvisqi/machine_learning">https://github.com/jarvisqi/machine_learning</a></p><p>机器学习&amp;深入学习资料笔记&amp;基本算法实现&amp;资源整理（ML &#x2F; CV &#x2F; NLP &#x2F; DM…）<a href="https://github.com/fire717/Machine-Learning">https://github.com/fire717/Machine-Learning</a></p><p>Datawhale成员整理的面经内容，包括机器学习，CV，NLP，推荐 <a href="https://github.com/datawhalechina/daily-interview">https://github.com/datawhalechina/daily-interview</a></p><p>人工智能实战大学（面试）学习路线图  <a href="https://github.com/tangyudi/Ai-Learn">https://github.com/tangyudi/Ai-Learn</a></p><p>2018&#x2F;2019&#x2F;校招笔记&#x2F;春招&#x2F;秋招&#x2F;自然者语言处理(NLP)&#x2F;深度机器学习(深度学习)&#x2F;学习(机器学习) <a href="https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese">https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese</a></p><p>深度学习算法教程</p><ul><li><a href="https://github.com/KeKe-Li/tutorial/tree/master">https://github.com/KeKe-Li/tutorial/tree/master</a></li><li><a href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li></ul><p>深度学习100例、深度识别学习、图片分类、目标、目标检测、自然语言处理nlp、文本分类、TensorFlow、PyTorch  <a href="https://github.com/kzbkzb/Python-AI">https://github.com/kzbkzb/Python-AI</a></p><h1 id="pyhanlp句法训练"><a href="#pyhanlp句法训练" class="headerlink" title="pyhanlp句法训练"></a>pyhanlp句法训练</h1><p><img src="https://user-images.githubusercontent.com/36963108/169463356-d2faf6c3-557d-49f4-83d7-235ec657c5b3.png" alt="image"></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python train.py </span><br><span class="line">下载 http://file.hankcs.com/corpus/ctb8.0-dep.zip 到 /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/ctb8.0-dep.zip</span><br><span class="line">100%   3.5 MiB 114.3 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">下载 http://file.hankcs.com/corpus/wiki-cn-cluster.zip 到 /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/wiki-cn-cluster.txt.zip</span><br><span class="line">100% 763.9 KiB 353.9 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">训练集句子数量: 14863                                                                             </span><br><span class="line">迭代 1/20 100.00%  耗时 544 秒。UAS=83.23 LAS=80.84 最高分！保存中...</span><br><span class="line">迭代 2/20 100.00%  耗时 569 秒。UAS=84.06 LAS=81.97 最高分！保存中...</span><br><span class="line">迭代 3/20 100.00%  耗时 557 秒。UAS=84.55 LAS=82.48 最高分！保存中...</span><br><span class="line">迭代 4/20 100.00%  耗时 556 秒。UAS=84.82 LAS=82.74 最高分！保存中...</span><br><span class="line">迭代 5/20 100.00%  耗时 553 秒。UAS=84.95 LAS=82.89 最高分！保存中...</span><br><span class="line">迭代 6/20 100.00%  耗时 549 秒。UAS=85.18 LAS=83.15 最高分！保存中...</span><br><span class="line">迭代 7/20 100.00%  耗时 560 秒。UAS=85.25 LAS=83.26 最高分！保存中...</span><br><span class="line">迭代 8/20 100.00%  耗时 562 秒。UAS=85.12 LAS=83.11</span><br><span class="line">迭代 9/20 100.00%  耗时 569 秒。UAS=85.23 LAS=83.24</span><br><span class="line">迭代 10/20 100.00%  耗时 571 秒。UAS=85.17 LAS=83.23</span><br><span class="line">迭代 11/20 100.00%  耗时 571 秒。UAS=85.20 LAS=83.22</span><br><span class="line">迭代 12/20 100.00%  耗时 581 秒。UAS=85.09 LAS=83.16</span><br><span class="line">迭代 13/20 100.00%  耗时 652 秒。UAS=85.16 LAS=83.24</span><br><span class="line">迭代 14/20 100.00%  耗时 677 秒。UAS=85.21 LAS=83.26</span><br><span class="line">迭代 15/20 100.00%  耗时 586 秒。UAS=85.24 LAS=83.31</span><br><span class="line">迭代 16/20 100.00%  耗时 554 秒。UAS=85.26 LAS=83.33 最高分！保存中...</span><br><span class="line">迭代 17/20 100.00%  耗时 537 秒。UAS=85.38 LAS=83.46 最高分！保存中...</span><br><span class="line">迭代 18/20 100.00%  耗时 548 秒。UAS=85.43 LAS=83.49 最高分！保存中...</span><br><span class="line">迭代 19/20 100.00%  耗时 553 秒。UAS=85.39 LAS=83.43</span><br><span class="line">迭代 20/20 100.00%  耗时 554 秒。UAS=85.41 LAS=83.46</span><br><span class="line">1       人      人      N       NN      _       2       nsubj   _       _</span><br><span class="line">2       吃      吃      V       VV      _       0       ROOT    _       _</span><br><span class="line">3       鱼      鱼      N       NN      _       2       dobj    _       _</span><br><span class="line"></span><br><span class="line">100 ... 200 ... 300 ... 400 ... 500 ... 600 ... 700 ... 800 ... 900 ... 1000 ... 1100 ... 1200 ... 1300 ... 1400 ... 1500 ... 1600 ... 1700 ... 1800 ... 1900 ... UAS=85.4 LAS=83.5</span><br></pre></td></tr></table></figure><h1 id="spacy句法"><a href="#spacy句法" class="headerlink" title="spacy句法"></a>spacy句法</h1><p>句法是指句子的各个组成部分的相互关系，句法分析分为句法结构分析（syntactic structure parsing）和依存关系分析(dependency parsing)。句法结构分析用于获取整个句子的句法结构，依存分析用于获取词汇之间的依存关系，目前的句法分析已经从句法结构分析转向依存句法分析。</p><p>依存语法通过分析语言单位内成分之间的依存关系揭示其句法结构，主张句子中核心动词是支配其它成分的中心成分，而它本身却不受其它任何成分的支配，所有受支配成分都以某种依存关系从属于支配者。</p><p>在20世纪70年代，Robinson提出依存语法中关于依存关系的四条公理：</p><ul><li>一个句子中只有一个成分是独立的；</li><li>其它成分直接依存于某一成分；</li><li>任何一个成分都不能依存与两个或两个以上的成分；</li><li>如果A成分直接依存于B成分，而C成分在句中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分；</li></ul><p>SpaCy 中文模型:<a href="https://github.com/howl-anderson/Chinese_models_for_SpaCy">https://github.com/howl-anderson/Chinese_models_for_SpaCy</a><br><a href="https://blog.csdn.net/lllhhhv/article/details/123335675">https://blog.csdn.net/lllhhhv/article/details/123335675</a><br>zh_core_web_trf、zh_core_web_md 等,它们的区别在于准确度和体积大小, zh_core_web_sm 体积小,准确度相比zh_core_web_trf差,zh_core_web_trf相对就体积大。这样可以适应不同场景.</p><h1 id="数据参考"><a href="#数据参考" class="headerlink" title="数据参考"></a>数据参考</h1><p>hanlp.pretrained.dep。CTB5_BIAFFINE_DEP_ZH&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;</a><br>在 CTB5 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017）。</p><p>hanlp.pretrained.dep。CTB7_BIAFFINE_DEP_ZH&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;</a><br>在 CTB7 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017）。</p><p>hanlp.pretrained.dep。CTB9_DEP_ELECTRA_SMALL&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;">https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 CTB9-SD330 上训练。性能为 UAS&#x3D;87.68% LAS&#x3D;83.54%。</p><p>hanlp.pretrained.dep。CTB9_UDC_ELECTRA_SMALL&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;">https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 CTB9-UD420 上训练。性能是 UAS&#x3D;85.92% LAS&#x3D;81.13% 。</p><p>hanlp.pretrained.dep。PMT1_DEP_ELECTRA_SMALL&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;">https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 PKU Multi-view Chinese Treebank (PMT) 1.0 ( Qiu et al. 2014 ) 上训练。性能是 UAS&#x3D;91.21% LAS&#x3D;88.65%。</p><p>hanlp.pretrained.dep。PTB_BIAFFINE_DEP_EN&#x3D; ‘<a href="https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;">https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;</a><br>在 PTB 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017 ）。</p><p>参考来自：<a href="https://hanlp.hankcs.com/docs/api/hanlp/index.html">https://hanlp.hankcs.com/docs/api/hanlp/index.html</a></p><h1 id="ctb数据集相关论文"><a href="#ctb数据集相关论文" class="headerlink" title="ctb数据集相关论文"></a>ctb数据集相关论文</h1><p><a href="https://github.com/textflint/textflint.github.io/blob/b387b412642bdaf61fc49173a4e6077c8a0d372a/Tasks/DPCN/paper_list.json">DPCN&#x2F;dataset_paper.json</a></p><p>hanlp句法分析训练问题解决：<a href="https://bbs.hankcs.com/t/topic/2868">https://bbs.hankcs.com/t/topic/2868</a></p><h1 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别:"></a>命名实体识别:</h1><p>命名实体识别从早期基于词典和规则的方法，到传统机器学习的方法， 后来采用基于深度学习的方法，一直到当下热门的注意力机制、图神经网络等研究方法， 命名实体识别技术路线随着时间在不断发展。</p><p><img src="https://user-images.githubusercontent.com/36963108/178393815-01045ee4-885e-4aec-9231-c65cfa3af835.png" alt="image"></p><p><a href="https://github.com/TianRanPig/chinese_ner">https://github.com/TianRanPig/chinese_ner</a></p><p><a href="https://github.com/CLUEbenchmark/CLUENER2020">https://github.com/CLUEbenchmark/CLUENER2020</a></p><p><a href="https://github.com/hemingkx/CLUENER2020">https://github.com/hemingkx/CLUENER2020</a></p><p><a href="https://github.com/lonePatient/BERT-NER-Pytorch">https://github.com/lonePatient/BERT-NER-Pytorch</a></p><p><a href="https://github.com/lemonhu/NER-BERT-pytorch">https://github.com/lemonhu/NER-BERT-pytorch</a></p><p><a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p><p><a href="https://github.com/TobiasLee/ChineseNER">https://github.com/TobiasLee/ChineseNER</a></p><p><a href="https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice">https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice</a></p><p><a href="https://github.com/luopeixiang/named_entity_recognition">https://github.com/luopeixiang/named_entity_recognition</a></p><p><a href="https://github.com/F-debug/Medical-named-entity-recognition">https://github.com/F-debug/Medical-named-entity-recognition</a></p><p><a href="https://github.com/kyzhouhzau/BERT-NER">https://github.com/kyzhouhzau/BERT-NER</a></p><p><a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER">https://github.com/macanv/BERT-BiLSTM-CRF-NER</a></p><p><a href="https://github.com/xuanzebi/BERT-CH-NER">https://github.com/xuanzebi/BERT-CH-NER</a></p><p><a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p><p>使用bert做领域分类、意图识别和槽位填充任务 <a href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p><p>基于pytorch的中文意图识别和槽位填充 <a href="https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling">https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling</a></p><p>基于BERT+Tensorflow-1.15+Horovod-0.22的NLU（意图识别+槽位填充）分布式GPU训练模块 <a href="https://github.com/jx1100370217/JointBERT_nlu_tf">https://github.com/jx1100370217/JointBERT_nlu_tf</a></p><p>使用bert做领域分类、配置识别和位置填充任务 <a href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p><p>中文语言理解基准、基准中文语言理解评估基准：数据集、预训练模型、语料库 <a href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p><p>用于联合意图分类和插槽填充的 BERT <a href="https://github.com/monologg/JointBERT">https://github.com/monologg/JointBERT</a></p><p><a href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification">https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification</a></p><p><a href="https://github.com/pymacbit/BERT-Intent-Classification">https://github.com/pymacbit/BERT-Intent-Classification</a></p><p><a href="https://github.com/ensembles4612/medical_intent_detector_using_BERT">https://github.com/ensembles4612/medical_intent_detector_using_BERT</a></p><p><a href="https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb">https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb</a></p><p><a href="https://github.com/sz128/slot_filling_and_intent_detection_of_SLU">https://github.com/sz128/slot_filling_and_intent_detection_of_SLU</a></p><p><a href="https://github.com/471417367/bert_intention_zh">https://github.com/471417367/bert_intention_zh</a></p><p>数据集自动标注工具–释放AI潜力！<a href="https://www.modelfun.cn/home">https://www.modelfun.cn/home</a></p><p>实体识别数据集 <a href="https://github.com/juand-r/entity-recognition-datasets">https://github.com/juand-r/entity-recognition-datasets</a></p><p>ner综述： <a href="https://blog.csdn.net/weixin_45884316/article/details/118684681">https://blog.csdn.net/weixin_45884316/article/details/118684681</a></p><p>使用 CLIP 将图像和句子嵌入到固定长度的向量中 <a href="https://github.com/jina-ai/clip-as-service">https://github.com/jina-ai/clip-as-service</a></p><p>other:</p><p><a href="https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset">https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset</a></p><p><a href="https://github.com/Hyfred/Pytroch_NER_tutorial">https://github.com/Hyfred/Pytroch_NER_tutorial</a></p><p><a href="https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp">https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp</a></p><p><a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py">https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py</a></p><p><a href="https://github.com/kamalkraj/BERT-NER">https://github.com/kamalkraj/BERT-NER</a></p><p>参考：<a href="https://github.com/kyzhouhzau/NLPGNN">https://github.com/kyzhouhzau/NLPGNN</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[1] BERT：用于语言理解的深度双向转换器的预训练</span><br><span class="line">[2] ALBERT：用于语言表示的自监督学习的 Lite BERT</span><br><span class="line">[3]语言模型是无监督的多任务学习者</span><br><span class="line">[4]用于量子化学的神经消息传递</span><br><span class="line">[ 5]使用图卷积网络进行半监督分类</span><br><span class="line">[6]图注意网络</span><br><span class="line">[7]图神经网络有多强大？</span><br><span class="line">[8] GraphSAGE：大图上的归纳表示学习</span><br><span class="line">[9]扩散改进了图学习</span><br><span class="line">[10]基准图神经网络</span><br><span class="line">[11]用于文本分类的文本级图神经网络</span><br><span class="line">[12]用于文本分类的图卷积网络</span><br><span class="line">[13]用于文本分类的张量图卷积网络</span><br><span class="line">[14]深入了解用于半监督学习的图卷积网络</span><br></pre></td></tr></table></figure><p>命名实体识别（NER）标注神器: <a href="https://blog.csdn.net/qq_44193969/article/details/123298406">https://blog.csdn.net/qq_44193969/article/details/123298406</a></p><p>实践：<a href="https://blog.csdn.net/qq_44193969/article/details/116008734">https://blog.csdn.net/qq_44193969/article/details/116008734</a></p><p><a href="https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction">https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction</a></p><p><a href="https://blog.csdn.net/weixin_40846933/article/details/106384566">https://blog.csdn.net/weixin_40846933/article/details/106384566</a></p></a></a></a></a></a></a></a></a>]]></content>
      
      
      <categories>
          
          <category> 人工智能论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学术论文排版工具LaTeX</title>
      <link href="/2018/04/05/latex/"/>
      <url>/2018/04/05/latex/</url>
      
        <content type="html"><![CDATA[<h1 id="1、latex安装方法"><a href="#1、latex安装方法" class="headerlink" title="1、latex安装方法"></a>1、latex安装方法</h1><p>官方的地址是<a href="https://link.zhihu.com/?target=http://mirror.ctan.org/systems/texlive/Images/texlive2021.iso">http:&#x2F;&#x2F;<strong>mirror.ctan.org&#x2F;systems</strong>&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>，但是可能速度较慢，以下是一些国内的镜像地址：国内的镜像地址：</p><ul><li>清华大学：<a href="https://link.zhihu.com/?target=https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirrors.tuna.tsinghua.edu.cn</strong>&#x2F;CTAN&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>；</li><li>北京交通大学：<a href="https://link.zhihu.com/?target=https://mirror.bjtu.edu.cn/ctan/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirror.bjtu.edu.cn&#x2F;ctan</strong>&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>；</li><li>上海交通大学：<a href="https://link.zhihu.com/?target=https://mirrors.sjtug.sjtu.edu.cn/ctan/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirrors.sjtug.sjtu.edu.cn</strong>&#x2F;ctan&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>；</li><li>中国科技大学：<a href="https://link.zhihu.com/?target=https://mirrors.ustc.edu.cn/CTAN/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirrors.ustc.edu.cn&#x2F;CTA</strong>N&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>；</li><li>重庆大学：<a href="https://link.zhihu.com/?target=https://mirrors.cqu.edu.cn/CTAN/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirrors.cqu.edu.cn&#x2F;CTAN</strong>&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a>；</li><li>腾讯云：<a href="https://link.zhihu.com/?target=https://mirrors.cloud.tencent.com/CTAN/systems/texlive/Images/texlive2021.iso">https:&#x2F;&#x2F;<strong>mirrors.cloud.tencent.com</strong>&#x2F;CTAN&#x2F;systems&#x2F;texlive&#x2F;Images&#x2F;texlive2021.iso</a></li></ul><p>下载与安装参考地址1：<a href="https://mirror-hk.koddos.net/CTAN/systems/texlive/Images/">https://mirror-hk.koddos.net/CTAN/systems/texlive/Images/</a></p><p>下载与安装参考地址2：<a href="https://www.jianshu.com/p/a8d46d00b833">https://www.jianshu.com/p/a8d46d00b833</a></p><p>推荐迅雷下载较快：</p><p><img src="https://img-blog.csdnimg.cn/31e7246bf92e46f1b0e4461528d7c1f1.png" alt="texd"></p><p>在线LaTeX编辑器：<a href="https://www.overleaf.com/">https://www.overleaf.com</a><br>TeX Live下载：<a href="https://www.tug.org/texlive/acquire-iso.html">https://www.tug.org/texlive/acquire-iso.html</a><br>MikTeX下载：<a href="https://miktex.org/download">https://miktex.org/download</a><br>LaTeX 公式编辑器：<a href="https://latex.codecogs.com/eqneditor/editor.php">https://latex.codecogs.com/eqneditor/editor.php</a><br>LaTeX介绍：<a href="https://github.com/CTeX-org/lshort-zh-cn">https://github.com/CTeX-org/lshort-zh-cn</a></p><p>安装步骤：我安装在D盘</p><h2 id="步骤1：选择iso文件鼠标右键点击装载"><a href="#步骤1：选择iso文件鼠标右键点击装载" class="headerlink" title="步骤1：选择iso文件鼠标右键点击装载"></a>步骤1：选择iso文件鼠标右键点击装载</h2><p><img src="https://img-blog.csdnimg.cn/43aef46c599240119b549c33c1018eb3.png" alt="在这里插入图片描述"></p><h2 id="步骤2：选择安装路径"><a href="#步骤2：选择安装路径" class="headerlink" title="步骤2：选择安装路径"></a>步骤2：选择安装路径</h2><p><img src="https://img-blog.csdnimg.cn/e69dd67e7f1042169f1dc6fc9bef0b40.png" alt="在这里插入图片描述"></p><h2 id="步骤3：确定安装路径之后点击安装"><a href="#步骤3：确定安装路径之后点击安装" class="headerlink" title="步骤3：确定安装路径之后点击安装"></a>步骤3：确定安装路径之后点击安装</h2><p><img src="https://img-blog.csdnimg.cn/a362eabb8d2046f6bf451afb6051097d.png" alt="&lt;span data-type="></p><h1 id="2、学习网站"><a href="#2、学习网站" class="headerlink" title="2、学习网站"></a>2、学习网站</h1><p><a href="https://www.latexstudio.net/">https://www.latexstudio.net/</a></p><p><a href="https://www.jianshu.com/nb/34744106">https://www.jianshu.com/nb/34744106</a></p>]]></content>
      
      
      <categories>
          
          <category> LaTeX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排版工具 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
