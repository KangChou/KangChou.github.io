<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    
<script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>

    
<link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">

  
  

  <!-- PACE Progress Bar START -->

  
  <title>è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½è®ºæ–‡æ±‡é›† | ovo$^{mc^2}$</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="å­¦ä¹ " />
  
  
  
  
  <meta name="description" content="è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±GOã€é»‘å®¢ã€Androidã€è®¡ç®—æœºåŸç†ã€äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ã€æœºå™¨å­¦ä¹ ã€æ•°æ®åº“ã€PHPã€javaã€æ¶æ„ã€æ¶ˆæ¯é˜Ÿåˆ—ã€ç®—æ³•ã€pythonã€çˆ¬è™«ã€æ“ä½œç³»ç»Ÿã€linuxã€Cè¯­è¨€ï¼š https:&#x2F;&#x2F;github.com&#x2F;TIM168&#x2F;technical_books è®¡ç®—æœºç§‘å­¦ï¼Œè½¯ä»¶æŠ€æœ¯ï¼Œåˆ›ä¸šï¼Œæ€æƒ³ç±»ï¼Œæ•°å­¦ç±»ï¼Œäººç‰©ä¼ è®°ä¹¦ç±ï¼šhttps:&#x2F;&#x2F;github.com&#x2F;0voice&#x2F;expert_rea">
<meta property="og:type" content="article">
<meta property="og:title" content="è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½è®ºæ–‡æ±‡é›†">
<meta property="og:url" content="https://www.coomatrix.com/2019/07/05/papers/index.html">
<meta property="og:site_name" content="ovo$^{mc^2}$">
<meta property="og:description" content="è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±GOã€é»‘å®¢ã€Androidã€è®¡ç®—æœºåŸç†ã€äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ã€æœºå™¨å­¦ä¹ ã€æ•°æ®åº“ã€PHPã€javaã€æ¶æ„ã€æ¶ˆæ¯é˜Ÿåˆ—ã€ç®—æ³•ã€pythonã€çˆ¬è™«ã€æ“ä½œç³»ç»Ÿã€linuxã€Cè¯­è¨€ï¼š https:&#x2F;&#x2F;github.com&#x2F;TIM168&#x2F;technical_books è®¡ç®—æœºç§‘å­¦ï¼Œè½¯ä»¶æŠ€æœ¯ï¼Œåˆ›ä¸šï¼Œæ€æƒ³ç±»ï¼Œæ•°å­¦ç±»ï¼Œäººç‰©ä¼ è®°ä¹¦ç±ï¼šhttps:&#x2F;&#x2F;github.com&#x2F;0voice&#x2F;expert_rea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png">
<meta property="article:published_time" content="2019-07-05T12:57:06.000Z">
<meta property="article:modified_time" content="2022-11-20T14:45:10.708Z">
<meta property="article:author" content="KangChou">
<meta property="article:tag" content="AIè®ºæ–‡">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/my.css">

  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="ovo$^{mc^2}$" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>

<script>
var themeMenus = {};

  themeMenus["/"] = "é¦–é¡µ"; 

  themeMenus["/archives"] = "å½’æ¡£"; 

  themeMenus["/categories"] = "åˆ†ç±»"; 

  themeMenus["/tags"] = "æ ‡ç­¾"; 

  themeMenus["/photography"] = "æ‘„å½±"; 

  themeMenus["/about"] = "å…³äº"; 

  themeMenus["/guestbook"] = "ç•™è¨€"; 

  themeMenus["/AI"] = "ai"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="ovo$^{mc^2}$" rel="home"> ovo$^{mc^2}$ </a>
            
          </h1>

          
            <div class="site-description">äº’è”ç½‘äººå·¥æ™ºèƒ½æŠ€æœ¯å¼€å‘è€…$$ä¸–é—´ä¸‡ç‰© e^{ i \pi}+1=0 äº’è”äº’é€š$$
é‚®ç®±:kangsinx@yeah.net     å¾®ä¿¡å…¬ä¼—å·(AIç§‘æŠ€ä¸ç®—æ³•ç¼–ç¨‹):kangsinx</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">é¦–é¡µ</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">å½’æ¡£</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">åˆ†ç±»</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">æ ‡ç­¾</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/photography">æ‘„å½±</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">å…³äº</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/guestbook">ç•™è¨€</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/AI">ai</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/blog.gif,css/images/pose.jpg,css/images/pose2.jpg,css/images/pose3.jpg,css/images/pose4.jpg,https://images.wallpaperscraft.com/image/couple_mountains_travel_125490_1280x720.jpg,https://cdn.pixabay.com/photo/2019/05/26/16/43/norway-4230682_1280.jpg".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;">
<article id="post-papers" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" target="_blank" href="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png" rel="gallery_claph11ci000hz0vd1kh63zgm noopener">
        <img src="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½è®ºæ–‡æ±‡é›†
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/07/05/papers/" class="article-date">
	  <time datetime="2019-07-05T12:57:06.000Z" itemprop="datePublished">ä¸ƒæœˆ 5, 2019</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  æœ¬æ–‡æ€»é˜…è¯»é‡<span id="busuanzi_value_page_pv"></span>æ¬¡
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±"><a href="#è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±" class="headerlink" title="è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±"></a>è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±</h1><p>GOã€é»‘å®¢ã€Androidã€è®¡ç®—æœºåŸç†ã€äººå·¥æ™ºèƒ½ã€å¤§æ•°æ®ã€æœºå™¨å­¦ä¹ ã€æ•°æ®åº“ã€PHPã€javaã€æ¶æ„ã€æ¶ˆæ¯é˜Ÿåˆ—ã€ç®—æ³•ã€pythonã€çˆ¬è™«ã€æ“ä½œç³»ç»Ÿã€linuxã€Cè¯­è¨€ï¼š</p>
<p><a target="_blank" rel="noopener" href="https://github.com/TIM168/technical_books">https://github.com/TIM168/technical_books</a></p>
<p>è®¡ç®—æœºç§‘å­¦ï¼Œè½¯ä»¶æŠ€æœ¯ï¼Œåˆ›ä¸šï¼Œæ€æƒ³ç±»ï¼Œæ•°å­¦ç±»ï¼Œäººç‰©ä¼ è®°ä¹¦ç±ï¼š<a target="_blank" rel="noopener" href="https://github.com/0voice/expert_readed_books">https://github.com/0voice/expert_readed_books</a></p>
<p>å›½å†…å‡ æ‰€å¤§å­¦ä¸“ä¸šè¯¾ç¨‹èµ„æ–™æ•´ç†ï¼š<a target="_blank" rel="noopener" href="https://github.com/lib-pku/libpku">https://github.com/lib-pku/libpku</a></p>
<p>NLPè‡ªç„¶è¯­è¨€å¤„ç†èµ„æ–™æ±‡æ€»ï¼š</p>
<pre><code>ä¸­è‹±æ–‡æ•æ„Ÿè¯ã€è¯­è¨€æ£€æµ‹ã€ä¸­å¤–æ‰‹æœº/ç”µè¯å½’å±åœ°/è¿è¥å•†æŸ¥è¯¢ã€åå­—æ¨æ–­æ€§åˆ«ã€æ‰‹æœºå·æŠ½å–ã€èº«ä»½è¯æŠ½å–ã€é‚®ç®±æŠ½å–ã€ä¸­æ—¥æ–‡äººååº“ã€ä¸­æ–‡ç¼©å†™åº“ã€æ‹†å­—è¯å…¸ã€è¯æ±‡æƒ…æ„Ÿå€¼ã€åœç”¨è¯ã€ååŠ¨è¯è¡¨ã€æš´æè¯è¡¨ã€ç¹ç®€ä½“è½¬æ¢ã€è‹±æ–‡æ¨¡æ‹Ÿä¸­æ–‡å‘éŸ³ã€æ±ªå³°æ­Œè¯ç”Ÿæˆå™¨ã€èŒä¸šåç§°è¯åº“ã€åŒä¹‰è¯åº“ã€åä¹‰è¯åº“ã€å¦å®šè¯åº“ã€æ±½è½¦å“ç‰Œè¯åº“ã€æ±½è½¦é›¶ä»¶è¯åº“ã€è¿ç»­è‹±æ–‡åˆ‡å‰²ã€å„ç§ä¸­æ–‡è¯å‘é‡ã€å…¬å¸åå­—å¤§å…¨ã€å¤è¯—è¯åº“ã€ITè¯åº“ã€è´¢ç»è¯åº“ã€æˆè¯­è¯åº“ã€åœ°åè¯åº“ã€å†å²åäººè¯åº“ã€è¯—è¯è¯åº“ã€åŒ»å­¦è¯åº“ã€é¥®é£Ÿè¯åº“ã€æ³•å¾‹è¯åº“ã€æ±½è½¦è¯åº“ã€åŠ¨ç‰©è¯åº“ã€ä¸­æ–‡èŠå¤©è¯­æ–™ã€ä¸­æ–‡è°£è¨€æ•°æ®ã€ç™¾åº¦ä¸­æ–‡é—®ç­”æ•°æ®é›†ã€å¥å­ç›¸ä¼¼åº¦åŒ¹é…ç®—æ³•é›†åˆã€bertèµ„æºã€æ–‡æœ¬ç”Ÿæˆ&amp;æ‘˜è¦ç›¸å…³å·¥å…·ã€cocoNLPä¿¡æ¯æŠ½å–å·¥å…·ã€å›½å†…ç”µè¯å·ç æ­£åˆ™åŒ¹é…ã€æ¸…åå¤§å­¦XLORE:ä¸­è‹±æ–‡è·¨è¯­è¨€ç™¾ç§‘çŸ¥è¯†å›¾è°±ã€æ¸…åå¤§å­¦äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼š
</code></pre>
<p><a target="_blank" rel="noopener" href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p>
<h1 id="AIä¹¦ç±ä¸ç®—æ³•æºç "><a href="#AIä¹¦ç±ä¸ç®—æ³•æºç " class="headerlink" title="AIä¹¦ç±ä¸ç®—æ³•æºç "></a>AIä¹¦ç±ä¸ç®—æ³•æºç </h1><p>ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ :<a target="_blank" rel="noopener" href="https://nndl.github.io/">https://nndl.github.io/</a></p>
<p>ç»Ÿè®¡å­¦æ–¹æ³•ï¼š<a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">https://github.com/SmirkCao/Lihang</a></p>
<p>ç»Ÿè®¡å­¦æ–¹æ³•ä¹ é¢˜ç­”æ¡ˆ:<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/">https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/SuperCV/Book">https://github.com/SuperCV/Book</a></p>
<p>æœºå™¨å­¦ä¹ ï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/tutorials">https://github.com/MorvanZhou/tutorials</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lawlite19/MachineLearning_Python">https://github.com/lawlite19/MachineLearning_Python</a></li>
<li>è¥¿ç“œä¹¦ <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/pumpkin-book">https://github.com/datawhalechina/pumpkin-book</a></li>
<li>æœºå™¨è§†è§‰:<a target="_blank" rel="noopener" href="https://github.com/Ewenwan/MVision">https://github.com/Ewenwan/MVision</a></li>
<li>è§†è§‰ç®—æ³•æ’å:<a target="_blank" rel="noopener" href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark</a></li>
<li>æœºå™¨å­¦ä¹ èµ„æºå¤§å…¨ä¸­æ–‡ç‰ˆ:<a target="_blank" rel="noopener" href="https://github.com/jobbole/awesome-machine-learning-cn">https://github.com/jobbole/awesome-machine-learning-cn</a></li>
<li>æœºå™¨æ•°å­¦ä¸ä»£ç å®ç°:<a target="_blank" rel="noopener" href="https://ailearning.apachecn.org/#/">https://ailearning.apachecn.org/#/</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">https://github.com/fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">https://github.com/Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zslucky/awesome-AI-books">https://github.com/zslucky/awesome-AI-books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/dsgiitr/d2l-pytorch">https://github.com/dsgiitr/d2l-pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/1033020837/Basic4AI">https://github.com/1033020837/Basic4AI</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/xmj-ai/deeplearning_ai_books">https://github.com/xmj-ai/deeplearning_ai_books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lllhhh/BooksKeeper">https://github.com/lllhhh/BooksKeeper</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cosen1024/awesome-cs-books">https://github.com/cosen1024/awesome-cs-books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/wugenqiang/NoteBook">https://github.com/wugenqiang/NoteBook</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bat67/awesome-ai-books-and-code">https://github.com/bat67/awesome-ai-books-and-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/china-testing/python-api-tesing">https://github.com/china-testing/python-api-tesing</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/iamshuaidi/CS-Book">https://github.com/iamshuaidi/CS-Book</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/itdevbooks/pdf">https://github.com/itdevbooks/pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/getsources/CS-Growing-book">https://github.com/getsources/CS-Growing-book</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/861664308/Tensorflow-Keras--">https://github.com/861664308/Tensorflow-Keras--</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/jlgulu/PythonAIPath-Geek">https://github.com/jlgulu/PythonAIPath-Geek</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zhangziliang04/aipm">https://github.com/zhangziliang04/aipm</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Baiyuetribe/paper2gui">https://github.com/Baiyuetribe/paper2gui</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Robinwho/Deep-Learning">https://github.com/Robinwho/Deep-Learning</a></p>
<p>æ„å»ºå¼€æºå¯¹è¯æœºå™¨äºº:<a target="_blank" rel="noopener" href="https://github.com/Chinese-NLP-book/rasa_chinese_book_code">https://github.com/Chinese-NLP-book/rasa_chinese_book_code</a></p>
<p><a target="_blank" rel="noopener" href="http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf">http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf">http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf</a></p>
<p>ã€ŠNavin Sabharwal - Hands-on Question Answering Systems with BERT_ Applications in Neural Networks and Natural Language Processing-Apress (2021)ã€‹</p>
<p>ã€ŠSudharsan Ravichandiran - Getting Started with Google BERT_ Build and train state-of-the-art natural language processing models using BERT-Packt Publishing Ltd (2021)ã€‹</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Microstrong0305/Python2AI">https://github.com/Microstrong0305/Python2AI</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chatopera/Synonyms">https://github.com/chatopera/Synonyms</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes">https://github.com/cj0012/AI-Practice-Tensorflow-Notes</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/YeonwooSung/ai_book">https://github.com/YeonwooSung/ai_book</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jikexueyuanwiki/tensorflow-zh">https://github.com/jikexueyuanwiki/tensorflow-zh</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JDHHH/AI-Books">https://github.com/JDHHH/AI-Books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lihanghang/Deep-learning-And-Paper">https://github.com/lihanghang/Deep-learning-And-Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/koryako/FundamentalsOfAI_book_code">https://github.com/koryako/FundamentalsOfAI_book_code</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhangbincheng1997/chatbot-aiml-webqa">https://github.com/zhangbincheng1997/chatbot-aiml-webqa</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/qqqil/books">https://github.com/qqqil/books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/books">https://github.com/KeKe-Li/books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95">https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dujltqzv/Some-Many-Books">https://github.com/Dujltqzv/Some-Many-Books</a></li>
</ul>
<h1 id="è®¡ç®—æœºè§†è§‰å®æ—¶åŠ¨æ€"><a href="#è®¡ç®—æœºè§†è§‰å®æ—¶åŠ¨æ€" class="headerlink" title="è®¡ç®—æœºè§†è§‰å®æ—¶åŠ¨æ€"></a>è®¡ç®—æœºè§†è§‰å®æ—¶åŠ¨æ€</h1><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/menu">https://openaccess.thecvf.com/menu</a></p>
<p><img src="https://user-images.githubusercontent.com/36963108/193174986-63d2ae54-ee0f-4507-8f32-b75d325d78a9.png" alt="image"></p>
<h1 id="3D-å¯¹è±¡æ£€æµ‹"><a href="#3D-å¯¹è±¡æ£€æµ‹" class="headerlink" title="3D å¯¹è±¡æ£€æµ‹"></a>3D å¯¹è±¡æ£€æµ‹</h1><p><img src="https://miro.medium.com/max/1400/0*JDqH7_kKaGpkoUmv.png"></p>
<p>å‚è€ƒæ¥è‡ªï¼š <a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection">https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TianhaoFu/Awesome-3D-Object-Detection">https://github.com/TianhaoFu/Awesome-3D-Object-Detection</a></p>
<h1 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI æ•°æ®é›†</a></li>
<li>3,712 ä¸ªè®­ç»ƒæ ·æœ¬</li>
<li>3,769 ä¸ªéªŒè¯æ ·æœ¬</li>
<li>7,518ä¸ªæµ‹è¯•æ ·æœ¬</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuScenes æ•°æ®é›†</a></li>
<li>28k è®­ç»ƒæ ·æœ¬</li>
<li>6k éªŒè¯æ ·æœ¬</li>
<li>6k æµ‹è¯•æ ·æœ¬</li>
<li><a target="_blank" rel="noopener" href="https://level-5.global/data/perception/">Lyft æ•°æ®é›†</a></li>
<li><a target="_blank" rel="noopener" href="https://waymo.com/open/download/">Waymo å¼€æ”¾æ•°æ®é›†</a></li>
<li>798 ä¸ªè®­ç»ƒåºåˆ—ï¼Œå¤§çº¦ 158ã€361 ä¸ª LiDAR æ ·æœ¬</li>
<li>202 ä¸ªéªŒè¯åºåˆ—ï¼ŒåŒ…å« 40ã€077 ä¸ª LiDAR æ ·æœ¬ã€‚</li>
</ul>
<h1 id="é¡¶çº§ä¼šè®®å’Œç ”è®¨ä¼š"><a href="#é¡¶çº§ä¼šè®®å’Œç ”è®¨ä¼š" class="headerlink" title="é¡¶çº§ä¼šè®®å’Œç ”è®¨ä¼š"></a>é¡¶çº§ä¼šè®®å’Œç ”è®¨ä¼š</h1><h1 id="ä¼šè®®"><a href="#ä¼šè®®" class="headerlink" title="ä¼šè®®"></a>ä¼šè®®</h1><ul>
<li>è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰</li>
<li>è®¡ç®—æœºè§†è§‰å›½é™…ä¼šè®®ï¼ˆICCVï¼‰</li>
<li>æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼ˆECCVï¼‰</li>
</ul>
<h1 id="ä½œåŠ"><a href="#ä½œåŠ" class="headerlink" title="ä½œåŠ"></a>ä½œåŠ</h1><ul>
<li>CVPR 2019 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šï¼ˆ<a target="_blank" rel="noopener" href="http://cvpr2019.wad.vision/">nuScenes 3D detection</a>ï¼‰</li>
<li>CVPR 2020 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šï¼ˆ<a target="_blank" rel="noopener" href="http://cvpr2020.wad.vision/">BDD1k 3D tracking</a>ï¼‰</li>
<li>CVPR 2021 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šï¼ˆ<a target="_blank" rel="noopener" href="http://cvpr2021.wad.vision/">waymo 3Dæ£€æµ‹</a>ï¼‰</li>
<li>CVPR 2022 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šï¼ˆ<a target="_blank" rel="noopener" href="http://cvpr2022.wad.vision/">waymo 3Dæ£€æµ‹</a>ï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/cvpr2021-3d-vision-robotics">CVPR 2021 3D è§†è§‰å’Œæœºå™¨äººç ”è®¨ä¼š</a></li>
<li><a target="_blank" rel="noopener" href="https://scene-understanding.com/">CVPR 2021 è§†è§‰ã€å›¾å½¢å’Œæœºå™¨äºº 3D åœºæ™¯ç†è§£ç ”è®¨ä¼š</a></li>
<li><a target="_blank" rel="noopener" href="http://wad.ai/">ICCV 2019 è‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼š</a></li>
<li><a target="_blank" rel="noopener" href="https://avvision.xyz/iccv21/">ICCV 2021 è‡ªåŠ¨é©¾é©¶æ±½è½¦è§†è§‰ç ”è®¨ä¼šï¼ˆAVVisionï¼‰</a>ï¼Œ<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Fan_Autonomous_Vehicle_Vision_2021_ICCV_Workshop_Summary_ICCVW_2021_paper.pdf">æ³¨</a></li>
<li><a target="_blank" rel="noopener" href="https://competitions.codalab.org/competitions/33236#learn_the_details">ICCV 2021 ç ”è®¨ä¼š SSLAD Track 2â€“3D å¯¹è±¡æ£€æµ‹</a></li>
<li><a target="_blank" rel="noopener" href="https://c4av-2020.github.io/">ECCV 2020 è‡ªåŠ¨é©¾é©¶æ±½è½¦æŒ‡ä»¤ç ”è®¨ä¼š</a></li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/pad2020">ECCV 2020 è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç ”è®¨ä¼š</a></li>
</ul>
<h1 id="è®ºæ–‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•ï¼‰"><a href="#è®ºæ–‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•ï¼‰" class="headerlink" title="è®ºæ–‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•ï¼‰"></a>è®ºæ–‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•ï¼‰</h1><ul>
<li>ç”¨äº LiDAR ç‚¹äº‘ä¸­ 3D å¯¹è±¡æ£€æµ‹çš„ç«¯åˆ°ç«¯å¤šè§†å›¾èåˆ<a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">è®ºæ–‡</a></li>
<li>ä½¿ç”¨å…¨å·ç§¯ç½‘ç»œï¼ˆç™¾åº¦ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.07916">è®ºæ–‡ä» 3D æ¿€å…‰é›·è¾¾è¿›è¡Œè½¦è¾†æ£€æµ‹</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.06396.pdf">VoxelNetï¼šåŸºäºç‚¹äº‘çš„ 3D å¯¹è±¡æ£€æµ‹è®ºæ–‡</a>çš„ç«¯åˆ°ç«¯å­¦ä¹ <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.06396.pdf"></a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.08689.pdf">ä½¿ç”¨æ·±åº¦å·ç§¯ç½‘ç»œè®ºæ–‡</a>åœ¨å ç”¨ç½‘æ ¼åœ°å›¾ä¸­è¿›è¡Œå¯¹è±¡æ£€æµ‹å’Œåˆ†ç±»<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.08689.pdf"></a></li>
<li>RT3Dï¼šç”¨äºè‡ªåŠ¨é©¾é©¶çš„ LiDAR ç‚¹äº‘ä¸­çš„å®æ—¶ 3-D è½¦è¾†æ£€æµ‹<a target="_blank" rel="noopener" href="https://www.onacademic.com/detail/journal_1000040467923610_4dfe.html">è®ºæ–‡</a></li>
<li>BirdNetï¼šæ¥è‡ª LiDAR ä¿¡æ¯<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.01195.pdf">è®ºæ–‡çš„ 3D å¯¹è±¡æ£€æµ‹æ¡†æ¶</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04902.pdf">LMNetï¼šä½¿ç”¨ 3D LiDARè®ºæ–‡</a>åœ¨ CPU ä¸Šè¿›è¡Œå®æ—¶å¤šç±»ç›®æ ‡æ£€æµ‹<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04902.pdf"></a></li>
<li>HDNET: Exploit HD Maps for 3D Object Detection<a href="https://link.zhihu.com/?target=http://proceedings.mlr.press/v87/yang18b/yang18b.pdf">è®ºæ–‡</a></li>
<li>PointNetï¼šç”¨äº 3D åˆ†ç±»å’Œåˆ†å‰²çš„ç‚¹é›†çš„æ·±åº¦å­¦ä¹ <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.00593.pdf">è®ºæ–‡</a></li>
<li>PointNet++ï¼šåº¦é‡ç©ºé—´ä¸­ç‚¹é›†çš„æ·±åº¦åˆ†å±‚ç‰¹å¾å­¦ä¹ <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02413">è®ºæ–‡</a></li>
<li>IPOD: Intensive Point-based Object Detector for Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05276v1">è®ºæ–‡</a></li>
<li>PIXORï¼šæ¥è‡ªç‚¹äº‘çš„å®æ—¶ 3D å¯¹è±¡æ£€æµ‹<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~wenjie/papers/cvpr18/pixor.pdf">è®ºæ–‡</a></li>
<li>DepthCNï¼šè½¦è¾†æ£€æµ‹ä½¿ç”¨ 3D-LIDAR å’Œ ConvNet<a target="_blank" rel="noopener" href="https://www.baidu.com/link?url=EaE2zYjHkWvF33nsET2eNvbFGFu8-D3wWPia04uyKm95jMetHsSv3Zk-tODPGm5clsgCUgtVULsZ6IQqv0EYS_Z8El7Zzh57XzlJroSkaOuC8yv7r1XXL4bUrM2tWrTgjwqzfMV2tMTnFNbMOmHLTkUobgMg7HKoS6WW6PfQzkG&wd=&eqid=8f320cfa0005b878000000055e528b6d">è®ºæ–‡</a></li>
<li>Voxel-FPNï¼šç‚¹äº‘ 3D å¯¹è±¡æ£€æµ‹ä¸­çš„å¤šå°ºåº¦ä½“ç´ ç‰¹å¾èšåˆ<a target="_blank" rel="noopener" href="https://arxiv.org/ftp/arxiv/papers/1907/1907.05286.pdf">è®ºæ–‡</a></li>
<li>STDï¼šç‚¹äº‘<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.10471">çº¸çš„ç¨€ç–åˆ°å¯†é›† 3D å¯¹è±¡æ£€æµ‹å™¨</a></li>
<li>å¿«é€Ÿç‚¹ R-CNN<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.02990">è®ºæ–‡</a></li>
<li>StarNetï¼šç‚¹äº‘ä¸­ç›®æ ‡æ£€æµ‹çš„ç›®æ ‡è®¡ç®—<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.11069">è®ºæ–‡</a></li>
<li>ç‚¹äº‘ 3D å¯¹è±¡æ£€æµ‹<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.09492v1">è®ºæ–‡çš„ç±»å¹³è¡¡åˆ†ç»„å’Œé‡‡æ ·</a></li>
<li>LaserNetï¼šä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.08701v1">è®ºæ–‡çš„é«˜æ•ˆæ¦‚ç‡ 3D å¯¹è±¡æ£€æµ‹å™¨</a></li>
<li>FVNetï¼š3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10750v1">è®ºæ–‡</a></li>
<li>Part-AÂ² Netï¼š3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03670v1">è®ºæ–‡</a></li>
<li>PointRCNNï¼š3D Object Proposal Generation and Detection from Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">è®ºæ–‡</a></li>
<li>Complex-YOLOï¼šç‚¹äº‘ä¸Šçš„å®æ—¶ 3D å¯¹è±¡æ£€æµ‹<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.06199">è®ºæ–‡</a></li>
<li>YOLO4D: A ST Approach for RT Multi-object Detection and Classification from LiDAR Point Clouds<a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">è®ºæ–‡</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.02350">YOLO3Dï¼šæ¥è‡ª LiDAR ç‚¹äº‘è®ºæ–‡</a>çš„ç«¯åˆ°ç«¯å®æ—¶ 3D é¢å‘å¯¹è±¡è¾¹ç•Œæ¡†æ£€æµ‹<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.02350"></a></li>
<li>ä½¿ç”¨ Pseudo-LiDAR ç‚¹äº‘<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.09847.pdf">è®ºæ–‡è¿›è¡Œå•ç›® 3D å¯¹è±¡æ£€æµ‹</a></li>
<li>Structure Aware Single-stage 3D Object Detection from Point Cloudï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.html">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/skyhehe123/SA-SSD">ä»£ç </a></li>
<li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detectionï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.05679">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/NUAAXQ/MLCVNet">ä»£ç </a></li>
<li>3DSSD: Point-based 3D Single Stage Object Detectorï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.10187">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/tomztyang/3DSSD">ä»£ç </a></li>
<li>LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attentionï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.01389">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/yinjunbo/3DVID">ä»£ç </a></li>
<li>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection(CVPR2020)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.13192">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PV-RCNN">ä»£ç </a></li>
<li>Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloudï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.01251">è®ºæ–‡</a>Â <a target="_blank" rel="noopener" href="https://github.com/WeijingShi/Point-GNN">ä»£ç </a></li>
<li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detectionï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.05679">è®ºæ–‡</a></li>
<li>Density Based Clustering for 3D Object Detection in Point Cloudsï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Ahmed_Density-Based_Clustering_for_3D_Object_Detection_in_Point_Clouds_CVPR_2020_paper.pdf">è®ºæ–‡</a></li>
<li>æ‰€è§å³æ‰€å¾—ï¼šExploiting Visibility for 3D Object Detectionï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.04986.pdf">è®ºæ–‡</a></li>
<li>PointPainting: Sequential Fusion for 3D Object Detection (CVPR2020)<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.10150.pdf">è®ºæ–‡</a></li>
<li>HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detectionï¼ˆCVPR2020ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.00186">è®ºæ–‡</a></li>
<li>LiDAR R-CNN: An Efficient and Universal 3D Object Detectorï¼ˆCVPR2021ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.15297">è®ºæ–‡</a></li>
<li>Center-based 3D Object Detection and Tracking (CVPR2021)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11275">è®ºæ–‡</a></li>
<li>3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection (CVPR2021)<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.04355.pdf">è®ºæ–‡</a></li>
<li>Embracing Single Stride 3D Object Detector with Sparse Transformerï¼ˆCVPR2022ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.06375.pdf">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/TuSimple/SST">ä»£ç </a></li>
<li>Point Density-Aware Voxels for LiDAR 3D Object Detection (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.05662">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/TRAILab/PDV">ä»£ç </a></li>
<li>A Unified Query-based Paradigm for Point Cloud Understanding (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">è®ºæ–‡</a></li>
<li>Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/Ghostish/Open3DSOT">ä»£ç </a></li>
<li>å¹¶éæ‰€æœ‰çš„ç‚¹éƒ½æ˜¯å¹³ç­‰çš„ï¼šLearning High Efficient Point-based Detectors for 3D LiDAR Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11139">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/yifanzhang713/IA-SSD">ä»£ç </a></li>
<li>å›åˆ°ç°å®ï¼šWeakly-supervised 3D Object Detection with Shape-guided Label Enhancementï¼ˆCVPR2022ï¼‰<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2203.05238">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/xuxw98/BackToReality">ä»£ç </a></li>
<li>Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/VoxSeT_cvpr22.pdf">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/skyhehe123/VoxSeT">ä»£ç </a></li>
<li>BoxeR: Box-Attention for 2D and 3D Transformers(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.13087">è®ºæ–‡</a>,<a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/boxer">ä»£ç </a>,<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UnUJJBwcAsRgz6TnQf_b7w">ä¸­æ–‡ä»‹ç»</a></li>
<li>è§„èŒƒæŠ•ç¥¨ï¼šTowards Robust Oriented Bounding Box Detection in 3D Scenes (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.12001">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/CanonicalVoting">ä»£ç </a></li>
<li>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.08195">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo">ä»£ç </a></li>
<li>TransFusionï¼šä½¿ç”¨ Transformers è¿›è¡Œ 3D å¯¹è±¡æ£€æµ‹çš„ç¨³å¥ LiDAR-Camera Fusionã€‚(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11496">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/xuyangbai/transfusion">ä»£ç </a></li>
<li>Point2Seqï¼šå°† 3D å¯¹è±¡æ£€æµ‹ä¸ºåºåˆ—ã€‚(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.13394">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/ocnflag/point2seq">ä»£ç </a></li>
<li>CAT-Detï¼šç”¨äºå¤šæ¨¡æ€ 3D å¯¹è±¡æ£€æµ‹çš„å¯¹æ¯”å¢å¼ºå˜å‹å™¨ï¼ˆCVPR2022ï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.00325">è®ºæ–‡</a></li>
<li>LiDAR Snowfall Simulation for Robust 3D Object Detection (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15118">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/syscv/lidar_snow_sim">ä»£ç </a></li>
<li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15175">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/visionml/pytracking">ä»£ç </a></li>
<li>Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09780">è®ºæ–‡</a></li>
<li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15175">è®ºæ–‡</a>ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/visionml/pytracking">ä»£ç </a></li>
</ul>
<h1 id="ç«èµ›è§£å†³æ–¹æ¡ˆ"><a href="#ç«èµ›è§£å†³æ–¹æ¡ˆ" class="headerlink" title="ç«èµ›è§£å†³æ–¹æ¡ˆ"></a>ç«èµ›è§£å†³æ–¹æ¡ˆ</h1><h1 id="å·¥ç¨‹"><a href="#å·¥ç¨‹" class="headerlink" title="å·¥ç¨‹"></a>å·¥ç¨‹</h1><h1 id="è°ƒæŸ¥"><a href="#è°ƒæŸ¥" class="headerlink" title="è°ƒæŸ¥"></a>è°ƒæŸ¥</h1><ul>
<li>2021.04 ç”¨äºè‡ªåŠ¨é©¾é©¶åº”ç”¨çš„åŸºäºç‚¹äº‘çš„ 3D å¯¹è±¡æ£€æµ‹å’Œåˆ†ç±»æ–¹æ³•ï¼šè°ƒæŸ¥å’Œåˆ†ç±»<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520304097">è®ºæ–‡</a></li>
<li>2021.07 ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ 3D å¯¹è±¡æ£€æµ‹ï¼šè°ƒæŸ¥<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.10823">è®ºæ–‡</a></li>
<li>2021.07 è‡ªåŠ¨é©¾é©¶ä¸­çš„å¤šæ¨¡æ€ 3D å¯¹è±¡æ£€æµ‹ï¼šè°ƒæŸ¥<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.12735">è®ºæ–‡</a></li>
<li>2021.10 åŸºäºæ¿€å…‰é›·è¾¾çš„ 3D ç‰©ä½“æ£€æµ‹æ–¹æ³•ä¸æ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0097849321001321">è®ºæ–‡ç»¼åˆè°ƒæŸ¥</a></li>
<li>2021.12 3D ç‚¹äº‘çš„æ·±åº¦å­¦ä¹ ï¼šè°ƒæŸ¥<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9127813">è®ºæ–‡</a></li>
</ul>
<h1 id="ä¹¦"><a href="#ä¹¦" class="headerlink" title="ä¹¦"></a>ä¹¦</h1><ul>
<li>åŸºäºæ¿€å…‰é›·è¾¾å’Œæ‘„åƒå¤´çš„ 3D å¯¹è±¡æ£€æµ‹ç®—æ³•ï¼šè®¾è®¡ä¸ä»¿çœŸ<a target="_blank" rel="noopener" href="https://www.amazon.com/Object-Detection-Algorithms-Based-Camera/dp/6200536538">ä¹¦</a></li>
</ul>
<h1 id="è§†é¢‘"><a href="#è§†é¢‘" class="headerlink" title="è§†é¢‘"></a>è§†é¢‘</h1><ul>
<li>Aivia åœ¨çº¿ç ”è®¨ä¼šï¼š3D å¯¹è±¡æ£€æµ‹å’Œè·Ÿè¸ª<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=P0TrkwAdFYQ">è§†é¢‘</a></li>
<li>3D å¯¹è±¡æ£€ç´¢ 2021 ç ”è®¨ä¼š<a target="_blank" rel="noopener" href="https://3dor2021.github.io/programme.html">è§†é¢‘</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4">æ¥è‡ª UCSDè§†é¢‘</a>çš„ SU å®éªŒå®¤çš„ 3D æ·±åº¦å­¦ä¹ æ•™ç¨‹<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4"></a></li>
<li>è®²åº§ï¼šè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆå›¾å®¾æ ¹å¤§å­¦ Andreas Geiger æ•™æˆï¼‰<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4">è§†é¢‘</a></li>
<li>ç‚¹äº‘å¯¹è±¡çš„å½“å‰æ–¹æ³•å’Œæœªæ¥æ–¹å‘ (2021.04)<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xFFCQVwYeec">è§†é¢‘</a></li>
<li>CPU ä¸Š 30+ FPS çš„æœ€æ–° 3D å¯¹è±¡æ£€æµ‹ â€” MediaPipe å’Œ OpenCV Python (2021.05)<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=f-Ibri14KMY">è§†é¢‘</a></li>
<li>MITè‡ªåŠ¨é©¾é©¶ç ”è®¨ä¼šï¼ˆ2019.11ï¼‰<a target="_blank" rel="noopener" href="https://space.bilibili.com/174493426/channel/series">è§†é¢‘</a></li>
<li>sensetime ç ”è®¨ä¼š1<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Bf4y1b7PF?spm_id_from=333.999.0.0">è§†é¢‘</a></li>
<li>sensetime ç ”è®¨ä¼š 2<a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/11CoKCxRFgzbIujMXxTZjHDo_hV0arEQ7sUFWFXWaX8o/edit#slide=id.p1">å¼ å¹»ç¯ç‰‡</a></li>
</ul>
<h1 id="è¯¾ç¨‹"><a href="#è¯¾ç¨‹" class="headerlink" title="è¯¾ç¨‹"></a>è¯¾ç¨‹</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf">å¤šä¼¦å¤šå¤§å­¦ï¼Œcsc2541</a></li>
<li><a target="_blank" rel="noopener" href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/">å›¾å®¾æ ¹å¤§å­¦ï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦</a>Â <em>ï¼ˆå¼ºçƒˆæ¨èï¼‰</em></li>
<li><a target="_blank" rel="noopener" href="https://apollo.auto/devcenter/devcenter.html">ç™¾åº¦-Udacity</a></li>
<li><a target="_blank" rel="noopener" href="http://bit.baidu.com/Subject/index/id/16.html">ç™¾åº¦-é˜¿æ³¢ç½—</a></li>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/specializations/self-driving-cars?ranMID=40328&ranEAID=9IqCvd3EEQc&ranSiteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&siteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=9IqCvd3EEQc">å¤šä¼¦å¤šå¤§å­¦ï¼Œè¯¾ç¨‹</a></li>
</ul>
<h1 id="åšå®¢"><a href="#åšå®¢" class="headerlink" title="åšå®¢"></a>åšå®¢</h1><ul>
<li><a target="_blank" rel="noopener" href="https://blog.waymo.com/">Waymo åšå®¢</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142401769">apolloä»‹ç»ä¹‹æ„ŸçŸ¥æ¨¡å—</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/daohu527/Dig-into-Apollo#ledger-%E7%9B%AE%E5%BD%95">Apollo ç¬”è®°ï¼ˆApollo å­¦ä¹ ç¬”è®°ï¼‰â€” Apollo åˆå­¦è€…å­¦ä¹ ç¬”è®°ã€‚</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44809266">PointNetç³»åˆ—è®ºæ–‡è§£è¯»</a></li>
<li><a target="_blank" rel="noopener" href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/deep3dbox.html">Deep3dBoxï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ å’Œå‡ ä½•è¿›è¡Œ 3D è¾¹ç•Œæ¡†ä¼°è®¡</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356892010">SECONDç®—æ³•è§£æ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361973979">PointRCNNæ·±åº¦è§£æ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363926237">Fast PointRCNNè®ºæ–‡è§£è¯»</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357626425">PointPillarsè®ºæ–‡å’Œä»£ç è§£æ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352419316">VoxelNetè®ºæ–‡å’Œä»£ç è§£æ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/444447881">CenterPointåˆ†æ</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148942116">PV-RCNNï¼š3Dç›®æ ‡æ£€æµ‹Waymoæ¨¡æ€æŒ‘æˆ˜èµ›+KITTIæ¦œå•æ¨¡æ€ç¬¬ä¸€æ¨¡æŒ‘æˆ˜èµ›</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359800738">LiDAR R-CNNï¼šä¸€ç§å¿«é€Ÿã€é€šç”¨çš„äºŒç±»3Dæ£€æµ‹å™¨</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/122426949">æ··åˆä½“ç´ ç½‘ç»œï¼ˆHVNetï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420708905">è‡ªåŠ¨é©¾é©¶æ±½è½¦|Â èŒƒå›´å›¾åƒçº¸åˆ†äº«</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476056546">SSTï¼šå•æ­¥æ”¾å¤§è£…ç½®Transformer 3Dæ¢æµ‹ä»ª</a></li>
</ul>
<h1 id="è‘—åç ”ç©¶ç»„-x2F-å­¦è€…"><a href="#è‘—åç ”ç©¶ç»„-x2F-å­¦è€…" class="headerlink" title="è‘—åç ”ç©¶ç»„&#x2F;å­¦è€…"></a>è‘—åç ”ç©¶ç»„&#x2F;å­¦è€…</h1><ul>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=yAWtq6QAAAAJ&hl=en">ç‹ä¹ƒç‡•@Tusimple</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en">ææ´ªç”Ÿ@CUHK</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=Fe7NTe0AAAAJ&hl=en">ä¸€æ¬¡ Tuzel@Apple</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=XP_Hxm4AAAAJ&hl=en">å¥¥æ–¯å¡Beijbom@nuTonomy</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=jyxO2akAAAAJ&hl=en">Raquel Urtasun@å¤šä¼¦å¤šå¤§å­¦</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=dzOd2hgAAAAJ&view_op=list_works&sortby=pubdate">Philipp KrÃ¤henbÃ¼hl@UT Austin</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=9B8PoXUAAAAJ&view_op=list_works&sortby=pubdate">å¾·ç“¦æ‹‰é©¬å—@CMU</a></li>
<li><a target="_blank" rel="noopener" href="https://jiaya.me/">è´¾å®¶äºš@CUHK</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser@princeton</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=5JlEyTAAAAAJ&view_op=list_works&sortby=pubdate">åˆ—å¥¥å°¼è¾¾æ–¯Â·å‰å·´æ–¯@æ–¯å¦ç¦</a></li>
<li><a target="_blank" rel="noopener" href="https://www.trailab.utias.utoronto.ca/">å²è’‚æ–‡Â·ç“¦æ–¯å…°å¾·@å¤šä¼¦å¤šå¤§å­¦</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=nFefEI8AAAAJ&view_op=list_works&sortby=pubdate">Ouais Alsharif@Google å¤§è„‘</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=i7U4YogAAAAJ&view_op=list_works&sortby=pubdate">æŸ´è‚²å®ï¼ˆå‰ï¼‰@waymo</a></li>
<li><a target="_blank" rel="noopener" href="http://yulanguo.me/">éƒ­ç‰å…°@NUDT</a></li>
<li><a target="_blank" rel="noopener" href="https://www4.comp.polyu.edu.hk/~cslzhang/">å¼ ç£Š@é¦™æ¸¯ç†å·¥å¤§å­¦</a></li>
<li><a target="_blank" rel="noopener" href="https://lihongyang.info/">ææ´ªæ´‹@sensetime</a></li>
</ul>
<h1 id="è‘—åçš„ä»£ç åº“"><a href="#è‘—åçš„ä»£ç åº“" class="headerlink" title="è‘—åçš„ä»£ç åº“"></a>è‘—åçš„ä»£ç åº“</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/PointCloudLibrary/pcl">ç‚¹äº‘åº“ (PCL)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/traveller59/spconv">Spconv</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/poodarchu/Det3D">Det3D</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdetection3d">æ¯«ç±³æ£€æµ‹3d</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/OpenPCDet">å¼€æ”¾PCDet</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tianweiy/CenterPoint">ä¸­å¿ƒç‚¹</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ApolloAuto">Apollo Autoâ€”â€”ç™¾åº¦å¼€æ”¾è‡ªåŠ¨é©¾é©¶å¹³å°</a></li>
<li><a target="_blank" rel="noopener" href="https://www.autoware.org/">AutoWareâ€”â€”ä¸œäº¬å¤§å­¦è‡ªåŠ¨é©¾é©¶å¹³å°</a></li>
<li><a target="_blank" rel="noopener" href="https://comma.ai/">Openpilot â€” ä¸€ç§å¼€æºè½¯ä»¶ï¼Œæ—¨åœ¨æ”¹è¿›å½“ä»Šé“è·¯ä¸Šå¤§å¤šæ•°æ–°è½¦çš„ç°æœ‰é©¾é©¶å‘˜è¾…åŠ©</a></li>
</ul>
<h1 id="æ·±åº¦å­¦ä¹ ç‚¹äº‘å‚è€ƒè®ºæ–‡æ¥æº"><a href="#æ·±åº¦å­¦ä¹ ç‚¹äº‘å‚è€ƒè®ºæ–‡æ¥æº" class="headerlink" title="æ·±åº¦å­¦ä¹ ç‚¹äº‘å‚è€ƒè®ºæ–‡æ¥æº"></a>æ·±åº¦å­¦ä¹ ç‚¹äº‘å‚è€ƒè®ºæ–‡æ¥æº</h1><h1>

<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">- Recent papers (from 2017)</span></span><br></pre></td></tr></table></figure>

</h1>

<h3> Keywords </h3>

<p><strong><code>dat.</code></strong>: dataset &amp;emsp; | &amp;emsp; <strong><code>cls.</code></strong>: classification &amp;emsp; | &amp;emsp; <strong><code>rel.</code></strong>: retrieval &amp;emsp; | &amp;emsp; <strong><code>seg.</code></strong>: segmentation<br><strong><code>det.</code></strong>: detection &amp;emsp; | &amp;emsp; <strong><code>tra.</code></strong>: tracking &amp;emsp; | &amp;emsp; <strong><code>pos.</code></strong>: pose &amp;emsp; | &amp;emsp; <strong><code>dep.</code></strong>: depth<br><strong><code>reg.</code></strong>: registration &amp;emsp; | &amp;emsp; <strong><code>rec.</code></strong>: reconstruction &amp;emsp; | &amp;emsp; <strong><code>aut.</code></strong>: autonomous driving<br><strong><code>oth.</code></strong>: other, including normal-related, correspondence, mapping, matching, alignment, compression, generative modelâ€¦</p>
<p>Statistics: ğŸ”¥ code is available &amp; stars &gt;&#x3D; 100 &amp;emsp;|&amp;emsp; â­ï¸ citation &gt;&#x3D; 50</p>
<hr>
<h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf">CVPR</a>] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/pointnet">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/fxia22/pointnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.pdf">CVPR</a>] Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs. [<strong><code>cls.</code></strong>] â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yi_SyncSpecCNN_Synchronized_Spectral_CVPR_2017_paper.pdf">CVPR</a>] SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/ericyi/SyncSpecCNN">torch</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf">CVPR</a>] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. [<a target="_blank" rel="noopener" href="http://www.scan-net.org/">project</a>][<a target="_blank" rel="noopener" href="http://www.scan-net.org/">git</a>] [<strong><code>dat.</code></strong> <strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Mostegel_Scalable_Surface_Reconstruction_CVPR_2017_paper.pdf">CVPR</a>] Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Straub_Efficient_Global_Point_CVPR_2017_paper.pdf">CVPR</a>] Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures. [<a target="_blank" rel="noopener" href="http://people.csail.mit.edu/jstraub/">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Vongkulbhisal_Discriminative_Optimization_Theory_CVPR_2017_paper.pdf">CVPR</a>] Discriminative Optimization: Theory and Applications to Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Elbaz_3D_Point_Cloud_CVPR_2017_paper.pdf">CVPR</a>] 3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder. [<a target="_blank" rel="noopener" href="https://github.com/gilbaz/LORAX">git</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf">CVPR</a>] Multi-View 3D Object Detection Network for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/bostondiditeam/MV3D">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf">CVPR</a>] 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions. [<a target="_blank" rel="noopener" href="https://github.com/andyzeng/3dmatch-toolbox">code</a>] [<strong><code>dat.</code></strong> <strong><code>pos.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.pdf">CVPR</a>] OctNet: Learning Deep 3D Representations at High Resolutions. [<a target="_blank" rel="noopener" href="https://github.com/griegler/octnet">torch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥ â­ï¸</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Klokov_Escape_From_Cells_ICCV_2017_paper.pdf">ICCV</a>] Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models. [<a target="_blank" rel="noopener" href="https://github.com/fxia22/kdnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>] â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper.pdf">ICCV</a>] 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/CKchaos/scn2pointcloud_tool">code</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Park_Colored_Point_Cloud_ICCV_2017_paper.pdf">ICCV</a>] Colored Point Cloud Registration Revisited. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Nan_PolyFit_Polygonal_Surface_ICCV_2017_paper.pdf">ICCV</a>] PolyFit: Polygonal Surface Reconstruction from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/LiangliangNan/PolyFit">code</a>] [<strong><code>rec.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ladicky_From_Point_Clouds_ICCV_2017_paper.pdf">ICCV</a>] From Point Clouds to Mesh using Regression. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf">ICCV</a>] 3D Graph Neural Networks for RGBD Semantic Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/yanx27/3DGNN_pytorch">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space">NeurIPS</a>] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/pointnet2">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/erikwijmans/Pointnet2_PyTorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6931-deep-sets">NeurIPS</a>] Deep Sets. [<a target="_blank" rel="noopener" href="https://github.com/manzilzaheer/DeepSets">pytorch</a>] [<strong><code>cls.</code></strong>] â­ï¸</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989161">ICRA</a>] Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks. [<a target="_blank" rel="noopener" href="https://github.com/lijiannuist/Vote3Deep_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989591">ICRA</a>] Fast segmentation of 3D point clouds: A paradigm on LiDAR data for autonomous vehicle applications. [<a target="_blank" rel="noopener" href="https://github.com/VincentCheungM/Run_based_segmentation">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989618">ICRA</a>] SegMatch: Segment based place recognition in 3D point clouds. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989664">ICRA</a>] Using 2 point+normal sets for fast registration of point clouds with small overlap. [<strong><code>reg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8202234">IROS</a>] Car detection for autonomous vehicle: LIDAR and vision fusion approach through deep learning framework. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8202239">IROS</a>] 3D object classification with point convolution network. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8205955">IROS</a>] 3D fully convolutional network for vehicle detection in point cloud. [<a target="_blank" rel="noopener" href="https://github.com/yukitsuji/3D_CNN_tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8206488">IROS</a>] Deep learning of directional truncated signed distance function for robust 3D object recognition. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8206584">IROS</a>] Analyzing the quality of matched 3D point clouds of objects. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://segcloud.stanford.edu/segcloud_2017.pdf">3DV</a>] SEGCloud: Semantic Segmentation of 3D Point Clouds. [<a target="_blank" rel="noopener" href="http://segcloud.stanford.edu/">project</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>] â­ï¸</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/ielx7/34/8454009/08046026.pdf?tp=&arnumber=8046026&isnumber=8454009&ref=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8=">TPAMI</a>] Structure-aware Data Consolidation. [<strong><code>oth.</code></strong>]</li>
</ul>
<hr>
<h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf">CVPR</a>] SPLATNet: Sparse Lattice Networks for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/NVlabs/splatnet">caffe</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.pdf">CVPR</a>] Attentional ShapeContextNet for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Mining_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling. [<a target="_blank" rel="noopener" href="http://www.merl.com/research/license#KCNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. [<a target="_blank" rel="noopener" href="http://www.merl.com/research/license#FoldingNet">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.pdf">CVPR</a>] Pointwise Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/scenenn/pointwise">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] PU-Net: Point Cloud Upsampling Network. [<a target="_blank" rel="noopener" href="https://github.com/yulequan/PU-Net">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf">CVPR</a>] SO-Net: Self-Organizing Network for Point Cloud Analysis. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/SO-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.pdf">CVPR</a>] Recurrent Slice Networks for 3D Segmentation of Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/qianguih/RSNet">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf">CVPR</a>] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/SparseConvNet">pytorch</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf">CVPR</a>] Deep Parametric Continuous Convolutional Neural Networks. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf">CVPR</a>] PIXOR: Real-time 3D Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/ankita-kalra/PIXOR">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf">CVPR</a>] SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/laughtervv/SGPN">tensorflow</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. [<a target="_blank" rel="noopener" href="https://github.com/loicland/superpoint_graph">pytorch</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf">CVPR</a>] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/tsinghua-rll/VoxelNet-tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yun_Reflection_Removal_for_CVPR_2018_paper.pdf">CVPR</a>] Reflection Removal for Large-Scale 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">CVPR</a>] Hand PointNet: 3D Hand Pose Estimation using Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/3huo/Hand-Pointnet">pytorch</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.pdf">CVPR</a>] PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition. [<a target="_blank" rel="noopener" href="https://github.com/mikacuy/pointnetvlad.git">tensorflow</a>] [<strong><code>rel.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf">CVPR</a>] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf">CVPR</a>] Density Adaptive Point Set Registration. [<a target="_blank" rel="noopener" href="https://github.com/felja633/DARE">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Birdal_A_Minimalist_Approach_CVPR_2018_paper.pdf">CVPR</a>] A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf">CVPR</a>] Inverse Composition Discriminative Optimization for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf">CVPR</a>] CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles. [<strong><code>tra.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf">CVPR</a>] PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf">CVPR</a>] PointGrid: A Deep Network for 3D Shape Understanding. [<a target="_blank" rel="noopener" href="https://github.com/trucleduc/PointGrid">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf">CVPR</a>] PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation. [<a target="_blank" rel="noopener" href="https://github.com/malavikabindhi/CS230-PointFusion">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf">CVPR</a>] Frustum PointNets for 3D Object Detection from RGB-D Data. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/frustum-pointnets">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf">CVPR</a>] Tangent Convolutions for Dense Prediction in 3D. [<a target="_blank" rel="noopener" href="https://github.com/tatarchm/tangent_conv">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Matheus_Gadelha_Multiresolution_Tree_Networks_ECCV_2018_paper.pdf">ECCV</a>] Multiresolution Tree Networks for 3D Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/matheusgadelha/MRTNet">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper.pdf">ECCV</a>] EC-Net: an Edge-aware Point set Consolidation Network. [<a target="_blank" rel="noopener" href="https://github.com/yulequan/EC-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper.pdf">ECCV</a>] 3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhou_Learning_and_Matching_ECCV_2018_paper.pdf">ECCV</a>] Learning and Matching Multi-View Descriptors for Registration of Point Clouds. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf">ECCV</a>] 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration. [<a target="_blank" rel="noopener" href="https://github.com/yewzijian/3DFeatNet">tensorflow</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.pdf">ECCV</a>] Local Spectral Graph Convolution for Point Set Feature Learning. [<a target="_blank" rel="noopener" href="https://github.com/fate3439/LocalSpecGCN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper.pdf">ECCV</a>] SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. [<a target="_blank" rel="noopener" href="https://github.com/xyf513/SpiderCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yinlong_Liu_Efficient_Global_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Global Point Cloud Registration by Matching Rotation Invariant Features Through Translation Search. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Kejie_Li_Efficient_Dense_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Dense Point Cloud Object Reconstruction using Deformation Vector Fields. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Dario_Rethage_Fully-Convolutional_Point_Networks_ECCV_2018_paper.pdf">ECCV</a>] Fully-Convolutional Point Networks for Large-Scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/drethage/fully-convolutional-point-network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf">ECCV</a>] Deep Continuous Fusion for Multi-Sensor 3D Object Detection. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Benjamin_Eckart_Fast_and_Accurate_ECCV_2018_paper.pdf">ECCV</a>] HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf">ECCV</a>] Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tolga_Birdal_PPF-FoldNet_Unsupervised_Learning_ECCV_2018_paper.pdf">ECCV</a>] PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Zeng_3DContextNet_K-d_Tree_Guided_Hierarchical_Learning_of_Point_Clouds_Using_ECCVW_2018_paper.pdf">ECCVW</a>] 3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.pdf">ECCVW</a>] YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16530/16302">AAAI</a>] Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction. [<a target="_blank" rel="noopener" href="https://github.com/chenhsuanlin/3D-point-cloud-generation">tensorflow</a>] [<strong><code>rec.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/aaai/junzhou_-AAAI-Adaptive_Graph_Convolutional_Neural_NetworksI.pdf">AAAI</a>] Adaptive Graph Convolutional Neural Networks. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7545-unsupervised-learning-of-shape-and-pose-with-differentiable-point-clouds">NeurIPS</a>] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/eldar/differentiable-point-clouds">tensorflow</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points">NeurIPS</a>] PointCNN: Convolution On X-Transformed Points. [<a target="_blank" rel="noopener" href="https://github.com/yangyanli/PointCNN">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/hxdengBerkeley/PointCNN.Pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.02392">ICML</a>] Learning Representations and Generative Models for 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/optas/latent_3d_points">code</a>] [<strong><code>oth.</code></strong>] ğŸ”¥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/ft_gateway.cfm?id=3201301&ftid=1991771&dwn=1&CFID=155708095&CFTOKEN=598df826a5b545a7-3E7CE91C-DE12-F588-FAEEF2551115E64E">TOG</a>] Point Convolutional Neural Networks by Extension Operators. [<a target="_blank" rel="noopener" href="https://github.com/matanatz/pcnn">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09263">SIGGRAPH</a>] P2P-NET: Bidirectional Point Displacement Net for Shape Transform. [<a target="_blank" rel="noopener" href="https://github.com/kangxue/P2P-NET">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.01759">SIGGRAPH Asia</a>] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/viscom-ulm/MCCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.04496">SIGGRAPH</a>] Learning local shape descriptors from part correspondences with multi-view convolutional networks. [<a target="_blank" rel="noopener" href="https://people.cs.umass.edu/~hbhuang/local_mvcnn/index.html">project</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.07659">MM</a>] PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02952">MM</a>] RGCNN: Regularized Graph CNN for Point Cloud Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/tegusi/RGCNN">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10783">MM</a>] Hybrid Point Cloud Attribute Compression Using Slice-based Layered Structure and Block-based Intra Prediction. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462884">ICRA</a>] End-to-end Learning of Multi-sensor 3D Tracking by Detection. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460837">ICRA</a>] Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462926">ICRA</a>] SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/priyankanagaraj1494/Squeezseg">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461257">ICRA</a>] Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461048">ICRA</a>] High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion. [<strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461095">ICRA</a>] Sampled-Point Network for Classification of Deformed Building Element Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460532">ICRA</a>] Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460605">ICRA</a>] Signature of Topologically Persistent Points for 3D Point Cloud Description. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461232">ICRA</a>] A General Pipeline for 3D Detection of Vehicles. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460716">ICRA</a>] Robust and Fast 3D Scan Alignment Using Mutual Information. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460940">ICRA</a>] Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460862">ICRA</a>] Surface-Based Exploration for Autonomous 3D Modeling. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460554">ICRA</a>] Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460887">ICRA</a>] Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460639">ICRA</a>] Real-Time Object Tracking in Sparse Point Clouds Based on 3D Interpolation. [<strong><code>tra.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460825">ICRA</a>] Robust Generalized Point Cloud Registration Using Hybrid Mixture Model. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461049">ICRA</a>] A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461000">ICRA</a>] Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461102">ICRA</a>] Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460910">ICRA</a>] Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460204">ICRA</a>] Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460834">ICRA</a>] Complex Urban LiDAR Data Set. [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IguZjmLf5V0&feature=youtu.be">video</a>] [<strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593693">IROS</a>] CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.[<a target="_blank" rel="noopener" href="https://github.com/epiception/CalibNet">tensorflow</a>] [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593839">IROS</a>] Dynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593733">IROS</a>] A 3D Laparoscopic Imaging System Based on Stereo-Photogrammetry with Random Patterns. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593558">IROS</a>] Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594024">IROS</a>] Octree map based on sparse point cloud and heuristic probability distribution for labeled images. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593854">IROS</a>] PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593953">IROS</a>] Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594299">IROS</a>] LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.[<a target="_blank" rel="noopener" href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM">code</a>] [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593741">IROS</a>] Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594362">IROS</a>] Stereo Camera Localization in 3D LiDAR Maps. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594049">IROS</a>] Joint 3D Proposal Generation and Object Detection from View Aggregation. [<strong><code>det.</code></strong>] â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594318">IROS</a>] Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593910">IROS</a>] Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594175">IROS</a>] NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor. [<a target="_blank" rel="noopener" href="https://github.com/CTTCGeoLab/VI_ROS">code</a>][<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593837">IROS</a>] A 3D Convolutional Neural Network Towards Real-Time Amodal 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594356">IROS</a>] Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models. [<strong><code>seg.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594514">IROS</a>] PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08241">IROS</a>] [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/3DmFV-Net">Tensorflow</a>]3DmFV: Point Cloud Classification and segmentation for unstructured 3D point clouds. [<strong><code>cls.</code></strong> ]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594042">IROS</a>] Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments. [<strong><code>oth.</code></strong> ]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://www.mdpi.com/1424-8220/18/10/3337">SENSORS</a>] SECOND: Sparsely Embedded Convolutional Detection. [<a target="_blank" rel="noopener" href="https://github.com/traveller59/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07289">ACCV</a>] Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds). [<a target="_blank" rel="noopener" href="https://github.com/cgtuebingen/Flex-Convolution">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.00671">3DV</a>] PCN: Point Completion Network. [<a target="_blank" rel="noopener" href="https://github.com/TonythePlaneswalker/pcn">tensorflow</a>] [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01711">ICASSP</a>] A Graph-CNN for 3D Point Cloud Classification. [<a target="_blank" rel="noopener" href="https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification">tensorflow</a>] [<strong><code>cls.</code></strong>] ğŸ”¥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.01195.pdf">ITSC</a>] BirdNet: a 3D Object Detection Framework from LiDAR information. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.00652">arXiv</a>] PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/MVIG-SJTU/pointSIFT">tensorflow</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.07872">arXiv</a>] Spherical Convolutional Neural Network for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07605">arXiv</a>] Adversarial Autoencoders for Generating 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11209">arXiv</a>] Iterative Transformer Network for 3D Point Cloud. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.12543">arXiv</a>] Topology-Aware Surface Reconstruction for Point Clouds. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01402">arXiv</a>] Inferring Point Clouds from Single Monocular Images by Depth Intermediation. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04302">arXiv</a>] Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05276">arXiv</a>] IPOD: Intensive Point-based Object Detector for Point Cloud. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11383">arXiv</a>] Feature Preserving and Uniformity-controllable Point Cloud Simplification on Graph. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.01060">arXiv</a>] POINTCLEANNET: Learning to Denoise and Remove Outliers from Dense Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/mrakotosaon/pointcleannet">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.06199">arXiv</a>] Complex-YOLO: Real-time 3D Object Detection on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03818">arxiv</a>] RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement. [<a target="_blank" rel="noopener" href="https://github.com/Kiwoo/RoarNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11029">arXiv</a>] Multi-column Point-CNN for Sketch Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05591">arXiv</a>] PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention. [<a target="_blank" rel="noopener" href="https://liuziwei7.github.io/projects/PointGrow">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.05795.pdf">arXiv</a>] Point Cloud GAN. [<a target="_blank" rel="noopener" href="https://github.com/chunliangli/Point-Cloud-GAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
</ul>
<hr>
<h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.07601">CVPR</a>] Relation-Shape Convolutional Neural Network for Point Cloud Analysis. [<a target="_blank" rel="noopener" href="https://github.com/Yochengliu/Relation-Shape-CNN">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://raoyongming.github.io/files/SFCNN.pdf">CVPR</a>] Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11397">CVPR</a>] DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds. [<a target="_blank" rel="noopener" href="https://ai4ce.github.io/DeepMapping/">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.07179">CVPR</a>] Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/mileyan/pseudo_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">CVPR</a>] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PointRCNN">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.07016">CVPR</a>] Generating 3D Adversarial Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/xiangchong1/3d-adv-pc">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03375v1">CVPR</a>] Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.08017">CVPR</a>] A-CNN: Annularly Convolutional Neural Networks on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/artemkomarichev/a-cnn">tensorflow</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07246">CVPR</a>] PointConv: Deep Convolutional Networks on 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/DylanWusee/pointconv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11647">CVPR</a>] Path-Invariant Map Networks. [<a target="_blank" rel="noopener" href="https://github.com/zaiweizhang/path_invariance_map_network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02713">CVPR</a>] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding. [<a target="_blank" rel="noopener" href="https://github.com/daerduoCarey/partnet_dataset">code</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.00680">CVPR</a>] GeoNet: Deep Geodesic Networks for Point Cloud Analysis. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.09852">CVPR</a>] Associatively Segmenting Instances and Semantics in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/WXinlong/ASIS">tensorflow</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.08988">CVPR</a>] Supervised Fitting of Geometric Primitives to 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/csimstu2/SPFN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00343">CVPR</a>] Octree guided CNN with Spherical Kernels for 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.09287.pdf">extension</a>] [<a target="_blank" rel="noopener" href="https://github.com/hlei-ziyan/SPH3D-GCN">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Point Cloud Registration using PointNet. [<a target="_blank" rel="noopener" href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00699v1">CVPR</a>] JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields. [<a target="_blank" rel="noopener" href="https://github.com/pqhieu/JSIS3D">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02113">CVPR</a>] Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05784">CVPR</a>] PointPillars: Fast Encoders for Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/nutonomy/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11286">CVPR</a>] Patch-based Progressive 3D Point Set Upsampling. [<a target="_blank" rel="noopener" href="https://github.com/yifita/3PU">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09793">CVPR</a>] PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval. [<a target="_blank" rel="noopener" href="https://github.com/XLechter/PCAN">code</a>] [<strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00709">CVPR</a>] PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/FoggYu/PartNet">pytorch</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02170">CVPR</a>] PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/aseembehl/pointflownet">code</a>] [<strong><code>det.</code></strong> <strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03483">CVPR</a>] SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences. [<a target="_blank" rel="noopener" href="https://github.com/intellhave/SDRSAC">matlab</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04019">CVPR</a>] Deep Reinforcement Learning of Volume-guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03461">CVPR</a>] Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10775v1">CVPR</a>] 3D Point-Capsule Networks. [<a target="_blank" rel="noopener" href="https://github.com/yongheng1991/3D-point-capsule-networks">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.08755">CVPR</a>] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/StanfordVL/MinkowskiEngine">pytorch</a>] [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.06879v2">CVPR</a>] The Perfect Match: 3D Point Cloud Matching with Smoothed Densities. [<a target="_blank" rel="noopener" href="https://github.com/zgojcic/3DSmoothNet">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.10136">CVPR</a>] FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization. [<a target="_blank" rel="noopener" href="https://bitbucket.org/gaowei19951004/poser/src/master/">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.01411">CVPR</a>] FlowNet3D: Learning Scene Flow in 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07782">CVPR</a>] Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.linliang.net/wp-content/uploads/2019/04/CVPR2019_PointClound.pdf">CVPR</a>] ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://jiaya.me/papers/pointweb_cvpr19.pdf">CVPR</a>] PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/hszhao/PointWeb">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.12304">CVPR</a>] RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion. [<a target="_blank" rel="noopener" href="https://github.com/iSarmad/RL-GAN-Net">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Robust &amp; Efficient Point Cloud Registration using PointNet. [<a target="_blank" rel="noopener" href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/332240602_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes">CVPR</a>] Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes. [<a target="_blank" rel="noopener" href="https://github.com/ziquan111/RobustPCLReconstruction">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.00709">CVPR</a>] Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/Nesti-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03320">CVPR</a>] GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf">CVPR</a>] Graph Attention Convolution for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02050">CVPR</a>] Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.08701v1">CVPR</a>] LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03498.pdf">CVPR</a>] LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://sites.google.com/view/lp-3dcnn/home">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.pdf">CVPR</a>] Structural Relational Reasoning of Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.03322">CVPR</a>] 3DN: 3D Deformation Network. [<a target="_blank" rel="noopener" href="https://github.com/laughtervv/3DN">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.pdf">CVPR</a>] Privacy Preserving Image-Based Localization. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR</a>] Argoverse: 3D Tracking and Forecasting With Rich Maps.[<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Giancola_Leveraging_Shape_Completion_for_3D_Siamese_Tracking_CVPR_2019_paper.pdf">CVPR</a>] Leveraging Shape Completion for 3D Siamese Tracking. [<a target="_blank" rel="noopener" href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking">pytorch</a>] [<strong><code>tra.</code></strong> ]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf">CVPRW</a>] Attentional PointNet for 3D-Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/anshulpaigwar/Attentional-PointNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_3D_Local_Features_for_Direct_Pairwise_Registration_CVPR_2019_paper.pdf">CVPR</a>] 3D Local Features for Direct Pairwise Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dovrat_Learning_to_Sample_CVPR_2019_paper.pdf">CVPR</a>] Learning to Sample. [<a target="_blank" rel="noopener" href="https://github.com/orendv/learning_to_sample">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pittaluga_Revealing_Scenes_by_Inverting_Structure_From_Motion_Reconstructions_CVPR_2019_paper.pdf">CVPR</a>] Revealing Scenes by Inverting Structure from Motion Reconstructions. [<a target="_blank" rel="noopener" href="https://github.com/francescopittaluga/invsfm">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf">CVPR</a>] DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image. [<a target="_blank" rel="noopener" href="https://github.com/JiaxiongQ/DeepLiDAR">pytorch</a>] [<strong><code>dep.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_HPLFlowNet_Hierarchical_Permutohedral_Lattice_FlowNet_for_Scene_Flow_Estimation_on_CVPR_2019_paper.pdf">CVPR</a>] HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/laoreja/HPLFlowNet">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09664v1">ICCV</a>] Deep Hough Voting for 3D Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/votenet">pytorch</a>] [<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/VoteNet">tensorflow</a>] [<strong><code>det.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03751">ICCV</a>] DeepGCNs: Can GCNs Go as Deep as CNNs? [<a target="_blank" rel="noopener" href="https://github.com/lightaime/deep_gcns">tensorflow</a>] <a target="_blank" rel="noopener" href="https://github.com/lightaime/deep_gcns_torch">[pytorch]</a> [<strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10844.pdf">ICCV</a>] PU-GAN: a Point Cloud Upsampling Adversarial Network. [<a target="_blank" rel="noopener" href="https://github.com/liruihui/PU-GAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition. [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.12320.pdf">ICCV</a>] PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows. [<a target="_blank" rel="noopener" href="https://github.com/stevenygd/PointFlow">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.12704.pdf">ICCV</a>] Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/11GJzouV6jt_aOpvrJ8l3J5x_R_-m-Lg8/view">ICCV</a>] SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [<a target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/SO-HandNet">code</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11017">ICCV</a>] DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.04616">ICCV</a>] Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data. [<strong><code>cls.</code></strong> <strong><code>dat.</code></strong>] [<a target="_blank" rel="noopener" href="https://github.com/hkust-vgd/scanobjectnn">code</a>] [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/scanobjectnn/">dataset</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.08889">ICCV</a>] KPConv: Flexible and Deformable Convolution for Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/HuguesTHOMAS/KPConv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.06295.pdf">ICCV</a>] ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics. [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/shellnet/">project</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.04422.pdf">ICCV</a>] Point-Based Multi-View Stereo Network. [<a target="_blank" rel="noopener" href="https://github.com/callmeray/PointMVSNet">pytorch</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03669">ICCV</a>] DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/Yochengliu/DensePoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.04153v2">ICCV</a>] DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.06292.pdf">ICCV</a>] 3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions. [<a target="_blank" rel="noopener" href="https://github.com/seowok/TreeGAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10469.pdf">ICCV</a>] Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.html">ICCV</a>] Fully Convolutional Geometric Features. [<a target="_blank" rel="noopener" href="https://github.com/chrischoy/FCGF">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.pdf">ICCV</a>] Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning. [<a target="_blank" rel="noopener" href="https://github.com/phermosilla/TotalDenoising">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00229">ICCV</a>] USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/USIP">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.pdf">ICCV</a>] Interpolated Convolutional Networks for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf">ICCV</a>] PointCloud Saliency Maps. [<a target="_blank" rel="noopener" href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10471.pdf">ICCV</a>] STD: Sparse-to-Dense 3D Object Detector for Point Cloud. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.pdf">ICCV</a>] Accelerated Gravitational Point Set Alignment with Altered Physical Laws. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf">ICCV</a>] Deep Closest Point: Learning Representations for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.pdf">ICCV</a>] Efficient Learning on Point Clouds with Basis Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/sergeyprokudin/bps">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.pdf">ICCV</a>] PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.pdf">ICCV</a>] Dynamic Points Agglomeration for Hierarchical Point Sets Learning. [<a target="_blank" rel="noopener" href="https://github.com/yuyi1005/DPAM">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Unsupervised Multi-Task Feature Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.pdf">ICCV</a>] VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/xianyuMeng/VV-Net-Voxel-VAE-Net-with-Group-Convolutions-for-Point-Cloud-Segmentation">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.pdf">ICCV</a>] GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion. [<a target="_blank" rel="noopener" href="https://github.com/justanhduc/graphx-conv">pytorch</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MeteorNet_Deep_Learning_on_Dynamic_3D_Point_Cloud_Sequences_ICCV_2019_paper.pdf">ICCV</a>] MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences. [<a target="_blank" rel="noopener" href="https://github.com/xingyul/meteornet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.02990">ICCV</a>] Fast Point R-CNN. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.pdf">ICCV</a>] Robust Variational Bayesian Point Set Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf">ICCV</a>] DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.pdf">ICCV</a>] 3D Instance Segmentation via Multi-Task Metric Learning. [<a target="_blank" rel="noopener" href="https://sites.google.com/view/3d-instance-mtml">code</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.pdf">ICCV</a>] 3D Face Modeling From Diverse Raw Scan Data. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.12249">ICCVW</a>] Range Adaptation for 3D Object Detection in LiDAR. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.08396.pdf">NeurIPS</a>] Self-Supervised Deep Learning on Point Clouds by Reconstructing Space. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01140">NeurIPS</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8706-exploiting-local-and-global-structure-for-point-cloud-semantic-segmentation-with-contextual-point-representations.pdf">NeurIPS</a>] Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations. [<a target="_blank" rel="noopener" href="https://github.com/fly519/ELGS">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.03739.pdf">NeurIPS</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8940-pointdan-a-multi-scale-3d-domain-adaption-network-for-point-cloud-representation.pdf">NeurIPS</a>] PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation. [<a target="_blank" rel="noopener" href="https://github.com/canqin001/PointDAN">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=SJeXSo09FQ">ICLR</a>] Learning Localized Generative Models for 3D Point Clouds via Graph Convolution. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07290">ICMLW</a>] LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11731">AAAI</a>] CAPNet: Continuous Approximation Projection For 3D Point Cloud Reconstruction Using 2D Supervision. [<a target="_blank" rel="noopener" href="https://github.com/val-iisc/capnet">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.02565">AAAI</a>] Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network. [<a target="_blank" rel="noopener" href="https://github.com/liuxinhai/Point2Sequence">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://par.nsf.gov/biblio/10086163">AAAI</a>] Point Cloud Processing via Recurrent Set Encoding. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.00333">AAAI</a>] PVRNet: Point-View Relation Neural Network for 3D Shape Recognition. [<a target="_blank" rel="noopener" href="https://github.com/Hxyou/PVRNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://gaoyue.org/paper/HGNN.pdf">AAAI</a>] Hypergraph Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/iMoonLab/HGNN">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.07829">TOG</a>] Dynamic Graph CNN for Learning on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/WangYueFt/dgcnn">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/WangYueFt/dgcnn">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] ğŸ”¥ â­ï¸</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.10170.pdf">TOG</a>] LOGAN: Unpaired Shape Transform in Latent Overcomplete Space. [<a target="_blank" rel="noopener" href="https://github.com/kangxue/LOGAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3355089.3356573">SIGGRAPH Asia</a>] RPM-Net: recurrent prediction of motion and parts from point cloud. [<a target="_blank" rel="noopener" href="https://github.com/Salingo/RPM-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.00575v1">SIGGRAPH Asia</a>] StructureNet: Hierarchical Graph Networks for 3D Shape Generation. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3343031.3351009">MM</a>] MMJN: Multi-Modal Joint Networks for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351061">MM</a>] 3D Point Cloud Geometry Compression on Deep Learning. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351042">MM</a>] SRINet: Learning Strictly Rotation-Invariant Representations for Point Cloud Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/tasx0823/SRINet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3350960">MM</a>] L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351076">MM</a>] Ground-Aware Point Cloud Semantic Segmentation for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/Jaiy/Ground-aware-Seg">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.08996">ICME</a>] Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By Lookup Tables. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04427">ICASSP</a>] 3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation. [<a target="_blank" rel="noopener" href="https://github.com/chaojingduan/Neural-Projection">code</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.06371">BMVC</a>] Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00319">ICRA</a>] Discrete Rotation Equivariance for Point Cloud Recognition. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/rot-equ-net">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.08495">ICRA</a>] SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/xuanyuzhou98/SqueezeSegV2">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.ais.uni-bonn.de/papers/ICRA_2019_Razlaw.pdf">ICRA</a>] Detection and Tracking of Small Objects in Sparse 3D Laser Range Data. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.02553">ICRA</a>] Oriented Point Sampling for Plane Detection in Unorganized Point Clouds. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_1.html">ICRA</a>] Point Cloud Compression for 3D LiDAR Sensor Using Recurrent Neural Network with Residual Blocks. [<a target="_blank" rel="noopener" href="https://github.com/ChenxiTU/Point-cloud-compression-by-RNN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06065">ICRA</a>] Focal Loss in 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/pyun-ram/FL3D">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06267">ICRA</a>] PointNetGPD: Detecting Grasp Configurations from Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/lianghongzhuo/PointNetGPD">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09742">ICRA</a>] 2D3D-MatchNet: Learning to Match Keypoints across 2D Image and 3D Point Cloud. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Speeding up Iterative Closest Point Using Stochastic Gradient Descent. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Uncertainty Estimation for Projecting Lidar Points Onto Camera Images for Moving Platforms. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06405v1">ICRA</a>] BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/VCCIV/BLVD">project</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] A Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arpg.colorado.edu/papers/hmrf_icp.pdf">ICRA</a>] Robust low-overlap 3-D point cloud registration for outlier rejection. [<a target="_blank" rel="noopener" href="https://github.com/JStech/ICP">matlab</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust Generalized Point Set Registration Using Inhomogeneous Hybrid Mixture Models Via Expectation. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07511">ICRA</a>] Dense 3D Visual Mapping via Semantic Simplification. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01649">ICRA</a>] MVX-Net: Multimodal VoxelNet for 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1810.01470">ICRA</a>] CELLO-3D: Estimating the Covariance of ICP in the Real World. [<strong><code>reg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/334720713_EPN_Edge-Aware_PointNet_for_Object_Recognition_from_Multi-View_25D_Point_Clouds">IROS</a>] EPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Merium88/Edge-Aware-PointNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.13030.pdf">IROS</a>] SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles. [<strong><code>oth.</code></strong>] [<strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01643v1.pdf">IROS</a>] PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.10964">IV</a>] End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving. [<strong><code>seg.</code></strong>] [<strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02375">Eurographics Workshop</a>] Generalizing Discrete Convolutions for Unstructured Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/aboulch/ConvPoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.02191">WACV</a>] 3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.06297.pdf">3DV</a>] Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/riconv/">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.11555">3DV</a>] Effective Rotation-invariant Point CNN with Spherical Harmonics kernels. [<a target="_blank" rel="noopener" href="https://github.com/adrienPoulenard/SPHnet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.13538.pdf">TVCG</a>] LassoNet: Deep Lasso-Selection of 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://lassonet.github.io/">project</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02532">arXiv</a>] Fast 3D Line Segment Detection From Unorganized Point Cloud. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01687">arXiv</a>] Point-Cloud Saliency Maps. [<a target="_blank" rel="noopener" href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1901.03006">arXiv</a>] Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers. [<a target="_blank" rel="noopener" href="https://github.com/Daniel-Liu-c0deb0t/3D-Neural-Network-Adversarial-Attacks">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.08396">arxiv</a>] Context Prediction for Unsupervised Deep Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.09280">arXiv</a>] Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.09394">arXiv</a>] NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.05247">arXiv</a>] 3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.10272">arXiv</a>] Zero-shot Learning of 3D Point Cloud Objects. [<a target="_blank" rel="noopener" href="https://github.com/alichr/Zero-shot-Learning-of-3D-Point-Cloud-Objects">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.09847">arXiv</a>] Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.01695">arXiv</a>] Real-time Multiple People Hand Localization in 4D Point Clouds. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.02858">arXiv</a>] Variational Graph Methods for Efficient Point Cloud Sparsification. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05807">arXiv</a>] Neural Style Transfer for Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07918">arXiv</a>] OREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10750">arXiv</a>] FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/LordLiang/FVNet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00069">arXiv</a>] Unpaired Point Cloud Completion on Real Scans using Adversarial Training. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00230">arXiv</a>] MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00817">arXiv</a>] DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric Learning on 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.07537">arXiv</a>] Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>] ğŸ”¥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.10795">arXiv</a>] Graph-based Inpainting for 3D Dynamic Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.11027">arXiv</a>] nuScenes: A multimodal dataset for autonomous driving. [<a target="_blank" rel="noopener" href="https://www.nuscenes.org/overview">link</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.08373">arXiv</a>] 3D Backbone Network for 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/Benzlxs/tDBN">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07605v3">arXiv</a>] Adversarial Autoencoders for Compact Representations of 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/MaciejZamorski/3d-AAE">pytorch</a>] [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.10014.pdf">arXiv</a>] Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.08705">arXiv</a>] GAPNet: Graph Attention based Point Neural Network for Exploiting Local Feature of Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/FrankCAN/GAPNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01140">arXiv</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1906.04173">arXiv</a>] Differentiable Surface Splatting for Point-based Geometry Processing. [<a target="_blank" rel="noopener" href="https://github.com/yifita/DSS">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.10887">arXiv</a>] Spatial Transformer for 3D Points. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03739">arXiv</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08240">arXiv</a>] Neural Point-Based Graphics. [<a target="_blank" rel="noopener" href="https://dmitryulyanov.github.io/neural_point_based_graphics">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.02111.pdf">arXiv</a>] Point Cloud Super Resolution with Adversarial Residual Graph Networks. [<strong><code>oth.</code></strong>] [<a target="_blank" rel="noopener" href="https://github.com/wuhuikai/PointCloudSuperResolution">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.10209.pdf">arXiv</a>] Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.11069v1.pdf">arXiv</a>] StarNet: Targeted Computation for Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo">tensorflow</a>] [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.10168.pdf">arXiv</a>] Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDAR. [<strong><code>tra.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.07650v1.pdf">arXiv</a>] SAWNet: A Spatially Aware Deep Neural Network for 3D Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/balwantraikekutte/SAWNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03670">arXiv</a>] Part-A^2 Net: 3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.03299.pdf">arXiv</a>] PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding Module for Classification and Segmentation. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.08287.pdf">arXiv</a>] PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/hehefan/PointRNN">tensorflow</a>] [<strong><code>tra.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.09798">arXiv</a>] PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points. [<a target="_blank" rel="noopener" href="https://github.com/paul007pl/PointAtrousGraph">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.05279.pdf">arXiv</a>] Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.09040.pdf">arXiv</a>] 3D-Rotation-Equivariant Quaternion Neural Networks. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.11026.pdf">arXiv</a>] Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.12885">arXiv</a>] Geometric Feedback Network for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00202">arXiv</a>] Relation Graph Network for 3D Object Detection in Point Clouds. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.13079.pdf">arXiv</a>] Deformable Filter Convolution for Point Cloud Reasoning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.03264.pdf">arXiv</a>] PU-GCN: Point Cloud Upsampling via Graph Convolutional Network. [<a target="_blank" rel="noopener" href="https://sites.google.com/kaust.edu.sa/pugcn">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.11098.pdf">arXiv</a>] StructEdit: Learning Structural Shape Variations. [<a target="_blank" rel="noopener" href="https://github.com/daerduoCarey/structedit">project</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02984v1.pdf">arXiv</a>] Grid-GCN for Fast and Scalable Point Cloud Learning. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.10150.pdf">arXiv</a>] PointPainting: Sequential Fusion for 3D Object Detection. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.07161.pdf">arXiv</a>] Transductive Zero-Shot Learning for 3D Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.10644.pdf">arXiv</a>] Geometry Sharing Network for 3D Point Cloud Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/MingyeXu/GS-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.12033">arvix</a>] Deep Learning for 3D Point Clouds: A Survey. [<a target="_blank" rel="noopener" href="https://github.com/QingyongHu/SoTA-Point-Cloud">code</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.01800v1.pdf">arXiv</a>] Spectral-GANs for High-Resolution 3D Point-cloud Generation. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12663.pdf">arXiv</a>] Point Attention Network for Semantic Segmentation of 3D Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07137v1.pdf">arXiv</a>] PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.08159">arXiv</a>] 3D Object Recognition with Ensemble Learning â€” A Study of Point Cloud-Based Deep Learning Models. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
</ul>
<hr>
<h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00280">AAAI</a>] Morphing and Sampling Network for Dense Point Cloud Completion. [<a target="_blank" rel="noopener" href="https://github.com/Colin97/MSN-Point-Cloud-Completion">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.05163.pdf">AAAI</a>] TANet: Robust 3D Object Detection from Point Clouds with Triple Attention. [<a target="_blank" rel="noopener" href="https://github.com/happinesslz/TANet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.10775">AAAI</a>] Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.09361">AAAI</a>] PRIN: Pointwise Rotation-Invariant Network. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00497">CVPR</a>] Just Go with the Flow: Self-Supervised Scene Flow Estimation. [<a target="_blank" rel="noopener" href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">code</a>][<strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00195">CVPR</a>] SGAS: Sequential Greedy Architecture Search. [<a target="_blank" rel="noopener" href="https://github.com/lightaime/sgas">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.11236.pdf">CVPR</a>] RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/QingyongHu/RandLA-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.05119">CVPR</a>] Learning multiview 3D point cloud registration. [<a target="_blank" rel="noopener" href="https://github.com/zgojcic/3D_multiview_reg">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.00410.pdf">CVPR</a>] PF-Net: Point Fractal Network for 3D Point Cloud Completion. [<a target="_blank" rel="noopener" href="https://github.com/zztianzz/PF-Net-Point-Fractal-Network.git">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.05679.pdf">CVPR</a>] MLCVNet: Multi-Level Context VoteNet for 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/NUAAXQ/MLCVNet">code</a>] [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_SampleNet_Differentiable_Point_Cloud_Sampling_CVPR_2020_paper.pdf">CVPR</a>] SampleNet: Differentiable Point Cloud Sampling. [<a target="_blank" rel="noopener" href="https://github.com/itailang/SampleNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.html">CVPR</a>] MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment.  [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.01014">CVPR</a>] Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences. [<a target="_blank" rel="noopener" href="https://github.com/XiaoshuiHuang/fmr">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.02545.pdf">CVPR</a>] Attentive Context Normalization for Robust Permutation-Equivariant Learning. [<a target="_blank" rel="noopener" href="https://github.com/vcg-uvic/acne">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.01456.pdf">CVPR</a>] Implicit Functions in Feature Space for Shape Reconstruction and Completion. [<a target="_blank" rel="noopener" href="https://github.com/jchibane/if-net">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.10876.pdf">CVPR</a>] PointAugment: an Auto-Augmentation Framework for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.08487.pdf">WACV</a>] FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.10692">arXiv</a>] ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes. [<strong><code>det.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.12098.pdf">ECCV</a>] Quaternion Equivariant Capsule Networks for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.10985.pdf">ECCV</a>] PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10826">ECCV</a>] DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares. [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/DeepFit">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.11784v2">ECCV</a>] DPDist: Comparing Point Clouds Using Deep Point Cloud Distance. [<a target="_blank" rel="noopener" href="https://github.com/dahliau/DPDist">code</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://hal.inria.fr/hal-02927350/document">IROS</a>] GndNet: Fast Ground Plane Estimation and Point Cloud Segmentation for Autonomous Vehicles. [<a target="_blank" rel="noopener" href="https://github.com/anshulpaigwar/GndNet">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.00118.pdf">ICLR</a>] AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/xingzhehe/AdvectiveNet-An-Eulerian-Lagrangian-Fluidic-Reservoir-for-Point-Cloud-Processing">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04569">arXiv</a>] Parameter-Efficient Person Re-identification in the 3D Space. <a target="_blank" rel="noopener" href="https://github.com/layumi/person-reid-3d">[code]</a>[<strong><code>rel.</code></strong>] ğŸ”¥</li>
</ul>
<h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><ul>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=O3bqkf_Puys">ICLR</a>] PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://hehefan.github.io/pdfs/p4transformer.pdf">CVPR</a>] Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos. [<a target="_blank" rel="noopener" href="https://github.com/hehefan/P4Transformer">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.00987">CVPR</a>] PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/weiyithu/PV-RAFT">code</a>][<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.07647">ICRA</a>] FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection. [<a target="_blank" rel="noopener" href="https://github.com/weiyithu/FGR">code</a>][<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hamdi_MVTN_Multi-View_Transformation_Network_for_3D_Shape_Recognition_ICCV_2021_paper.pdf">ICCV</a>] MVTN: Multi-View Transformation Network for 3D Shape Recognition. [<a target="_blank" rel="noopener" href="https://github.com/ajhamdi/MVTN">code</a>][<strong><code>det.</code></strong> <strong><code>rel.</code></strong>]</li>
</ul>
<h1>

<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">- Datasets</span></span><br></pre></td></tr></table></figure>

</h1>

<ul>
<li>[<a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/">KITTI</a>] The KITTI Vision Benchmark Suite. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://modelnet.cs.princeton.edu/">ModelNet</a>] The Princeton ModelNet . [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.shapenet.org/">ShapeNet</a>]  A collaborative dataset between researchers at Princeton, Stanford and TTIC. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://shapenet.org/download/parts">PartNet</a>] The PartNet dataset provides fine grained part annotation of objects in ShapeNetCore. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://kevinkaixu.net/projects/partnet.html">PartNet</a>] PartNet benchmark from Nanjing University and National University of Defense Technology. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://buildingparser.stanford.edu/dataset.html#Download">S3DIS</a>] The Stanford Large-Scale 3D Indoor Spaces Dataset. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.scan-net.org/">ScanNet</a>] Richly-annotated 3D Reconstructions of Indoor Scenes. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D</a>] The Stanford 3D Scanning Repository. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://staffhome.ecm.uwa.edu.au/~00053650/databases.html">UWA Dataset</a>] . [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://shape.cs.princeton.edu/benchmark/">Princeton Shape Benchmark</a>] The Princeton Shape Benchmark.</li>
<li>[<a target="_blank" rel="noopener" href="http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml">SYDNEY URBAN OBJECTS DATASET</a>] This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://projects.asl.ethz.ch/datasets/doku.php?id=home">ASL Datasets Repository(ETH)</a>] This site is dedicated to provide datasets for the Robotics community with the aim to facilitate result evaluations and comparisons. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong> <strong><code>reg.</code></strong> <strong><code>det</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.semantic3d.net/">Large-Scale Point Cloud Classification Benchmark(ETH)</a>] This benchmark closes the gap and provides a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://asrl.utias.utoronto.ca/datasets/3dmap/">Robotic 3D Scan Repository</a>] The Canadian Planetary Emulation Terrain 3D Mapping Dataset is a collection of three-dimensional laser scans gathered at two unique planetary analogue rover test facilities in Canada.</li>
<li>[<a target="_blank" rel="noopener" href="http://radish.sourceforge.net/">Radish</a>] The Robotics Data Set Repository (Radish for short) provides a collection of standard robotics data sets.</li>
<li>[<a target="_blank" rel="noopener" href="http://data.ign.fr/benchmarks/UrbanAnalysis/#">IQmulus &amp; TerraMobilita Contest</a>] The database contains 3D MLS data from a dense urban environment in Paris (France), composed of 300 million points. The acquisition was made in January 2013. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/">Oakland 3-D Point Cloud Dataset</a>] This repository contains labeled 3-D point cloud laser data collected from a moving platform in a urban environment.</li>
<li>[<a target="_blank" rel="noopener" href="http://kos.informatik.uni-osnabrueck.de/3Dscans/">Robotic 3D Scan Repository</a>] This repository provides 3D point clouds from robotic experimentsï¼Œlog files of robot runs and standard 3D data sets for the robotics community.</li>
<li>[<a target="_blank" rel="noopener" href="http://robots.engin.umich.edu/SoftwareData/Ford">Ford Campus Vision and Lidar Data Set</a>] The dataset is collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck.</li>
<li>[<a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/teichman/stc/">The Stanford Track Collection</a>] This dataset contains about 14,000 labeled tracks of objects as observed in natural street scenes by a Velodyne HDL-64E S2 LIDAR.</li>
<li>[<a target="_blank" rel="noopener" href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL3D+</a>] Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild. [<strong><code>pos.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.kaggle.com/daavoo/3d-mnist">3D MNIST</a>] The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://wad.ai/2019/challenge.html">WAD</a>] [<a target="_blank" rel="noopener" href="http://apolloscape.auto/tracking.html">ApolloScape</a>] The datasets are provided by Baidu Inc. [<strong><code>tra.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://d3u7q4379vrm7e.cloudfront.net/object-detection">nuScenes</a>] The nuScenes dataset is a large-scale autonomous driving dataset.</li>
<li>[<a target="_blank" rel="noopener" href="https://uwaterloo.ca/waterloo-intelligent-systems-engineering-lab/projects/precise-synthetic-image-and-lidar-presil-dataset-autonomous">PreSIL</a>] Depth information, semantic segmentation (images), point-wise segmentation (point clouds), ground point labels (point clouds), and detailed annotations for all vehicles and people. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.00160">paper</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://3dmatch.cs.princeton.edu/">3D Match</a>] Keypoint Matching Benchmark, Geometric Registration Benchmark, RGB-D Reconstruction Datasets. [<strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/VCCIV/BLVD">BLVD</a>] (a) 3D detection, (b) 4D tracking, (c) 5D interactive event recognition and (d) 5D intention prediction. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06405v1">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.03605">PedX</a>] 3D Pose Estimation of Pedestrians, more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.03605">ICRA 2019 paper</a>] [<strong><code>pos.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://usa.honda-ri.com/H3D">H3D</a>] Full-surround 3D multi-object detection and tracking dataset. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.01568">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li><a target="_blank" rel="noopener" href="https://www.argoverse.org/">[Argoverse BY ARGO AI]</a> Two public datasets (3D Tracking and Motion Forecasting) supported by highly detailed maps to test, experiment, and teach self-driving vehicles how to understand the world around them.[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR 2019 paper</a>][<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://niessner.github.io/Matterport/">Matterport3D</a>] RGB-D: 10,800 panoramic views from 194,400 RGB-D images. Annotations: surface reconstructions, camera poses, and 2D and 3D semantic segmentations. Keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and scene classification. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.06158">3DV 2017 paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/niessner/Matterport">code</a>] [<a target="_blank" rel="noopener" href="https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/">blog</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.04758">SynthCity</a>] SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Nine categories. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://level5.lyft.com/dataset/?source=post_page">Lyft Level 5</a>] Include high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://semantic-kitti.org/">SemanticKITTI</a>] Sequential Semantic Segmentation, 28 classes, for autonomous driving. All sequences of KITTI odometry labeled. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01416">ICCV 2019 paper</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://npm3d.fr/paris-lille-3d">NPM3D</a>] The Paris-Lille-3D  has been produced by a Mobile Laser System (MLS) in two different cities in France (Paris and Lille). [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://waymo.com/open/">The Waymo Open Dataset</a>] The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/I2RDL2/ASTAR-3D">A*3D: An Autonomous Driving Dataset in Challeging Environments</a>] A*3D: An Autonomous Driving Dataset in Challeging Environments. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/canqin001/PointDAN">PointDA-10 Dataset</a>] Domain Adaptation for point clouds.</li>
<li>[<a target="_blank" rel="noopener" href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford Robotcar</a>] The dataset captures many different combinations of weather, traffic and pedestrians. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://scale.com/open-datasets/pandaset">PandaSet</a>] Public large-scale dataset for autonomous driving provided by Hesai &amp; Scale. It enables researchers to study challenging urban driving situations using the full sensor suit of a real self-driving-car. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset">3D-FRONT</a> <a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future">3D-FUTURE</a>] [Alibaba] 3D-FRONT contains 10,000 houses (or apartments) and ~70,000 rooms with layout information. 3D-FUTURE contains 20,000+ clean and realistic synthetic scenes in 5,000+ diverse rooms which contain 10,000+ unique high quality 3D instances of furniture.</li>
<li>[<a target="_blank" rel="noopener" href="https://3d.dataset.site/">Campus3D</a>] The Campus3D contains a photogrametry point cloud which has 931.7 million points, covering 1.58 km2 of 6 connected campus regions of NUS. The dataset are point-wisely annotated with a hierarchical structure of 24 semantic labels and contains 2,530 instances based on the labels. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.04968.pdf">MM 2020 paper</a>][<a target="_blank" rel="noopener" href="https://github.com/shinke-li/Campus3D">code</a>][ <strong><code>det.</code></strong> <strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
</ul>
<h1 id="å‚è€ƒæ¥æº"><a href="#å‚è€ƒæ¥æº" class="headerlink" title="å‚è€ƒæ¥æº"></a>å‚è€ƒæ¥æº</h1><p><a target="_blank" rel="noopener" href="https://cvpr2021.thecvf.com/">https://cvpr2021.thecvf.com/</a><br><a target="_blank" rel="noopener" href="https://cvpr2022.thecvf.com/">https://cvpr2022.thecvf.com/</a></p>
<p>è®ºæ–‡ä¸codeæŸ¥è¯¢ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://paperswithcode.com/">https://paperswithcode.com/</a><br>AIè®ºæ–‡æŸ¥è¯¢åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/list/cs.AI/recent">https://arxiv.org/list/cs.AI/recent</a></p>
<p>è®ºæ–‡ï¼š<a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation">https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation</a>ç»¼è¿°ï¼š<a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/survey-computer-vision-2020">https://github.com/extreme-assistant/survey-computer-vision-2020</a>\</p>
<ul>
<li>æ¨èé˜…è¯»ï¼š<code>&lt;br&gt;</code><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation">ICCV2021&#x2F;2019&#x2F;2017 è®ºæ–‡&#x2F;ä»£ç &#x2F;è§£è¯»&#x2F;ç›´æ’­åˆé›†</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/survey-computer-vision">2020-2021å¹´è®¡ç®—æœºè§†è§‰ç»¼è¿°è®ºæ–‡æ±‡æ€»</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/Awesome-CV-Team">å›½å†…å¤–ä¼˜ç§€çš„è®¡ç®—æœºè§†è§‰å›¢é˜Ÿæ±‡æ€»</a></li>
</ul>
</li>
</ul>
<hr>
<h1 id="cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017ï¼ˆPapers-x2F-Codes-x2F-Project-x2F-Paper-readingï¼‰"><a href="#cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017ï¼ˆPapers-x2F-Codes-x2F-Project-x2F-Paper-readingï¼‰" class="headerlink" title="cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017ï¼ˆPapers&#x2F;Codes&#x2F;Project&#x2F;Paper readingï¼‰"></a>cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017ï¼ˆPapers&#x2F;Codes&#x2F;Project&#x2F;Paper readingï¼‰</h1><p>è®ºæ–‡è§£è¯»æ±‡æ€»ï¼š<a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/3031">https://bbs.cvmart.net/articles/3031</a> <code>&lt;br&gt;</code><br>è®ºæ–‡åˆ†ç±»æ±‡æ€»ï¼š<a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4267%60">https://bbs.cvmart.net/articles/4267`</a><br><br><code> 2000~2020å¹´å†å±ŠCVPRæœ€ä½³è®ºæ–‡ä»£ç ï¼Œè§£è¯»ç­‰æ±‡æ€»ï¼šhttp://bbs.cvmart.net/topics/665/CVPR-Best-Paper</code><br><code> </code><br>&#96;</p>
<h1 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h1><p><a href="#8">8. CVPR2021æœ€æ–°ä¿¡æ¯åŠè®ºæ–‡ä¸‹è½½</a><code>&lt;br&gt;</code><br><a href="#7">7. CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹</a><code>&lt;br&gt;</code><br><a href="#6">6. CVPR2020è®ºæ–‡ä¸‹è½½&#x2F;ä»£ç &#x2F;è§£è¯»&#x2F;ç›´æ’­</a><code>&lt;br&gt;</code><br><a href="#5">5. CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹</a><code>&lt;br&gt;</code><br><a href="#4">4. CVPR2019å…¨éƒ¨è®ºæ–‡ä¸‹è½½&#x2F;å¼€æºä»£ç </a><code>&lt;br&gt;</code><br><a href="#3">3. CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹</a><code>&lt;br&gt;</code><br><a href="#2">2. CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«</a><code>&lt;br&gt;</code><br><a href="#1">1. CVPR2018&#x2F;CVPR2017</a><code>&lt;br&gt;</code></p>
<br>
<a name="8">

<h1 id="8-CVPR2021æœ€æ–°è®ºæ–‡åˆ†ç±»æ±‡æ€»-æŒç»­æ›´æ–°"><a href="#8-CVPR2021æœ€æ–°è®ºæ–‡åˆ†ç±»æ±‡æ€»-æŒç»­æ›´æ–°" class="headerlink" title="8.CVPR2021æœ€æ–°è®ºæ–‡åˆ†ç±»æ±‡æ€»(æŒç»­æ›´æ–°)"></a><a href>8.CVPR2021æœ€æ–°è®ºæ–‡åˆ†ç±»æ±‡æ€»(æŒç»­æ›´æ–°)</a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation/blob/master/CVPR2021.md">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReadingï¼Demos&#x2F;ç›´æ’­åˆ†äº«ï¼è®ºæ–‡åˆ†äº«ä¼šç­‰</a><code>&lt;br&gt;</code><ul>
<li><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1TWPkRukz9JC4Br-g_Ws5OA">CVPR2021å…¨éƒ¨è®ºæ–‡ä¸‹è½½ï¼ˆå…±1661ç¯‡ï¼‰</a> æå–ç ï¼šsu7e</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4368">CVPR2021 è®ºæ–‡è§£è¯»æ±‡æ€» + æŠ€æœ¯ç›´æ’­æ±‡æ€»</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4366">CVPR2021 Oralè®ºæ–‡æ±‡æ€»&#x2F;è§£è¯»</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="7">

<h1 id="7-CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt"><a href="#7-CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt" class="headerlink" title="7.CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;"></a><a href>7.CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Ho7qtrpF9FhHGaamkQo6Lw">ä¸€æ–‡çœ‹å°½CVPR2021 2D ç›®æ ‡æ£€æµ‹è®ºæ–‡ï¼ˆ27ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ysfwYQ3sVvXINPzBR91S7A">ä¸€æ–‡çœ‹å°½CVPR2021 å›¾åƒå¼‚å¸¸æ£€æµ‹è®ºæ–‡ï¼ˆ6ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/w1jPD2AbxnENUBgfdFLFSg">ä¸€æ–‡çœ‹å°½CVPR2021 ä¼ªè£…ç›®æ ‡æ£€æµ‹+æ—‹è½¬ç›®æ ‡æ£€æµ‹è®ºæ–‡ï¼ˆ6ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_a0UmZSSxvVUFUGOnMrMhw">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå…¨æ™¯åˆ†å‰²è®ºæ–‡æ±‡æ€»ï¼ˆå…±15ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5832">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šäººå‘˜é‡è¯†åˆ«æ±‡æ€»ï¼ˆå…±26ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5831">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šè¡ŒäººæŠ€æœ¯æ±‡æ€»ï¼ˆå…±7ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5829">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šåŒ»å­¦å½±åƒæ±‡æ€»ï¼ˆå…±22ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5560">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šè¶…åˆ†è¾¨ç‡æ±‡æ€»ï¼ˆå…±32ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5824">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå›¾åƒä¿®å¤æ±‡æ€»ï¼ˆå…±20ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5828">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå›¾åƒå»å™ªæ±‡æ€»ï¼ˆå…±14ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5827">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå»é›¾å»æ¨¡ç³Šæ±‡æ€»ï¼ˆå…±14ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5826">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå›¾åƒè§†é¢‘å»é›¨æ±‡æ€»ï¼ˆå…±10ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5562">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šæ–‡æœ¬å›¾åƒæ±‡æ€»ï¼ˆå…±17ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5811">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šäººè„¸è¯†åˆ«æ±‡æ€»ï¼ˆå…±15ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5810">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šäººè„¸é€ å‡æ£€æµ‹æ±‡æ€»ï¼ˆå…±9ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5809">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šå›¾åƒå‹ç¼©æ±‡æ€»ï¼ˆå…±5ç¯‡ï¼‰</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5830">CVPR2021 è®ºæ–‡å¤§ç›˜ç‚¹ï¼šé¥æ„Ÿä¸èˆªæ‹å½±åƒæ±‡æ€»ï¼ˆå…±7ç¯‡ï¼‰</a></li>
</ul>
<br>
<a name="6">

<h1 id="6-CVPR2020è®ºæ–‡ä¸‹è½½-x2F-ä»£ç -x2F-è§£è¯»-x2F-ç›´æ’­"><a href="#6-CVPR2020è®ºæ–‡ä¸‹è½½-x2F-ä»£ç -x2F-è§£è¯»-x2F-ç›´æ’­" class="headerlink" title="6.CVPR2020è®ºæ–‡ä¸‹è½½&#x2F;ä»£ç &#x2F;è§£è¯»&#x2F;ç›´æ’­"></a><a href>6.CVPR2020è®ºæ–‡ä¸‹è½½&#x2F;ä»£ç &#x2F;è§£è¯»&#x2F;ç›´æ’­</a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2020/blob/master/CVPR2020.md#cvpr2020%E6%9C%80%E6%96%B0%E4%BF%A1%E6%81%AF%E5%8F%8A%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E8%B4%B4paperscodesprojectpaperreadingdemos%E7%9B%B4%E6%92%AD%E5%88%86%E4%BA%AB%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AD%89">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReadingï¼Demos&#x2F;ç›´æ’­åˆ†äº«ï¼è®ºæ–‡åˆ†äº«ä¼šç­‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1UXW6iviZ_d3wpdujNgWJSQ">CVPR2020å…¨éƒ¨è®ºæ–‡ä¸‹è½½ï¼ˆå…±1467ç¯‡ï¼‰</a><code>&lt;br&gt;</code><br>æå–ç ï¼špun7<code>&lt;br&gt;&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/3031">CVPR2020 è®ºæ–‡è§£è¯»æ±‡æ€» + æŠ€æœ¯ç›´æ’­æ±‡æ€»</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="5">

<h1 id="5-CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt"><a href="#5-CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt" class="headerlink" title="5.CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;"></a><a href>5.CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/3028">20.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-åŠ¨ä½œæ£€æµ‹ä¸åŠ¨ä½œåˆ†å‰²ï¼ˆ13ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/3000">19.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-åŠ¨ä½œè¯†åˆ«ï¼ˆ21ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2992">18.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å…‰æµï¼ˆ12ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2964">17.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å›¾åƒä¸è§†é¢‘æ£€ç´¢ï¼ˆ16ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2953">16.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-é¥æ„Ÿä¸èˆªæ‹å½±åƒå¤„ç†è¯†åˆ«ï¼ˆ18ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2923">15.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å›¾åƒè´¨é‡è¯„ä»·ï¼ˆ7ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2903">14.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å›¾åƒä¿®å¤ Inpainting ï¼ˆ7ç¯‡ï¼‰</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2902">13.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å›¾åƒå¢å¼ºä¸å›¾åƒæ¢å¤ï¼ˆ22ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2876">12.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å»é›¨å»é›¾å»æ¨¡ç³Šï¼ˆ8ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2855">11.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-åŒ»å­¦å½±åƒå¤„ç†è¯†åˆ«ï¼ˆ19ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2854">10.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-æŠ å›¾ Matting ï¼ˆ3ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2829">9.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å›¾åƒåˆ†å‰²ï¼ˆ25ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2818">8.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å…¨æ™¯åˆ†å‰²ä¸è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆ8ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2725">7.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-è¶…åˆ†è¾¨ï¼ˆ21ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2732">6.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-ç›®æ ‡æ£€æµ‹ï¼ˆ64ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2720">5.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-äººè„¸æŠ€æœ¯ï¼ˆ64ç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2733">4.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-ç›®æ ‡è·Ÿè¸ªï¼ˆ33ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2778">3.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-æ–‡æœ¬å›¾åƒï¼ˆ16ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2751">2.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-è¡Œäººæ£€æµ‹ä¸é‡è¯†åˆ«ï¼ˆ33ç¯‡ï¼‰</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2806">1.CVPR 2020 è®ºæ–‡å¤§ç›˜ç‚¹-å®ä¾‹åˆ†å‰²ï¼ˆ18ç¯‡ï¼‰</a><code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<p><code>&lt;br&gt;&lt;br&gt;</code></p>
<br>
<a name="4">

<h1 id="4-CVPR2019å…¨éƒ¨è®ºä¸‹è½½-x2F-å¼€æºä»£ç -lt-br-gt"><a href="#4-CVPR2019å…¨éƒ¨è®ºä¸‹è½½-x2F-å¼€æºä»£ç -lt-br-gt" class="headerlink" title="4.CVPR2019å…¨éƒ¨è®ºä¸‹è½½&#x2F;å¼€æºä»£ç &lt;br&gt;"></a><a href>4.CVPR2019å…¨éƒ¨è®ºä¸‹è½½&#x2F;å¼€æºä»£ç <code>&lt;br&gt;</code></a></h1><p><a href>å…¨éƒ¨1294ç¯‡<code>&lt;br&gt;</code></a></p>
<ul>
<li><a href>å…¨éƒ¨é“¾æ¥ï¼šhttp://openaccess.thecvf.com/CVPR2019.py <code>&lt;br&gt;</code></a></li>
<li><a href>ä¸‹è½½é“¾æ¥:<code>&lt;br&gt;</code><br>é“¾æ¥:https://pan.baidu.com/s/1dhXrWFHeKeJ1kFsKBxQzVg  å¯†ç :f53l</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2019/blob/master/cvpr_2019_githublinks.csv">CVPR 2019å…¨éƒ¨è®ºæ–‡å¼€æºæºç æ±‡æ€»Excelç‚¹è¿™é‡Œ</a></li>
</ul>
<p><code>&lt;br&gt;&lt;br&gt;</code></p>
<br>
<a name="3">

<h1 id="3-CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt"><a href="#3-CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹-lt-br-gt" class="headerlink" title="3.CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;"></a><a href>3.CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/articles/523/cvpr-2019-lun-wen-da-pan-dian-mu-biao-gen-zong-pian">CVPR 2019 è®ºæ–‡å¤§ç›˜ç‚¹-ç›®æ ‡è·Ÿè¸ªç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/452/cvpr-2019-lun-wen-da-pan-dian-chao-fen-bian-lv-pian">CVPR 2019 è®ºæ–‡å¤§ç›˜ç‚¹-è¶…åˆ†è¾¨ç‡ç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/451/cvpr-2019-lun-wen-da-pan-dian-ren-lian-ji-shu-pian">CVPR 2019 è®ºæ–‡å¤§ç›˜ç‚¹-äººè„¸æŠ€æœ¯ç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/l8Cfi3CIt2gqVC9i3LV6hw">CVPR 2019 è®ºæ–‡å¤§ç›˜ç‚¹â€”ç›®æ ‡æ£€æµ‹ç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/535/CVPR2019-Text">CVPR 2019 è®ºæ–‡å¤§ç›˜ç‚¹â€”æ–‡æœ¬å›¾åƒç¯‡</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/464/cvpr-2019-gong-bu-mo-xing-jian-zhi-lun-wen-hui-zong">CVPR2019æ¨¡å‹å‰ªæè®ºæ–‡æ±‡æ€»</a><code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<br>
<a name="2">

<h1 id="2-CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«-lt-br-gt"><a href="#2-CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«-lt-br-gt" class="headerlink" title="2.CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«&lt;br&gt;"></a><a href>2.CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/609/CVPR-2019">å¾®è½¯äºšç ”é™¢CVPR2019çº¿ä¸‹åˆ†äº«ä¼šè§†é¢‘å›æ”¾åŠPPTä¸‹è½½</a></li>
<li>3&#x2F;28æ™šç‚¹äº‘åˆ†å‰²åˆ†äº«å›æ”¾<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/351/%E8%81%94%E5%90%88%E5%88%86%E5%89%B2%E7%82%B9%E4%BA%91%E4%B8%AD%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%92%8C%E8%AF%AD%E4%B9%89">ç‹é‘«é¾™ï¼šè”åˆåˆ†å‰²ç‚¹äº‘ä¸­çš„å®ä¾‹å’Œè¯­ä¹‰ï¼ˆå¼€æºï¼Œåˆ—è¡¨id 27)</a><code>&lt;br&gt;</code></li>
<li>4æœˆ18æ—¥æ™šç›®æ ‡æ£€æµ‹åˆ†äº«å›æ”¾<code>&lt;br&gt;</code><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/CvzFG63c1bTuWFSIzNSxBA">CMUè¯¸å®¸è¾°:åŸºäºAnchor-freeç‰¹å¾é€‰æ‹©æ¨¡å—çš„å•é˜¶ç›®æ ‡æ£€æµ‹(CVPR2019ï¼Œåˆ—è¡¨id 88)</a> <code>&lt;br&gt;</code></li>
<li>5æœˆ9æ—¥æ™šå•ç›®æ ‡è·Ÿè¸ªåˆ†äº«å›æ”¾<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/3vlVXQDh6ou8Gdhg4xY2Tg">å¼ å¿—é¹:åŸºäºsiameseç½‘ç»œçš„å•ç›®æ ‡è·Ÿè¸ª(CVPR2019 Oralï¼Œåˆ—è¡¨id 65)</a><code>&lt;br&gt;</code></li>
<li>[5æœˆ30æ—¥æ™šäººè„¸è¯†åˆ«åˆ†äº«å›æ”¾<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/SIHFTbDc_XjbfYfpgwNYeQ">é‚“å¥åº·-CVPR2019:ArcFace æ„å»ºé«˜æ•ˆçš„äººè„¸è¯†åˆ«ç³»ç»Ÿ(CVPR2019ï¼Œåˆ—è¡¨id 243)</a>ï¼š<code>&lt;br&gt;</code></li>
<li>6æœˆ13æ—¥æ™šä¸‰ç»´å¤šäººå¤šè§†è§’å§¿æ€è¯†åˆ«åˆ†äº«å›æ”¾<code>&lt;br&gt;</code><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Td510LMs3UWV_8d5kDgFYw">è‘£å³»å»·ï¼šå¤šè§†è§’ä¸‹å¤šäººä¸‰ç»´å§¿æ€ä¼°è®¡ CVPR2019ï¼Œåˆ—è¡¨id 106</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="1">

<h1 id="1-CVPR2018-x2F-CVPR2017-lt-br-gt"><a href="#1-CVPR2018-x2F-CVPR2017-lt-br-gt" class="headerlink" title="1.CVPR2018&#x2F;CVPR2017&lt;br&gt;"></a><a href>1.CVPR2018&#x2F;CVPR2017<code>&lt;br&gt;</code></a></h1><ul>
<li><a href>CVPR 2018å…¨éƒ¨è®ºæ–‡ä¸‹è½½ç™¾åº¦äº‘é“¾æ¥ï¼šhttps://pan.baidu.com/s/1bhYzNz2TGijUdfPIdyEGtg <code>&lt;br&gt;</code> å¯†ç :gyk2</a></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/articles/56/cvpr-2018-lun-wen-jie-du-ji-jin-190326-geng-xin"><strong>CVPR 2018è®ºæ–‡è§£è¯»æ±‡æ€»</strong></a></li>
<li>CVPR 2017å…¨éƒ¨è®ºæ–‡ä¸‹è½½ç™¾åº¦äº‘é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w">https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w</a>  <code>&lt;br&gt;</code> å¯†ç :o6tu</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27651707"><strong>CVPR 2017è®ºæ–‡è§£è¯»é›†é”¦</strong></a></li>
</ul>
<h3 id="å‚è€ƒé“¾æ¥-lt-br-gt"><a href="#å‚è€ƒé“¾æ¥-lt-br-gt" class="headerlink" title="å‚è€ƒé“¾æ¥&lt;br&gt;"></a>å‚è€ƒé“¾æ¥<code>&lt;br&gt;</code></h3><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A">https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hoya012/CVPR-2019-Paper-Statistics">https://github.com/hoya012/CVPR-2019-Paper-Statistics</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jonahthelion/cvpr_with_code">https://github.com/jonahthelion/cvpr_with_code</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/amusi/daily-paper-computer-vision">https://github.com/amusi/daily-paper-computer-vision</a> <code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<h1 id="NLPs"><a href="#NLPs" class="headerlink" title="NLPs"></a>NLPs</h1><p>Deep learning speech learning library</p>
<p>Py2neo æ‰‹å†Œ<br>Py2neoæ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯åº“å’Œå·¥å…·åŒ…ï¼Œç”¨äºä»Pythonåº”ç”¨ç¨‹åºå’Œå‘½ä»¤è¡Œä¸­ä½¿ç”¨Neo4j ã€‚è¯¥åº“æ”¯æŒ Bolt å’Œ HTTPï¼Œå¹¶æä¾›é«˜çº§ APIã€OGMã€ç®¡ç†å·¥å…·ã€äº¤äº’å¼æ§åˆ¶å°ã€Pygments çš„ Cypher è¯æ³•åˆ†æå™¨ä»¥åŠè®¸å¤šå…¶ä»–èŠ±é‡Œèƒ¡å“¨ã€‚ä»ç‰ˆæœ¬ 2021.1 å¼€å§‹ï¼ŒPy2neo åŒ…å«å¯¹è·¯ç”±çš„å®Œå…¨æ”¯æŒï¼Œæ­£å¦‚ Neo4j é›†ç¾¤æ‰€å…¬å¼€çš„é‚£æ ·ã€‚è¿™å¯ä»¥ä½¿ç”¨neo4j:&#x2F;&#x2F;â€¦URI æˆ–ä¼ é€’routing&#x3D;Trueç»™Graphæ„é€ å‡½æ•°æ¥å¯ç”¨ã€‚<a target="_blank" rel="noopener" href="https://py2neo.org/2021.1/">https://py2neo.org/2021.1/</a></p>
<h1 id="æ•°æ®é›†-1"><a href="#æ•°æ®é›†-1" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h1><p>ä¸­æ–‡ã€è‹±æ–‡NERã€è‹±æ±‰æœºå™¨ç¿»è¯‘æ•°æ®é›†ã€‚ä¸­è‹±æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†ï¼Œä¸­è‹±æ–‡æœºå™¨ç¿»è¯‘æ•°æ®é›†ï¼Œä¸­æ–‡åˆ†è¯æ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/quincyliang/nlp-public-dataset">https://github.com/quincyliang/nlp-public-dataset</a></p>
<p>CTBè¯æ€§æ ‡æ³¨é›†</p>
<p><img src="https://user-images.githubusercontent.com/36963108/170655243-6267fbc4-246d-40f0-8303-7c97a934918a.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/36963108/170656452-738fbb77-03bc-4315-a3fb-d14a50a9b5f6.png" alt="image"></p>
<p>ck: <a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173">https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173</a></p>
<p>æ ‡æ³¨æ ‡ç­¾è¯´æ˜ï¼š<a target="_blank" rel="noopener" href="https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf">https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40332976/article/details/120331450">https://blog.csdn.net/qq_40332976/article/details/120331450</a></p>
<h1 id="èµ„æ–™æ±‡æ€»"><a href="#èµ„æ–™æ±‡æ€»" class="headerlink" title="èµ„æ–™æ±‡æ€»"></a>èµ„æ–™æ±‡æ€»</h1><p>ä¸€ä¸ªè½»é‡çº§ã€ç®€å•æ˜“ç”¨çš„ RNN å”¤é†’è¯ç›‘å¬å™¨: <a target="_blank" rel="noopener" href="https://github.com/MycroftAI/mycroft-precise">https://github.com/MycroftAI/mycroft-precise</a></p>
<p>zh:<a target="_blank" rel="noopener" href="http://fancyerii.github.io/books/mycroft-precise/">http://fancyerii.github.io/books/mycroft-precise/</a></p>
<p>åŸºäºæ ‘è“æ´¾çš„äººå·¥æ™ºèƒ½å°è½¦ï¼Œå®ç°è¯†åˆ«ã€æç¤ºã€æ™ºèƒ½æ—…æ¸¸çº¿è·¯ã€ç¦»çº¿å›¾åƒ:<br><a target="_blank" rel="noopener" href="https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi">https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi</a></p>
<p>ä¸­æ–‡NLPæ•°æ®é›†:<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUEDatasetSearch">https://github.com/CLUEbenchmark/CLUEDatasetSearch</a></p>
<p>æ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p>
<p>ä¸­æ–‡ NLP èµ„æºç²¾é€‰åˆ—è¡¨ ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³èµ„æ–™:<br><a target="_blank" rel="noopener" href="https://github.com/crownpku/Awesome-Chinese-NLP">https://github.com/crownpku/Awesome-Chinese-NLP</a></p>
<p>è§†è§‰èŠå¤©æœºå™¨äºº:<a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/visual-dialog">https://paperswithcode.com/paper/visual-dialog</a></p>
<p>Bert&#x2F;Transformeræ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–åŠ é€Ÿ: <a target="_blank" rel="noopener" href="https://blog.csdn.net/nature553863/article/details/120292394%EF%BC%9A">https://blog.csdn.net/nature553863/article/details/120292394ï¼š</a></p>
<p>å¯ä»¥å‹ç¼© BERT çš„æ‰€æœ‰æ–¹å¼ï¼š<a target="_blank" rel="noopener" href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html">http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html</a><br><a target="_blank" rel="noopener" href="https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html">https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html</a></p>
<p>BERTè½»é‡åŒ–æ¢ç´¢â€”æ¨¡å‹å‰ªæï¼ˆBERT Pruningï¼‰â€”Rasaç»´åº¦å‰ªæ:<a target="_blank" rel="noopener" href="https://blog.csdn.net/ai_1046067944/article/details/103609152">https://blog.csdn.net/ai_1046067944/article/details/103609152</a></p>
<p>å‹ç¼© BERT ä»¥åŠ å¿«é¢„æµ‹é€Ÿåº¦:<a target="_blank" rel="noopener" href="https://rasa.com/blog/compressing-bert-for-faster-prediction-2/">https://rasa.com/blog/compressing-bert-for-faster-prediction-2/</a></p>
<p>è®ºæ–‡ç»¼è¿°ä¸BERTç›¸å…³æœ€æ–°è®ºæ–‡:<a target="_blank" rel="noopener" href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></p>
<p>ä¸­æ–‡è‡ªç„¶è¯­è¨€æ’è¡Œæ¦œåŠè®ºæ–‡æŸ¥è¯¢:<a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/index.html">https://www.cluebenchmarks.com/index.html</a></p>
<p>è®¡ç®—è¯­è¨€å­¦å›½é™…ä¼šè®®è®ºæ–‡é›†:<a target="_blank" rel="noopener" href="https://aclanthology.org/volumes/2020.coling-main/">https://aclanthology.org/volumes/2020.coling-main/</a></p>
<p>è®¡ç®—è¯­è¨€å­¦åä¼šç¬¬ 58 å±Šå¹´ä¼šè®ºæ–‡é›†:<a target="_blank" rel="noopener" href="https://aclanthology.org/volumes/2020.acl-main/">https://aclanthology.org/volumes/2020.acl-main/</a></p>
<p>è®¡ç®—è¯­è¨€å­¦2åä¼š2021å¹´ä¼šè®ºæ–‡æœé›†ï¼š<a target="_blank" rel="noopener" href="https://aclanthology.org/events/acl-2021/">https://aclanthology.org/events/acl-2021/</a></p>
<p>ä¸­æ–‡BERTå…¨è¯æ©è”½é¢„è®­ç»ƒï¼ˆä¸­æ–‡BERT-wwmç³»åˆ—æ¨¡å‹ï¼‰<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p>
<p>ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡è·¨é¢†åŸŸé¢å‘ä»»åŠ¡çš„å¯¹è¯æ•°æ®é›†:<a target="_blank" rel="noopener" href="https://github.com/thu-coai/CrossWOZ">https://github.com/thu-coai/CrossWOZ</a></p>
<p>å…³äºConvLab-2ï¼šç”¨äºæ„å»ºã€è¯„ä¼°å’Œè¯Šæ–­å¯¹è¯ç³»ç»Ÿçš„å¼€æºå·¥å…·åŒ…ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰ï¼š<a target="_blank" rel="noopener" href="https://github.com/thu-coai/ConvLab-2">https://github.com/thu-coai/ConvLab-2</a></p>
<p>è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ (VL-PTM) çš„æœ€æ–°è¿›å±•(è¯­éŸ³è§†è§‰èåˆ):<a target="_blank" rel="noopener" href="https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers">https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers</a></p>
<p>æ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†é˜…è¯»æ¸…å•:<a target="_blank" rel="noopener" href="https://github.com/IsaacChanghau/DL-NLP-Readings">https://github.com/IsaacChanghau/DL-NLP-Readings</a></p>
<p>è§†è§‰é—®ç­” (VQA)ï¼ˆå›¾åƒ&#x2F;è§†é¢‘é—®ç­”ï¼‰ã€è§†è§‰é—®é¢˜ç”Ÿæˆã€è§†è§‰å¯¹è¯ã€è§†è§‰å¸¸è¯†æ¨ç†å’Œç›¸å…³é¢†åŸŸçš„ç²¾é€‰åˆ—è¡¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></p>
<p>æ±‡æ€»å¾—ä¸é”™çš„nlpå­¦ä¹ èµ„æ–™:<a target="_blank" rel="noopener" href="https://jackkuo666.github.io/">https://jackkuo666.github.io/</a></p>
<p>dl4nlpè‡ªç„¶è¯­è¨€å¤„ç†æ·±åº¦å­¦ä¹ è¯¾ç¨‹ææ–™:<a target="_blank" rel="noopener" href="https://github.com/liu-nlp/dl4nlp">https://github.com/liu-nlp/dl4nlp</a></p>
<p>è®ºæ–‡ä¸æ•°æ®é›†ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://www.ai2news.com/area/">https://www.ai2news.com/area/</a></p>
<p>HanLPçš„Pythonæ¥å£ï¼Œæ”¯æŒè‡ªåŠ¨ä¸‹è½½ä¸å‡çº§HanLPï¼Œå…¼å®¹py2ã€py3ã€‚å†…éƒ¨ç®—æ³•ç»è¿‡å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œè€ƒéªŒï¼Œé…å¥—ä¹¦ç±ã€Šè‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨ã€‹å·²ç»å‡ºç‰ˆï¼Œæ¬¢è¿æŸ¥é˜…éšä¹¦ä»£ç :<a target="_blank" rel="noopener" href="https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1">https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1</a></p>
<h1 id="pyhanlpæ–‡æœ¬è®­ç»ƒä¸é¢„æµ‹APIæ¥å£"><a href="#pyhanlpæ–‡æœ¬è®­ç»ƒä¸é¢„æµ‹APIæ¥å£" class="headerlink" title="pyhanlpæ–‡æœ¬è®­ç»ƒä¸é¢„æµ‹APIæ¥å£"></a>pyhanlpæ–‡æœ¬è®­ç»ƒä¸é¢„æµ‹APIæ¥å£</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tests.test_utility <span class="keyword">import</span> ensure_data</span><br><span class="line"></span><br><span class="line">IClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.IClassifier&#x27;</span>)</span><br><span class="line">NaiveBayesClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier&#x27;</span>)</span><br><span class="line"><span class="comment"># ä¸­æ–‡æƒ…æ„ŸæŒ–æ˜è¯­æ–™-ChnSentiCorp æ•°æ®æ¥è‡ªï¼š</span></span><br><span class="line">chn_senti_corp = ensure_data(<span class="string">&quot;ChnSentiCorpæƒ…æ„Ÿåˆ†æé…’åº—è¯„è®º&quot;</span>, <span class="string">&quot;http://file.hankcs.com/corpus/ChnSentiCorp.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">classifier, text</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ã€Š%sã€‹ æƒ…æ„Ÿææ€§æ˜¯ ã€%sã€‘&quot;</span> % (text, classifier.classify(text)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = NaiveBayesClassifier()</span><br><span class="line">    <span class="comment">#  åˆ›å»ºåˆ†ç±»å™¨ï¼Œæ›´é«˜çº§çš„åŠŸèƒ½è¯·å‚è€ƒIClassifierçš„æ¥å£å®šä¹‰</span></span><br><span class="line">    classifier.train(chn_senti_corp)</span><br><span class="line">    <span class="comment">#  è®­ç»ƒåçš„æ¨¡å‹æ”¯æŒæŒä¹…åŒ–ï¼Œä¸‹æ¬¡å°±ä¸å¿…è®­ç»ƒäº†</span></span><br><span class="line">    predict(classifier, <span class="string">&quot;å‰å°å®¢æˆ¿æœåŠ¡æ€åº¦éå¸¸å¥½ï¼æ—©é¤å¾ˆä¸°å¯Œï¼Œæˆ¿ä»·å¾ˆå¹²å‡€ã€‚å†æ¥å†å‰ï¼&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;ç»“æœå¤§å¤±æ‰€æœ›ï¼Œç¯å…‰æ˜æš—ï¼Œç©ºé—´æå…¶ç‹­å°ï¼ŒåºŠå«è´¨é‡æ¶åŠ£ï¼Œæˆ¿é—´è¿˜ä¼´ç€ä¸€è‚¡éœ‰å‘³ã€‚&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;å¯åˆ©ç”¨æ–‡æœ¬åˆ†ç±»å®ç°æƒ…æ„Ÿåˆ†æï¼Œæ•ˆæœä¸æ˜¯ä¸è¡Œ&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>åŸåˆ›æ¥è‡ªè¿™é‡Œ:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0a131e042238">https://www.jianshu.com/p/0a131e042238</a></p>
<p><img src="https://user-images.githubusercontent.com/36963108/188380358-f15635f6-7ff8-4467-9aeb-88ecfef0acaa.png" alt="image"></p>
<p>å¥½ä¸œè¥¿ï¼š<a target="_blank" rel="noopener" href="https://github.com/Kyubyong/nlp_tasks">https://github.com/Kyubyong/nlp_tasks</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/songyingxin/NLPer-Interview">https://github.com/songyingxin/NLPer-Interview</a></p>
<p>æ€»ç»“æ¢³ç†è‡ªç„¶è¯­è¨€å¤„ç†å·¥ç¨‹å¸ˆ(NLP)éœ€è¦ç§¯ç´¯çš„å„æ–¹é¢çŸ¥è¯†ï¼ŒåŒ…æ‹¬é¢è¯•é¢˜ï¼Œå„ç§åŸºç¡€çŸ¥è¯†ï¼Œå·¥ç¨‹èƒ½åŠ›ç­‰ç­‰ï¼Œæå‡æ ¸å¿ƒç«äº‰åŠ› <a target="_blank" rel="noopener" href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p>
<p>å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜<br>ç­”æ¡ˆè§£æ(1)-å²ä¸Šæœ€å…¨Transformeré¢è¯•é¢˜<br>Pytorchä»£ç åˆ†æâ€“å¦‚ä½•è®©Bertåœ¨finetuneå°æ•°æ®é›†æ—¶æ›´â€œç¨³â€ä¸€ç‚¹<br>è§£å†³è€å¤§éš¾é—®é¢˜-å¦‚ä½•ä¸€è¡Œä»£ç å¸¦ä½ éšå¿ƒæ‰€æ¬²é‡æ–°åˆå§‹åŒ–bertçš„æŸäº›å‚æ•°(é™„Pytorchä»£ç è¯¦ç»†è§£è¯»)<br>3åˆ†é’Ÿä»é›¶è§£è¯»Transformerçš„Encoder<br>åŸç‰ˆTransformerçš„ä½ç½®ç¼–ç ç©¶ç«Ÿæœ‰æ²¡æœ‰åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯<br>BNè¸©å‘è®°â€“è°ˆä¸€ä¸‹Batch Normalizationçš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯<br>è°ˆä¸€ä¸‹ç›¸å¯¹ä½ç½®ç¼–ç <br>NLPä»»åŠ¡ä¸­-layer-normæ¯”BatchNormå¥½åœ¨å“ªé‡Œ<br>è°ˆä¸€è°ˆDecoderæ¨¡å—<br>Transformerçš„å¹¶è¡ŒåŒ–<br>Transformerå…¨éƒ¨æ–‡ç« åˆè¾‘<br>RNNçš„æ¢¯åº¦æ¶ˆå¤±æœ‰ä»€ä¹ˆä¸ä¼—ä¸åŒçš„åœ°æ–¹.md<br>VIT-å¦‚ä½•å°†Transformeræ›´å¥½çš„åº”ç”¨åˆ°CVé¢†åŸŸ</p>
<p>å¥½ä¹¦ï¼š<a target="_blank" rel="noopener" href="https://github.com/FudanNLP/nlp-beginner">https://github.com/FudanNLP/nlp-beginner</a></p>
<p>è®ºæ–‡ä¸code:<a target="_blank" rel="noopener" href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p>
<p>è·Ÿè¸ªè‡ªç„¶è¯­è¨€å¤„ç†çš„è¿›å±•:<a target="_blank" rel="noopener" href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p>
<p>å…ƒç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://research.facebook.com/research-areas/">https://research.facebook.com/research-areas/</a></p>
<p>æ­¤é¡¹ç›®æ˜¯æœºå™¨å­¦ä¹ (Machine Learning)ã€æ·±åº¦å­¦ä¹ (Deep Learning)ã€NLPé¢è¯•ä¸­å¸¸è€ƒåˆ°çš„çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°ï¼Œä¹Ÿæ˜¯ä½œä¸ºä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆå¿…ä¼šçš„ç†è®ºåŸºç¡€çŸ¥è¯†ã€‚<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb">https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb</a></p>
<p>NLP-Models-Tensorflow  <a target="_blank" rel="noopener" href="https://github.com/huseinzol05/NLP-Models-Tensorflow">https://github.com/huseinzol05/NLP-Models-Tensorflow</a></p>
<p>ç›¸ä¼¼åº¦ <a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md">https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p>
<p>NLP é¢†åŸŸç»å…¸ä¹¦ç±ã€ŠSpeech and Language Processingã€‹ç¬¬ä¸‰ç‰ˆ   <a target="_blank" rel="noopener" href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p>
<p>é¡¹ç›®æ˜¯æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ã€NLPé¢è¯•ä¸­å¸¸è€ƒåˆ°çš„çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆä¼šå¿…é€‰çš„ç†è®ºåŸºç¡€çŸ¥è¯† <a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p>NLPä»¥åŠç›¸å…³çš„å­¦ä¹ å®è·µ <a target="_blank" rel="noopener" href="https://github.com/jarvisqi/machine_learning">https://github.com/jarvisqi/machine_learning</a></p>
<p>æœºå™¨å­¦ä¹ &amp;æ·±å…¥å­¦ä¹ èµ„æ–™ç¬”è®°&amp;åŸºæœ¬ç®—æ³•å®ç°&amp;èµ„æºæ•´ç†ï¼ˆML &#x2F; CV &#x2F; NLP &#x2F; DMâ€¦ï¼‰<a target="_blank" rel="noopener" href="https://github.com/fire717/Machine-Learning">https://github.com/fire717/Machine-Learning</a></p>
<p>Datawhaleæˆå‘˜æ•´ç†çš„é¢ç»å†…å®¹ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ï¼ŒCVï¼ŒNLPï¼Œæ¨è <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/daily-interview">https://github.com/datawhalechina/daily-interview</a></p>
<p>äººå·¥æ™ºèƒ½å®æˆ˜å¤§å­¦ï¼ˆé¢è¯•ï¼‰å­¦ä¹ è·¯çº¿å›¾  <a target="_blank" rel="noopener" href="https://github.com/tangyudi/Ai-Learn">https://github.com/tangyudi/Ai-Learn</a></p>
<p>2018&#x2F;2019&#x2F;æ ¡æ‹›ç¬”è®°&#x2F;æ˜¥æ‹›&#x2F;ç§‹æ‹›&#x2F;è‡ªç„¶è€…è¯­è¨€å¤„ç†(NLP)&#x2F;æ·±åº¦æœºå™¨å­¦ä¹ (æ·±åº¦å­¦ä¹ )&#x2F;å­¦ä¹ (æœºå™¨å­¦ä¹ ) <a target="_blank" rel="noopener" href="https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese">https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese</a></p>
<p>æ·±åº¦å­¦ä¹ ç®—æ³•æ•™ç¨‹</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial/tree/master">https://github.com/KeKe-Li/tutorial/tree/master</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li>
</ul>
<p>æ·±åº¦å­¦ä¹ 100ä¾‹ã€æ·±åº¦è¯†åˆ«å­¦ä¹ ã€å›¾ç‰‡åˆ†ç±»ã€ç›®æ ‡ã€ç›®æ ‡æ£€æµ‹ã€è‡ªç„¶è¯­è¨€å¤„ç†nlpã€æ–‡æœ¬åˆ†ç±»ã€TensorFlowã€PyTorch  <a target="_blank" rel="noopener" href="https://github.com/kzbkzb/Python-AI">https://github.com/kzbkzb/Python-AI</a></p>
<h1 id="pyhanlpå¥æ³•è®­ç»ƒ"><a href="#pyhanlpå¥æ³•è®­ç»ƒ" class="headerlink" title="pyhanlpå¥æ³•è®­ç»ƒ"></a>pyhanlpå¥æ³•è®­ç»ƒ</h1><p><img src="https://user-images.githubusercontent.com/36963108/169463356-d2faf6c3-557d-49f4-83d7-235ec657c5b3.png" alt="image"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">python train.py </span><br><span class="line">ä¸‹è½½ http://file.hankcs.com/corpus/ctb8.0-dep.zip åˆ° /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/ctb8.0-dep.zip</span><br><span class="line">100%   3.5 MiB 114.3 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">ä¸‹è½½ http://file.hankcs.com/corpus/wiki-cn-cluster.zip åˆ° /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/wiki-cn-cluster.txt.zip</span><br><span class="line">100% 763.9 KiB 353.9 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">è®­ç»ƒé›†å¥å­æ•°é‡: 14863                                                                             </span><br><span class="line">è¿­ä»£ 1/20 100.00%  è€—æ—¶ 544 ç§’ã€‚UAS=83.23 LAS=80.84 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 2/20 100.00%  è€—æ—¶ 569 ç§’ã€‚UAS=84.06 LAS=81.97 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 3/20 100.00%  è€—æ—¶ 557 ç§’ã€‚UAS=84.55 LAS=82.48 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 4/20 100.00%  è€—æ—¶ 556 ç§’ã€‚UAS=84.82 LAS=82.74 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 5/20 100.00%  è€—æ—¶ 553 ç§’ã€‚UAS=84.95 LAS=82.89 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 6/20 100.00%  è€—æ—¶ 549 ç§’ã€‚UAS=85.18 LAS=83.15 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 7/20 100.00%  è€—æ—¶ 560 ç§’ã€‚UAS=85.25 LAS=83.26 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 8/20 100.00%  è€—æ—¶ 562 ç§’ã€‚UAS=85.12 LAS=83.11</span><br><span class="line">è¿­ä»£ 9/20 100.00%  è€—æ—¶ 569 ç§’ã€‚UAS=85.23 LAS=83.24</span><br><span class="line">è¿­ä»£ 10/20 100.00%  è€—æ—¶ 571 ç§’ã€‚UAS=85.17 LAS=83.23</span><br><span class="line">è¿­ä»£ 11/20 100.00%  è€—æ—¶ 571 ç§’ã€‚UAS=85.20 LAS=83.22</span><br><span class="line">è¿­ä»£ 12/20 100.00%  è€—æ—¶ 581 ç§’ã€‚UAS=85.09 LAS=83.16</span><br><span class="line">è¿­ä»£ 13/20 100.00%  è€—æ—¶ 652 ç§’ã€‚UAS=85.16 LAS=83.24</span><br><span class="line">è¿­ä»£ 14/20 100.00%  è€—æ—¶ 677 ç§’ã€‚UAS=85.21 LAS=83.26</span><br><span class="line">è¿­ä»£ 15/20 100.00%  è€—æ—¶ 586 ç§’ã€‚UAS=85.24 LAS=83.31</span><br><span class="line">è¿­ä»£ 16/20 100.00%  è€—æ—¶ 554 ç§’ã€‚UAS=85.26 LAS=83.33 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 17/20 100.00%  è€—æ—¶ 537 ç§’ã€‚UAS=85.38 LAS=83.46 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 18/20 100.00%  è€—æ—¶ 548 ç§’ã€‚UAS=85.43 LAS=83.49 æœ€é«˜åˆ†ï¼ä¿å­˜ä¸­...</span><br><span class="line">è¿­ä»£ 19/20 100.00%  è€—æ—¶ 553 ç§’ã€‚UAS=85.39 LAS=83.43</span><br><span class="line">è¿­ä»£ 20/20 100.00%  è€—æ—¶ 554 ç§’ã€‚UAS=85.41 LAS=83.46</span><br><span class="line">1       äºº      äºº      N       NN      _       2       nsubj   _       _</span><br><span class="line">2       åƒ      åƒ      V       VV      _       0       ROOT    _       _</span><br><span class="line">3       é±¼      é±¼      N       NN      _       2       dobj    _       _</span><br><span class="line"></span><br><span class="line">100 ... 200 ... 300 ... 400 ... 500 ... 600 ... 700 ... 800 ... 900 ... 1000 ... 1100 ... 1200 ... 1300 ... 1400 ... 1500 ... 1600 ... 1700 ... 1800 ... 1900 ... UAS=85.4 LAS=83.5</span><br></pre></td></tr></table></figure>

<h1 id="spacyå¥æ³•"><a href="#spacyå¥æ³•" class="headerlink" title="spacyå¥æ³•"></a>spacyå¥æ³•</h1><p>å¥æ³•æ˜¯æŒ‡å¥å­çš„å„ä¸ªç»„æˆéƒ¨åˆ†çš„ç›¸äº’å…³ç³»ï¼Œå¥æ³•åˆ†æåˆ†ä¸ºå¥æ³•ç»“æ„åˆ†æï¼ˆsyntactic structure parsingï¼‰å’Œä¾å­˜å…³ç³»åˆ†æ(dependency parsing)ã€‚å¥æ³•ç»“æ„åˆ†æç”¨äºè·å–æ•´ä¸ªå¥å­çš„å¥æ³•ç»“æ„ï¼Œä¾å­˜åˆ†æç”¨äºè·å–è¯æ±‡ä¹‹é—´çš„ä¾å­˜å…³ç³»ï¼Œç›®å‰çš„å¥æ³•åˆ†æå·²ç»ä»å¥æ³•ç»“æ„åˆ†æè½¬å‘ä¾å­˜å¥æ³•åˆ†æã€‚</p>
<p>ä¾å­˜è¯­æ³•é€šè¿‡åˆ†æè¯­è¨€å•ä½å†…æˆåˆ†ä¹‹é—´çš„ä¾å­˜å…³ç³»æ­ç¤ºå…¶å¥æ³•ç»“æ„ï¼Œä¸»å¼ å¥å­ä¸­æ ¸å¿ƒåŠ¨è¯æ˜¯æ”¯é…å…¶å®ƒæˆåˆ†çš„ä¸­å¿ƒæˆåˆ†ï¼Œè€Œå®ƒæœ¬èº«å´ä¸å—å…¶å®ƒä»»ä½•æˆåˆ†çš„æ”¯é…ï¼Œæ‰€æœ‰å—æ”¯é…æˆåˆ†éƒ½ä»¥æŸç§ä¾å­˜å…³ç³»ä»å±äºæ”¯é…è€…ã€‚</p>
<p>åœ¨20ä¸–çºª70å¹´ä»£ï¼ŒRobinsonæå‡ºä¾å­˜è¯­æ³•ä¸­å…³äºä¾å­˜å…³ç³»çš„å››æ¡å…¬ç†ï¼š</p>
<ul>
<li>ä¸€ä¸ªå¥å­ä¸­åªæœ‰ä¸€ä¸ªæˆåˆ†æ˜¯ç‹¬ç«‹çš„ï¼›</li>
<li>å…¶å®ƒæˆåˆ†ç›´æ¥ä¾å­˜äºæŸä¸€æˆåˆ†ï¼›</li>
<li>ä»»ä½•ä¸€ä¸ªæˆåˆ†éƒ½ä¸èƒ½ä¾å­˜ä¸ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Šçš„æˆåˆ†ï¼›</li>
<li>å¦‚æœAæˆåˆ†ç›´æ¥ä¾å­˜äºBæˆåˆ†ï¼Œè€ŒCæˆåˆ†åœ¨å¥ä¸­ä½äºAå’ŒBä¹‹é—´ï¼Œé‚£ä¹ˆCæˆ–è€…ç›´æ¥ä¾å­˜äºBï¼Œæˆ–è€…ç›´æ¥ä¾å­˜äºAå’ŒBä¹‹é—´çš„æŸä¸€æˆåˆ†ï¼›</li>
</ul>
<p>SpaCy ä¸­æ–‡æ¨¡å‹:<a target="_blank" rel="noopener" href="https://github.com/howl-anderson/Chinese_models_for_SpaCy">https://github.com/howl-anderson/Chinese_models_for_SpaCy</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/lllhhhv/article/details/123335675">https://blog.csdn.net/lllhhhv/article/details/123335675</a><br>zh_core_web_trfã€zh_core_web_md ç­‰,å®ƒä»¬çš„åŒºåˆ«åœ¨äºå‡†ç¡®åº¦å’Œä½“ç§¯å¤§å°, zh_core_web_sm ä½“ç§¯å°,å‡†ç¡®åº¦ç›¸æ¯”zh_core_web_trfå·®,zh_core_web_trfç›¸å¯¹å°±ä½“ç§¯å¤§ã€‚è¿™æ ·å¯ä»¥é€‚åº”ä¸åŒåœºæ™¯.</p>
<h1 id="æ•°æ®å‚è€ƒ"><a href="#æ•°æ®å‚è€ƒ" class="headerlink" title="æ•°æ®å‚è€ƒ"></a>æ•°æ®å‚è€ƒ</h1><p>hanlp.pretrained.depã€‚CTB5_BIAFFINE_DEP_ZH&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;</a><br>åœ¨ CTB5 ä¸Šè®­ç»ƒçš„Biaffine LSTM æ¨¡å‹ï¼ˆDozat &amp; Manning 2017ï¼‰ã€‚</p>
<p>hanlp.pretrained.depã€‚CTB7_BIAFFINE_DEP_ZH&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;</a><br>åœ¨ CTB7 ä¸Šè®­ç»ƒçš„Biaffine LSTM æ¨¡å‹ï¼ˆDozat &amp; Manning 2017ï¼‰ã€‚</p>
<p>hanlp.pretrained.depã€‚CTB9_DEP_ELECTRA_SMALL&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;">https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;</a><br>Electra å°å‹ç¼–ç å™¨ ( Clark et al. 2020 ) å’Œ Biaffine è§£ç å™¨ ( Dozat &amp; Manning 2017 ) åœ¨ CTB9-SD330 ä¸Šè®­ç»ƒã€‚æ€§èƒ½ä¸º UAS&#x3D;87.68% LAS&#x3D;83.54%ã€‚</p>
<p>hanlp.pretrained.depã€‚CTB9_UDC_ELECTRA_SMALL&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;">https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;</a><br>Electra å°å‹ç¼–ç å™¨ ( Clark et al. 2020 ) å’Œ Biaffine è§£ç å™¨ ( Dozat &amp; Manning 2017 ) åœ¨ CTB9-UD420 ä¸Šè®­ç»ƒã€‚æ€§èƒ½æ˜¯ UAS&#x3D;85.92% LAS&#x3D;81.13% ã€‚</p>
<p>hanlp.pretrained.depã€‚PMT1_DEP_ELECTRA_SMALL&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;">https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;</a><br>Electra å°å‹ç¼–ç å™¨ ( Clark et al. 2020 ) å’Œ Biaffine è§£ç å™¨ ( Dozat &amp; Manning 2017 ) åœ¨ PKU Multi-view Chinese Treebank (PMT) 1.0 ( Qiu et al. 2014 ) ä¸Šè®­ç»ƒã€‚æ€§èƒ½æ˜¯ UAS&#x3D;91.21% LAS&#x3D;88.65%ã€‚</p>
<p>hanlp.pretrained.depã€‚PTB_BIAFFINE_DEP_EN&#x3D; â€˜<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;">https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;</a><br>åœ¨ PTB ä¸Šè®­ç»ƒçš„Biaffine LSTM æ¨¡å‹ï¼ˆDozat &amp; Manning 2017 ï¼‰ã€‚</p>
<p>å‚è€ƒæ¥è‡ªï¼š<a target="_blank" rel="noopener" href="https://hanlp.hankcs.com/docs/api/hanlp/index.html">https://hanlp.hankcs.com/docs/api/hanlp/index.html</a></p>
<h1 id="ctbæ•°æ®é›†ç›¸å…³è®ºæ–‡"><a href="#ctbæ•°æ®é›†ç›¸å…³è®ºæ–‡" class="headerlink" title="ctbæ•°æ®é›†ç›¸å…³è®ºæ–‡"></a>ctbæ•°æ®é›†ç›¸å…³è®ºæ–‡</h1><p><a target="_blank" rel="noopener" href="https://github.com/textflint/textflint.github.io/blob/b387b412642bdaf61fc49173a4e6077c8a0d372a/Tasks/DPCN/paper_list.json">DPCN&#x2F;dataset_paper.json</a></p>
<p>hanlpå¥æ³•åˆ†æè®­ç»ƒé—®é¢˜è§£å†³ï¼š<a target="_blank" rel="noopener" href="https://bbs.hankcs.com/t/topic/2868">https://bbs.hankcs.com/t/topic/2868</a></p>
<h1 id="å‘½åå®ä½“è¯†åˆ«"><a href="#å‘½åå®ä½“è¯†åˆ«" class="headerlink" title="å‘½åå®ä½“è¯†åˆ«:"></a>å‘½åå®ä½“è¯†åˆ«:</h1><p>å‘½åå®ä½“è¯†åˆ«ä»æ—©æœŸåŸºäºè¯å…¸å’Œè§„åˆ™çš„æ–¹æ³•ï¼Œåˆ°ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œ åæ¥é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œä¸€ç›´åˆ°å½“ä¸‹çƒ­é—¨çš„æ³¨æ„åŠ›æœºåˆ¶ã€å›¾ç¥ç»ç½‘ç»œç­‰ç ”ç©¶æ–¹æ³•ï¼Œ å‘½åå®ä½“è¯†åˆ«æŠ€æœ¯è·¯çº¿éšç€æ—¶é—´åœ¨ä¸æ–­å‘å±•ã€‚</p>
<p><img src="https://user-images.githubusercontent.com/36963108/178393815-01045ee4-885e-4aec-9231-c65cfa3af835.png" alt="image"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TianRanPig/chinese_ner">https://github.com/TianRanPig/chinese_ner</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUENER2020">https://github.com/CLUEbenchmark/CLUENER2020</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/hemingkx/CLUENER2020">https://github.com/hemingkx/CLUENER2020</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lonePatient/BERT-NER-Pytorch">https://github.com/lonePatient/BERT-NER-Pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lemonhu/NER-BERT-pytorch">https://github.com/lemonhu/NER-BERT-pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TobiasLee/ChineseNER">https://github.com/TobiasLee/ChineseNER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice">https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/luopeixiang/named_entity_recognition">https://github.com/luopeixiang/named_entity_recognition</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/F-debug/Medical-named-entity-recognition">https://github.com/F-debug/Medical-named-entity-recognition</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kyzhouhzau/BERT-NER">https://github.com/kyzhouhzau/BERT-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/macanv/BERT-BiLSTM-CRF-NER">https://github.com/macanv/BERT-BiLSTM-CRF-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/xuanzebi/BERT-CH-NER">https://github.com/xuanzebi/BERT-CH-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
<p>ä½¿ç”¨bertåšé¢†åŸŸåˆ†ç±»ã€æ„å›¾è¯†åˆ«å’Œæ§½ä½å¡«å……ä»»åŠ¡ <a target="_blank" rel="noopener" href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p>
<p>åŸºäºpytorchçš„ä¸­æ–‡æ„å›¾è¯†åˆ«å’Œæ§½ä½å¡«å…… <a target="_blank" rel="noopener" href="https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling">https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling</a></p>
<p>åŸºäºBERT+Tensorflow-1.15+Horovod-0.22çš„NLUï¼ˆæ„å›¾è¯†åˆ«+æ§½ä½å¡«å……ï¼‰åˆ†å¸ƒå¼GPUè®­ç»ƒæ¨¡å— <a target="_blank" rel="noopener" href="https://github.com/jx1100370217/JointBERT_nlu_tf">https://github.com/jx1100370217/JointBERT_nlu_tf</a></p>
<p>ä½¿ç”¨bertåšé¢†åŸŸåˆ†ç±»ã€é…ç½®è¯†åˆ«å’Œä½ç½®å¡«å……ä»»åŠ¡ <a target="_blank" rel="noopener" href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p>
<p>ä¸­æ–‡è¯­è¨€ç†è§£åŸºå‡†ã€åŸºå‡†ä¸­æ–‡è¯­è¨€ç†è§£è¯„ä¼°åŸºå‡†ï¼šæ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹ã€è¯­æ–™åº“ <a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p>
<p>ç”¨äºè”åˆæ„å›¾åˆ†ç±»å’Œæ’æ§½å¡«å……çš„ BERT <a target="_blank" rel="noopener" href="https://github.com/monologg/JointBERT">https://github.com/monologg/JointBERT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification">https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/pymacbit/BERT-Intent-Classification">https://github.com/pymacbit/BERT-Intent-Classification</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/ensembles4612/medical_intent_detector_using_BERT">https://github.com/ensembles4612/medical_intent_detector_using_BERT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb">https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/sz128/slot_filling_and_intent_detection_of_SLU">https://github.com/sz128/slot_filling_and_intent_detection_of_SLU</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/471417367/bert_intention_zh">https://github.com/471417367/bert_intention_zh</a></p>
<p>æ•°æ®é›†è‡ªåŠ¨æ ‡æ³¨å·¥å…·â€“é‡Šæ”¾AIæ½œåŠ›ï¼<a target="_blank" rel="noopener" href="https://www.modelfun.cn/home">https://www.modelfun.cn/home</a></p>
<p>å®ä½“è¯†åˆ«æ•°æ®é›† <a target="_blank" rel="noopener" href="https://github.com/juand-r/entity-recognition-datasets">https://github.com/juand-r/entity-recognition-datasets</a></p>
<p>nerç»¼è¿°ï¼š <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45884316/article/details/118684681">https://blog.csdn.net/weixin_45884316/article/details/118684681</a></p>
<p>ä½¿ç”¨ CLIP å°†å›¾åƒå’Œå¥å­åµŒå…¥åˆ°å›ºå®šé•¿åº¦çš„å‘é‡ä¸­ <a target="_blank" rel="noopener" href="https://github.com/jina-ai/clip-as-service">https://github.com/jina-ai/clip-as-service</a></p>
<p>other:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset">https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Hyfred/Pytroch_NER_tutorial">https://github.com/Hyfred/Pytroch_NER_tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp">https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py">https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kamalkraj/BERT-NER">https://github.com/kamalkraj/BERT-NER</a></p>
<p>å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/kyzhouhzau/NLPGNN">https://github.com/kyzhouhzau/NLPGNN</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[1] BERTï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘è½¬æ¢å™¨çš„é¢„è®­ç»ƒ</span><br><span class="line">[2] ALBERTï¼šç”¨äºè¯­è¨€è¡¨ç¤ºçš„è‡ªç›‘ç£å­¦ä¹ çš„ Lite BERT</span><br><span class="line">[3]è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ è€…</span><br><span class="line">[4]ç”¨äºé‡å­åŒ–å­¦çš„ç¥ç»æ¶ˆæ¯ä¼ é€’</span><br><span class="line">[ 5]ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œè¿›è¡ŒåŠç›‘ç£åˆ†ç±»</span><br><span class="line">[6]å›¾æ³¨æ„ç½‘ç»œ</span><br><span class="line">[7]å›¾ç¥ç»ç½‘ç»œæœ‰å¤šå¼ºå¤§ï¼Ÿ</span><br><span class="line">[8] GraphSAGEï¼šå¤§å›¾ä¸Šçš„å½’çº³è¡¨ç¤ºå­¦ä¹ </span><br><span class="line">[9]æ‰©æ•£æ”¹è¿›äº†å›¾å­¦ä¹ </span><br><span class="line">[10]åŸºå‡†å›¾ç¥ç»ç½‘ç»œ</span><br><span class="line">[11]ç”¨äºæ–‡æœ¬åˆ†ç±»çš„æ–‡æœ¬çº§å›¾ç¥ç»ç½‘ç»œ</span><br><span class="line">[12]ç”¨äºæ–‡æœ¬åˆ†ç±»çš„å›¾å·ç§¯ç½‘ç»œ</span><br><span class="line">[13]ç”¨äºæ–‡æœ¬åˆ†ç±»çš„å¼ é‡å›¾å·ç§¯ç½‘ç»œ</span><br><span class="line">[14]æ·±å…¥äº†è§£ç”¨äºåŠç›‘ç£å­¦ä¹ çš„å›¾å·ç§¯ç½‘ç»œ</span><br></pre></td></tr></table></figure>

<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ ‡æ³¨ç¥å™¨: <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44193969/article/details/123298406">https://blog.csdn.net/qq_44193969/article/details/123298406</a></p>
<p>å®è·µï¼š<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44193969/article/details/116008734">https://blog.csdn.net/qq_44193969/article/details/116008734</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction">https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40846933/article/details/106384566">https://blog.csdn.net/weixin_40846933/article/details/106384566</a></p>
</a></a></a></a></a></a></a></a>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87/">äººå·¥æ™ºèƒ½è®ºæ–‡</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI%E8%AE%BA%E6%96%87/" rel="tag">AIè®ºæ–‡</a></li></ul>

      
        <div id="donation_div"></div>


<script src="/js/vdonate.js"></script>

<script>
var a = new Donate({
  title: 'å¦‚æœè§‰å¾—æˆ‘çš„æ–‡ç« å¯¹æ‚¨æœ‰ç”¨ï¼Œè¯·éšæ„æ‰“èµã€‚æ‚¨çš„æ”¯æŒå°†é¼“åŠ±æˆ‘ç»§ç»­åˆ›ä½œ!', // å¯é€‰å‚æ•°ï¼Œæ‰“èµæ ‡é¢˜
  btnText: 'æ‰“èµæ”¯æŒ', // å¯é€‰å‚æ•°ï¼Œæ‰“èµæŒ‰é’®æ–‡å­—
  el: document.getElementById('donation_div'),
  wechatImage: 'https://github.com/KangChou/2020PythonKangChou/blob/master/wxzf.jpg',
  alipayImage: ''
});
</script>
      

      <!-- è¦æ·»åŠ çš„å†…å®¹ -->
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>ä½œè€…:  </strong>KangChou</a>
          </li>
          <li class="post-copyright-link">
          <strong>æ–‡ç« é“¾æ¥:  </strong>
          <a href="/2019/07/05/papers/" target="_blank" title="è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½è®ºæ–‡æ±‡é›†">https://www.coomatrix.com/2019/07/05/papers/</a>
          </li>
          <li class="post-copyright-license">
            <strong>ç‰ˆæƒå£°æ˜:   </strong>
            æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼
          </li>
        </ul>
      <div>
      
      <!---->

            
      
        
	<div id="comment">
		<!-- æ¥å¿…åŠ›Cityç‰ˆå®‰è£…ä»£ç  -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC80OTQ4NC8yNTk3Ng==">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>ä¸ºæ­£å¸¸ä½¿ç”¨æ¥å¿…åŠ›è¯„è®ºåŠŸèƒ½è¯·æ¿€æ´»JavaScript</noscript>
		</div>
		<!-- Cityç‰ˆå®‰è£…ä»£ç å·²å®Œæˆ -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/04/05/blog/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">ä¸Šä¸€ç¯‡</strong>
      <div class="article-nav-title">
        
          å€¼å¾—ä½ é˜…è¯»çš„Hexoä¸ªäººåšå®¢æ­å»ºï¼šä¸ç”¨è´­ä¹°æœåŠ¡å™¨ï¼Œä¸ç”¨è´­ä¹°åŸŸåï¼Œä¸è¦é’±ï¼Œä¸ç”¨æ•²ä»£ç ç­‰ç­‰ï¼Œæ˜¯çš„ï¼Œä½ æ²¡æœ‰çœ‹é”™ï¼Œå¿«æ¥è½¬è½½å­¦ä¹ å§ï¼
        
      </div>
    </a>
  
  
    <a href="/2018/04/05/latex/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">ä¸‹ä¸€ç¯‡</strong>
      <div class="article-nav-title">å­¦æœ¯è®ºæ–‡æ’ç‰ˆå·¥å…·LaTeX</div>
    </a>
  
</nav>

  
</article>



<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article" style="overflow-y: scroll; max-width: 28%;">
    <strong class="toc-title">æ–‡ç« ç›®å½•</strong>
    
      <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E4%B9%A6%E7%B1%8D"><span class="nav-number">1.</span> <span class="nav-text">è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ä¹¦ç±</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AI%E4%B9%A6%E7%B1%8D%E4%B8%8E%E7%AE%97%E6%B3%95%E6%BA%90%E7%A0%81"><span class="nav-number">2.</span> <span class="nav-text">AIä¹¦ç±ä¸ç®—æ³•æºç </span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81"><span class="nav-number">3.</span> <span class="nav-text">è®¡ç®—æœºè§†è§‰å®æ—¶åŠ¨æ€</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D-%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B"><span class="nav-number">4.</span> <span class="nav-text">3D å¯¹è±¡æ£€æµ‹</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.</span> <span class="nav-text">æ•°æ®é›†</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A1%B6%E7%BA%A7%E4%BC%9A%E8%AE%AE%E5%92%8C%E7%A0%94%E8%AE%A8%E4%BC%9A"><span class="nav-number">6.</span> <span class="nav-text">é¡¶çº§ä¼šè®®å’Œç ”è®¨ä¼š</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%9A%E8%AE%AE"><span class="nav-number">7.</span> <span class="nav-text">ä¼šè®®</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%9C%E5%9D%8A"><span class="nav-number">8.</span> <span class="nav-text">ä½œåŠ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">è®ºæ–‡ï¼ˆåŸºäºæ¿€å…‰é›·è¾¾çš„æ–¹æ³•ï¼‰</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AB%9E%E8%B5%9B%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">10.</span> <span class="nav-text">ç«èµ›è§£å†³æ–¹æ¡ˆ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B7%A5%E7%A8%8B"><span class="nav-number">11.</span> <span class="nav-text">å·¥ç¨‹</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B0%83%E6%9F%A5"><span class="nav-number">12.</span> <span class="nav-text">è°ƒæŸ¥</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B9%A6"><span class="nav-number">13.</span> <span class="nav-text">ä¹¦</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%86%E9%A2%91"><span class="nav-number">14.</span> <span class="nav-text">è§†é¢‘</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B"><span class="nav-number">15.</span> <span class="nav-text">è¯¾ç¨‹</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%9A%E5%AE%A2"><span class="nav-number">16.</span> <span class="nav-text">åšå®¢</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%91%97%E5%90%8D%E7%A0%94%E7%A9%B6%E7%BB%84-x2F-%E5%AD%A6%E8%80%85"><span class="nav-number">17.</span> <span class="nav-text">è‘—åç ”ç©¶ç»„&#x2F;å­¦è€…</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%91%97%E5%90%8D%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BA%93"><span class="nav-number">18.</span> <span class="nav-text">è‘—åçš„ä»£ç åº“</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%82%B9%E4%BA%91%E5%8F%82%E8%80%83%E8%AE%BA%E6%96%87%E6%9D%A5%E6%BA%90"><span class="nav-number">19.</span> <span class="nav-text">æ·±åº¦å­¦ä¹ ç‚¹äº‘å‚è€ƒè®ºæ–‡æ¥æº</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">20.</span> <span class="nav-text">

1
- Recent papers (from 2017)


</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">20.0.1.</span> <span class="nav-text"> Keywords </span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017"><span class="nav-number">20.1.</span> <span class="nav-text">2017</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2018"><span class="nav-number">20.2.</span> <span class="nav-text">2018</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">20.3.</span> <span class="nav-text">2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2020"><span class="nav-number">20.4.</span> <span class="nav-text">2020</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2021"><span class="nav-number">20.5.</span> <span class="nav-text">2021</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">21.</span> <span class="nav-text">

1
- Datasets


</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%9D%A5%E6%BA%90"><span class="nav-number">22.</span> <span class="nav-text">å‚è€ƒæ¥æº</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017%EF%BC%88Papers-x2F-Codes-x2F-Project-x2F-Paper-reading%EF%BC%89"><span class="nav-number">23.</span> <span class="nav-text">cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017ï¼ˆPapers&#x2F;Codes&#x2F;Project&#x2F;Paper readingï¼‰</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95"><span class="nav-number">24.</span> <span class="nav-text">ç›®å½•</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-CVPR2021%E6%9C%80%E6%96%B0%E8%AE%BA%E6%96%87%E5%88%86%E7%B1%BB%E6%B1%87%E6%80%BB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0"><span class="nav-number">25.</span> <span class="nav-text">8.CVPR2021æœ€æ–°è®ºæ–‡åˆ†ç±»æ±‡æ€»(æŒç»­æ›´æ–°)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-CVPR2021%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">26.</span> <span class="nav-text">7.CVPR2021è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-CVPR2020%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD-x2F-%E4%BB%A3%E7%A0%81-x2F-%E8%A7%A3%E8%AF%BB-x2F-%E7%9B%B4%E6%92%AD"><span class="nav-number">27.</span> <span class="nav-text">6.CVPR2020è®ºæ–‡ä¸‹è½½&#x2F;ä»£ç &#x2F;è§£è¯»&#x2F;ç›´æ’­</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-CVPR2020%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">28.</span> <span class="nav-text">5.CVPR2020è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-CVPR2019%E5%85%A8%E9%83%A8%E8%AE%BA%E4%B8%8B%E8%BD%BD-x2F-%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81-lt-br-gt"><span class="nav-number">29.</span> <span class="nav-text">4.CVPR2019å…¨éƒ¨è®ºä¸‹è½½&#x2F;å¼€æºä»£ç &lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-CVPR2019%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">30.</span> <span class="nav-text">3.CVPR2019è®ºæ–‡åˆ†æ–¹å‘ç›˜ç‚¹&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-CVPR2019%E8%AE%BA%E6%96%87%E7%9B%B4%E6%92%AD%E5%88%86%E4%BA%AB-lt-br-gt"><span class="nav-number">31.</span> <span class="nav-text">2.CVPR2019è®ºæ–‡ç›´æ’­åˆ†äº«&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-CVPR2018-x2F-CVPR2017-lt-br-gt"><span class="nav-number">32.</span> <span class="nav-text">1.CVPR2018&#x2F;CVPR2017&lt;br&gt;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5-lt-br-gt"><span class="nav-number">32.0.1.</span> <span class="nav-text">å‚è€ƒé“¾æ¥&lt;br&gt;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLPs"><span class="nav-number">33.</span> <span class="nav-text">NLPs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="nav-number">34.</span> <span class="nav-text">æ•°æ®é›†</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB"><span class="nav-number">35.</span> <span class="nav-text">èµ„æ–™æ±‡æ€»</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pyhanlp%E6%96%87%E6%9C%AC%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8BAPI%E6%8E%A5%E5%8F%A3"><span class="nav-number">36.</span> <span class="nav-text">pyhanlpæ–‡æœ¬è®­ç»ƒä¸é¢„æµ‹APIæ¥å£</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pyhanlp%E5%8F%A5%E6%B3%95%E8%AE%AD%E7%BB%83"><span class="nav-number">37.</span> <span class="nav-text">pyhanlpå¥æ³•è®­ç»ƒ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spacy%E5%8F%A5%E6%B3%95"><span class="nav-number">38.</span> <span class="nav-text">spacyå¥æ³•</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%82%E8%80%83"><span class="nav-number">39.</span> <span class="nav-text">æ•°æ®å‚è€ƒ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ctb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-number">40.</span> <span class="nav-text">ctbæ•°æ®é›†ç›¸å…³è®ºæ–‡</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB"><span class="nav-number">41.</span> <span class="nav-text">å‘½åå®ä½“è¯†åˆ«:</span></a></li></ol>
    
    </div>
  </aside>


</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2022 ovo$^{mc^2}$ All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              æœ¬ç«™è®¿å®¢æ•°<span id="busuanzi_value_site_uv"></span>äººæ¬¡  
              æœ¬ç«™æ€»è®¿é—®é‡<span id="busuanzi_value_site_pv"></span>æ¬¡
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->

<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/photography" class="mobile-nav-link">photography</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/guestbook" class="mobile-nav-link">guestbook</a>
  
    <a href="/AI" class="mobile-nav-link">AI</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>




<script src="/js/scripts.js"></script>


<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>


<script src="/js/main.js"></script>








  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="https://dnqof95d40fo6.cloudfront.net/atw7f8.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>


