<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    
<script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>

    
<link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">

  
  

  <!-- PACE Progress Bar START -->

  
  <title>计算机科学与人工智能论文汇集 | ovo$^{mc^2}$</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="学习" />
  
  
  
  
  <meta name="description" content="计算机科学技术书籍GO、黑客、Android、计算机原理、人工智能、大数据、机器学习、数据库、PHP、java、架构、消息队列、算法、python、爬虫、操作系统、linux、C语言： https:&#x2F;&#x2F;github.com&#x2F;TIM168&#x2F;technical_books 计算机科学，软件技术，创业，思想类，数学类，人物传记书籍：https:&#x2F;&#x2F;github.com&#x2F;0voice&#x2F;expert_rea">
<meta property="og:type" content="article">
<meta property="og:title" content="计算机科学与人工智能论文汇集">
<meta property="og:url" content="https://www.coomatrix.com/2019/07/05/papers/index.html">
<meta property="og:site_name" content="ovo$^{mc^2}$">
<meta property="og:description" content="计算机科学技术书籍GO、黑客、Android、计算机原理、人工智能、大数据、机器学习、数据库、PHP、java、架构、消息队列、算法、python、爬虫、操作系统、linux、C语言： https:&#x2F;&#x2F;github.com&#x2F;TIM168&#x2F;technical_books 计算机科学，软件技术，创业，思想类，数学类，人物传记书籍：https:&#x2F;&#x2F;github.com&#x2F;0voice&#x2F;expert_rea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png">
<meta property="article:published_time" content="2019-07-05T12:57:06.000Z">
<meta property="article:modified_time" content="2022-11-20T14:45:10.708Z">
<meta property="article:author" content="KangChou">
<meta property="article:tag" content="AI论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/my.css">

  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="ovo$^{mc^2}$" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>

<script>
var themeMenus = {};

  themeMenus["/"] = "首页"; 

  themeMenus["/archives"] = "归档"; 

  themeMenus["/categories"] = "分类"; 

  themeMenus["/tags"] = "标签"; 

  themeMenus["/photography"] = "摄影"; 

  themeMenus["/about"] = "关于"; 

  themeMenus["/guestbook"] = "留言"; 

  themeMenus["/AI"] = "ai"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="ovo$^{mc^2}$" rel="home"> ovo$^{mc^2}$ </a>
            
          </h1>

          
            <div class="site-description">互联网人工智能技术开发者$$世间万物 e^{ i \pi}+1=0 互联互通$$
邮箱:kangsinx@yeah.net     微信公众号(AI科技与算法编程):kangsinx</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/photography">摄影</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/guestbook">留言</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/AI">ai</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/blog.gif,css/images/pose.jpg,css/images/pose2.jpg,css/images/pose3.jpg,css/images/pose4.jpg,https://images.wallpaperscraft.com/image/couple_mountains_travel_125490_1280x720.jpg,https://cdn.pixabay.com/photo/2019/05/26/16/43/norway-4230682_1280.jpg".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;">
<article id="post-papers" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" target="_blank" href="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png" rel="gallery_claph11ci000hz0vd1kh63zgm noopener">
        <img src="https://img-blog.csdnimg.cn/b55a7eb8e4624f2cac0bee5ef4dec9bc.png" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      计算机科学与人工智能论文汇集
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/07/05/papers/" class="article-date">
	  <time datetime="2019-07-05T12:57:06.000Z" itemprop="datePublished">七月 5, 2019</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="计算机科学技术书籍"><a href="#计算机科学技术书籍" class="headerlink" title="计算机科学技术书籍"></a>计算机科学技术书籍</h1><p>GO、黑客、Android、计算机原理、人工智能、大数据、机器学习、数据库、PHP、java、架构、消息队列、算法、python、爬虫、操作系统、linux、C语言：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/TIM168/technical_books">https://github.com/TIM168/technical_books</a></p>
<p>计算机科学，软件技术，创业，思想类，数学类，人物传记书籍：<a target="_blank" rel="noopener" href="https://github.com/0voice/expert_readed_books">https://github.com/0voice/expert_readed_books</a></p>
<p>国内几所大学专业课程资料整理：<a target="_blank" rel="noopener" href="https://github.com/lib-pku/libpku">https://github.com/lib-pku/libpku</a></p>
<p>NLP自然语言处理资料汇总：</p>
<pre><code>中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术：
</code></pre>
<p><a target="_blank" rel="noopener" href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p>
<h1 id="AI书籍与算法源码"><a href="#AI书籍与算法源码" class="headerlink" title="AI书籍与算法源码"></a>AI书籍与算法源码</h1><p>神经网络与深度学习:<a target="_blank" rel="noopener" href="https://nndl.github.io/">https://nndl.github.io/</a></p>
<p>统计学方法：<a target="_blank" rel="noopener" href="https://github.com/SmirkCao/Lihang">https://github.com/SmirkCao/Lihang</a></p>
<p>统计学方法习题答案:<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/">https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/SuperCV/Book">https://github.com/SuperCV/Book</a></p>
<p>机器学习：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/tutorials">https://github.com/MorvanZhou/tutorials</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lawlite19/MachineLearning_Python">https://github.com/lawlite19/MachineLearning_Python</a></li>
<li>西瓜书 <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/pumpkin-book">https://github.com/datawhalechina/pumpkin-book</a></li>
<li>机器视觉:<a target="_blank" rel="noopener" href="https://github.com/Ewenwan/MVision">https://github.com/Ewenwan/MVision</a></li>
<li>视觉算法排名:<a target="_blank" rel="noopener" href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark</a></li>
<li>机器学习资源大全中文版:<a target="_blank" rel="noopener" href="https://github.com/jobbole/awesome-machine-learning-cn">https://github.com/jobbole/awesome-machine-learning-cn</a></li>
<li>机器数学与代码实现:<a target="_blank" rel="noopener" href="https://ailearning.apachecn.org/#/">https://ailearning.apachecn.org/#/</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/fengdu78/lihang-code">https://github.com/fengdu78/lihang-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Dod-o/Statistical-Learning-Method_Code">https://github.com/Dod-o/Statistical-Learning-Method_Code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/WenDesi/lihang_book_algorithm">https://github.com/WenDesi/lihang_book_algorithm</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zslucky/awesome-AI-books">https://github.com/zslucky/awesome-AI-books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/dsgiitr/d2l-pytorch">https://github.com/dsgiitr/d2l-pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/1033020837/Basic4AI">https://github.com/1033020837/Basic4AI</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/xmj-ai/deeplearning_ai_books">https://github.com/xmj-ai/deeplearning_ai_books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lllhhh/BooksKeeper">https://github.com/lllhhh/BooksKeeper</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cosen1024/awesome-cs-books">https://github.com/cosen1024/awesome-cs-books</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/wugenqiang/NoteBook">https://github.com/wugenqiang/NoteBook</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bat67/awesome-ai-books-and-code">https://github.com/bat67/awesome-ai-books-and-code</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/china-testing/python-api-tesing">https://github.com/china-testing/python-api-tesing</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/iamshuaidi/CS-Book">https://github.com/iamshuaidi/CS-Book</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/itdevbooks/pdf">https://github.com/itdevbooks/pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/getsources/CS-Growing-book">https://github.com/getsources/CS-Growing-book</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/861664308/Tensorflow-Keras--">https://github.com/861664308/Tensorflow-Keras--</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/jlgulu/PythonAIPath-Geek">https://github.com/jlgulu/PythonAIPath-Geek</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zhangziliang04/aipm">https://github.com/zhangziliang04/aipm</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Baiyuetribe/paper2gui">https://github.com/Baiyuetribe/paper2gui</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Robinwho/Deep-Learning">https://github.com/Robinwho/Deep-Learning</a></p>
<p>构建开源对话机器人:<a target="_blank" rel="noopener" href="https://github.com/Chinese-NLP-book/rasa_chinese_book_code">https://github.com/Chinese-NLP-book/rasa_chinese_book_code</a></p>
<p><a target="_blank" rel="noopener" href="http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf">http://lnbook.wenqujingdian.com/Public/editor/attached/file/3/018/017/18581.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf">http://home.ustc.edu.cn/~yang96/Elements_of_Information_Theory-second_edition.pdf</a></p>
<p>《Navin Sabharwal - Hands-on Question Answering Systems with BERT_ Applications in Neural Networks and Natural Language Processing-Apress (2021)》</p>
<p>《Sudharsan Ravichandiran - Getting Started with Google BERT_ Build and train state-of-the-art natural language processing models using BERT-Packt Publishing Ltd (2021)》</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/fengdu78/deeplearning_ai_books">https://github.com/fengdu78/deeplearning_ai_books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Microstrong0305/Python2AI">https://github.com/Microstrong0305/Python2AI</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chatopera/Synonyms">https://github.com/chatopera/Synonyms</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/cj0012/AI-Practice-Tensorflow-Notes">https://github.com/cj0012/AI-Practice-Tensorflow-Notes</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/YeonwooSung/ai_book">https://github.com/YeonwooSung/ai_book</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jikexueyuanwiki/tensorflow-zh">https://github.com/jikexueyuanwiki/tensorflow-zh</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/JDHHH/AI-Books">https://github.com/JDHHH/AI-Books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lihanghang/Deep-learning-And-Paper">https://github.com/lihanghang/Deep-learning-And-Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/koryako/FundamentalsOfAI_book_code">https://github.com/koryako/FundamentalsOfAI_book_code</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhangbincheng1997/chatbot-aiml-webqa">https://github.com/zhangbincheng1997/chatbot-aiml-webqa</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/qqqil/books">https://github.com/qqqil/books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/books">https://github.com/KeKe-Li/books</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95">https://github.com/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%96%B9%E6%B3%95</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Dujltqzv/Some-Many-Books">https://github.com/Dujltqzv/Some-Many-Books</a></li>
</ul>
<h1 id="计算机视觉实时动态"><a href="#计算机视觉实时动态" class="headerlink" title="计算机视觉实时动态"></a>计算机视觉实时动态</h1><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/menu">https://openaccess.thecvf.com/menu</a></p>
<p><img src="https://user-images.githubusercontent.com/36963108/193174986-63d2ae54-ee0f-4507-8f32-b75d325d78a9.png" alt="image"></p>
<h1 id="3D-对象检测"><a href="#3D-对象检测" class="headerlink" title="3D 对象检测"></a>3D 对象检测</h1><p><img src="https://miro.medium.com/max/1400/0*JDqH7_kKaGpkoUmv.png"></p>
<p>参考来自： <a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection">https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TianhaoFu/Awesome-3D-Object-Detection">https://github.com/TianhaoFu/Awesome-3D-Object-Detection</a></p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d">KITTI 数据集</a></li>
<li>3,712 个训练样本</li>
<li>3,769 个验证样本</li>
<li>7,518个测试样本</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuScenes 数据集</a></li>
<li>28k 训练样本</li>
<li>6k 验证样本</li>
<li>6k 测试样本</li>
<li><a target="_blank" rel="noopener" href="https://level-5.global/data/perception/">Lyft 数据集</a></li>
<li><a target="_blank" rel="noopener" href="https://waymo.com/open/download/">Waymo 开放数据集</a></li>
<li>798 个训练序列，大约 158、361 个 LiDAR 样本</li>
<li>202 个验证序列，包含 40、077 个 LiDAR 样本。</li>
</ul>
<h1 id="顶级会议和研讨会"><a href="#顶级会议和研讨会" class="headerlink" title="顶级会议和研讨会"></a>顶级会议和研讨会</h1><h1 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h1><ul>
<li>计算机视觉与模式识别会议（CVPR）</li>
<li>计算机视觉国际会议（ICCV）</li>
<li>欧洲计算机视觉会议（ECCV）</li>
</ul>
<h1 id="作坊"><a href="#作坊" class="headerlink" title="作坊"></a>作坊</h1><ul>
<li>CVPR 2019 自动驾驶研讨会（<a target="_blank" rel="noopener" href="http://cvpr2019.wad.vision/">nuScenes 3D detection</a>）</li>
<li>CVPR 2020 自动驾驶研讨会（<a target="_blank" rel="noopener" href="http://cvpr2020.wad.vision/">BDD1k 3D tracking</a>）</li>
<li>CVPR 2021 自动驾驶研讨会（<a target="_blank" rel="noopener" href="http://cvpr2021.wad.vision/">waymo 3D检测</a>）</li>
<li>CVPR 2022 自动驾驶研讨会（<a target="_blank" rel="noopener" href="http://cvpr2022.wad.vision/">waymo 3D检测</a>）</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/cvpr2021-3d-vision-robotics">CVPR 2021 3D 视觉和机器人研讨会</a></li>
<li><a target="_blank" rel="noopener" href="https://scene-understanding.com/">CVPR 2021 视觉、图形和机器人 3D 场景理解研讨会</a></li>
<li><a target="_blank" rel="noopener" href="http://wad.ai/">ICCV 2019 自动驾驶研讨会</a></li>
<li><a target="_blank" rel="noopener" href="https://avvision.xyz/iccv21/">ICCV 2021 自动驾驶汽车视觉研讨会（AVVision）</a>，<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Fan_Autonomous_Vehicle_Vision_2021_ICCV_Workshop_Summary_ICCVW_2021_paper.pdf">注</a></li>
<li><a target="_blank" rel="noopener" href="https://competitions.codalab.org/competitions/33236#learn_the_details">ICCV 2021 研讨会 SSLAD Track 2–3D 对象检测</a></li>
<li><a target="_blank" rel="noopener" href="https://c4av-2020.github.io/">ECCV 2020 自动驾驶汽车指令研讨会</a></li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/pad2020">ECCV 2020 自动驾驶感知研讨会</a></li>
</ul>
<h1 id="论文（基于激光雷达的方法）"><a href="#论文（基于激光雷达的方法）" class="headerlink" title="论文（基于激光雷达的方法）"></a>论文（基于激光雷达的方法）</h1><ul>
<li>用于 LiDAR 点云中 3D 对象检测的端到端多视图融合<a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">论文</a></li>
<li>使用全卷积网络（百度）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.07916">论文从 3D 激光雷达进行车辆检测</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.06396.pdf">VoxelNet：基于点云的 3D 对象检测论文</a>的端到端学习<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.06396.pdf"></a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.08689.pdf">使用深度卷积网络论文</a>在占用网格地图中进行对象检测和分类<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.08689.pdf"></a></li>
<li>RT3D：用于自动驾驶的 LiDAR 点云中的实时 3-D 车辆检测<a target="_blank" rel="noopener" href="https://www.onacademic.com/detail/journal_1000040467923610_4dfe.html">论文</a></li>
<li>BirdNet：来自 LiDAR 信息<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.01195.pdf">论文的 3D 对象检测框架</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04902.pdf">LMNet：使用 3D LiDAR论文</a>在 CPU 上进行实时多类目标检测<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04902.pdf"></a></li>
<li>HDNET: Exploit HD Maps for 3D Object Detection<a href="https://link.zhihu.com/?target=http://proceedings.mlr.press/v87/yang18b/yang18b.pdf">论文</a></li>
<li>PointNet：用于 3D 分类和分割的点集的深度学习<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.00593.pdf">论文</a></li>
<li>PointNet++：度量空间中点集的深度分层特征学习<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02413">论文</a></li>
<li>IPOD: Intensive Point-based Object Detector for Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05276v1">论文</a></li>
<li>PIXOR：来自点云的实时 3D 对象检测<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~wenjie/papers/cvpr18/pixor.pdf">论文</a></li>
<li>DepthCN：车辆检测使用 3D-LIDAR 和 ConvNet<a target="_blank" rel="noopener" href="https://www.baidu.com/link?url=EaE2zYjHkWvF33nsET2eNvbFGFu8-D3wWPia04uyKm95jMetHsSv3Zk-tODPGm5clsgCUgtVULsZ6IQqv0EYS_Z8El7Zzh57XzlJroSkaOuC8yv7r1XXL4bUrM2tWrTgjwqzfMV2tMTnFNbMOmHLTkUobgMg7HKoS6WW6PfQzkG&wd=&eqid=8f320cfa0005b878000000055e528b6d">论文</a></li>
<li>Voxel-FPN：点云 3D 对象检测中的多尺度体素特征聚合<a target="_blank" rel="noopener" href="https://arxiv.org/ftp/arxiv/papers/1907/1907.05286.pdf">论文</a></li>
<li>STD：点云<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.10471">纸的稀疏到密集 3D 对象检测器</a></li>
<li>快速点 R-CNN<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.02990">论文</a></li>
<li>StarNet：点云中目标检测的目标计算<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.11069">论文</a></li>
<li>点云 3D 对象检测<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.09492v1">论文的类平衡分组和采样</a></li>
<li>LaserNet：一种用于自动驾驶<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.08701v1">论文的高效概率 3D 对象检测器</a></li>
<li>FVNet：3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10750v1">论文</a></li>
<li>Part-A² Net：3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03670v1">论文</a></li>
<li>PointRCNN：3D Object Proposal Generation and Detection from Point Cloud<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">论文</a></li>
<li>Complex-YOLO：点云上的实时 3D 对象检测<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.06199">论文</a></li>
<li>YOLO4D: A ST Approach for RT Multi-object Detection and Classification from LiDAR Point Clouds<a target="_blank" rel="noopener" href="https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-object-detection/blob/master">论文</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.02350">YOLO3D：来自 LiDAR 点云论文</a>的端到端实时 3D 面向对象边界框检测<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.02350"></a></li>
<li>使用 Pseudo-LiDAR 点云<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.09847.pdf">论文进行单目 3D 对象检测</a></li>
<li>Structure Aware Single-stage 3D Object Detection from Point Cloud（CVPR2020）<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.html">论文</a> <a target="_blank" rel="noopener" href="https://github.com/skyhehe123/SA-SSD">代码</a></li>
<li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detection（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.05679">论文</a> <a target="_blank" rel="noopener" href="https://github.com/NUAAXQ/MLCVNet">代码</a></li>
<li>3DSSD: Point-based 3D Single Stage Object Detector（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.10187">论文</a> <a target="_blank" rel="noopener" href="https://github.com/tomztyang/3DSSD">代码</a></li>
<li>LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.01389">论文</a> <a target="_blank" rel="noopener" href="https://github.com/yinjunbo/3DVID">代码</a></li>
<li>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection(CVPR2020)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.13192">论文</a> <a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PV-RCNN">代码</a></li>
<li>Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.01251">论文</a> <a target="_blank" rel="noopener" href="https://github.com/WeijingShi/Point-GNN">代码</a></li>
<li>MLCVNet: Multi-Level Context VoteNet for 3D Object Detection（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.05679">论文</a></li>
<li>Density Based Clustering for 3D Object Detection in Point Clouds（CVPR2020）<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Ahmed_Density-Based_Clustering_for_3D_Object_Detection_in_Point_Clouds_CVPR_2020_paper.pdf">论文</a></li>
<li>所见即所得：Exploiting Visibility for 3D Object Detection（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.04986.pdf">论文</a></li>
<li>PointPainting: Sequential Fusion for 3D Object Detection (CVPR2020)<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.10150.pdf">论文</a></li>
<li>HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection（CVPR2020）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.00186">论文</a></li>
<li>LiDAR R-CNN: An Efficient and Universal 3D Object Detector（CVPR2021）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.15297">论文</a></li>
<li>Center-based 3D Object Detection and Tracking (CVPR2021)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11275">论文</a></li>
<li>3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection (CVPR2021)<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.04355.pdf">论文</a></li>
<li>Embracing Single Stride 3D Object Detector with Sparse Transformer（CVPR2022）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.06375.pdf">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/TuSimple/SST">代码</a></li>
<li>Point Density-Aware Voxels for LiDAR 3D Object Detection (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.05662">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/TRAILab/PDV">代码</a></li>
<li>A Unified Query-based Paradigm for Point Cloud Understanding (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">论文</a></li>
<li>Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.01252#:~:text=Abstract%3A%203D%20point%20cloud%20understanding,including%20detection%2C%20segmentation%20and%20classification.">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/Ghostish/Open3DSOT">代码</a></li>
<li>并非所有的点都是平等的：Learning High Efficient Point-based Detectors for 3D LiDAR Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11139">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/yifanzhang713/IA-SSD">代码</a></li>
<li>回到现实：Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement（CVPR2022）<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2203.05238">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/xuxw98/BackToReality">代码</a></li>
<li>Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds (CVPR2022)<a target="_blank" rel="noopener" href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/VoxSeT_cvpr22.pdf">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/skyhehe123/VoxSeT">代码</a></li>
<li>BoxeR: Box-Attention for 2D and 3D Transformers(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.13087">论文</a>,<a target="_blank" rel="noopener" href="https://github.com/kienduynguyen/boxer">代码</a>,<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UnUJJBwcAsRgz6TnQf_b7w">中文介绍</a></li>
<li>规范投票：Towards Robust Oriented Bounding Box Detection in 3D Scenes (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.12001">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/CanonicalVoting">代码</a></li>
<li>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.08195">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo">代码</a></li>
<li>TransFusion：使用 Transformers 进行 3D 对象检测的稳健 LiDAR-Camera Fusion。(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11496">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/xuyangbai/transfusion">代码</a></li>
<li>Point2Seq：将 3D 对象检测为序列。(CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.13394">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/ocnflag/point2seq">代码</a></li>
<li>CAT-Det：用于多模态 3D 对象检测的对比增强变压器（CVPR2022）<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.00325">论文</a></li>
<li>LiDAR Snowfall Simulation for Robust 3D Object Detection (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15118">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/syscv/lidar_snow_sim">代码</a></li>
<li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15175">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/visionml/pytracking">代码</a></li>
<li>Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09780">论文</a></li>
<li>Unified Transformer Tracker for Object Tracking (CVPR2022)<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15175">论文</a>，<a target="_blank" rel="noopener" href="https://github.com/visionml/pytracking">代码</a></li>
</ul>
<h1 id="竞赛解决方案"><a href="#竞赛解决方案" class="headerlink" title="竞赛解决方案"></a>竞赛解决方案</h1><h1 id="工程"><a href="#工程" class="headerlink" title="工程"></a>工程</h1><h1 id="调查"><a href="#调查" class="headerlink" title="调查"></a>调查</h1><ul>
<li>2021.04 用于自动驾驶应用的基于点云的 3D 对象检测和分类方法：调查和分类<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520304097">论文</a></li>
<li>2021.07 用于自动驾驶的 3D 对象检测：调查<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.10823">论文</a></li>
<li>2021.07 自动驾驶中的多模态 3D 对象检测：调查<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.12735">论文</a></li>
<li>2021.10 基于激光雷达的 3D 物体检测方法与深度学习的自动驾驶<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0097849321001321">论文综合调查</a></li>
<li>2021.12 3D 点云的深度学习：调查<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9127813">论文</a></li>
</ul>
<h1 id="书"><a href="#书" class="headerlink" title="书"></a>书</h1><ul>
<li>基于激光雷达和摄像头的 3D 对象检测算法：设计与仿真<a target="_blank" rel="noopener" href="https://www.amazon.com/Object-Detection-Algorithms-Based-Camera/dp/6200536538">书</a></li>
</ul>
<h1 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h1><ul>
<li>Aivia 在线研讨会：3D 对象检测和跟踪<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=P0TrkwAdFYQ">视频</a></li>
<li>3D 对象检索 2021 研讨会<a target="_blank" rel="noopener" href="https://3dor2021.github.io/programme.html">视频</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4">来自 UCSD视频</a>的 SU 实验室的 3D 深度学习教程<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4"></a></li>
<li>讲座：自动驾驶汽车（图宾根大学 Andreas Geiger 教授）<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vfL6uJYFrp4">视频</a></li>
<li>点云对象的当前方法和未来方向 (2021.04)<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xFFCQVwYeec">视频</a></li>
<li>CPU 上 30+ FPS 的最新 3D 对象检测 — MediaPipe 和 OpenCV Python (2021.05)<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=f-Ibri14KMY">视频</a></li>
<li>MIT自动驾驶研讨会（2019.11）<a target="_blank" rel="noopener" href="https://space.bilibili.com/174493426/channel/series">视频</a></li>
<li>sensetime 研讨会1<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Bf4y1b7PF?spm_id_from=333.999.0.0">视频</a></li>
<li>sensetime 研讨会 2<a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/11CoKCxRFgzbIujMXxTZjHDo_hV0arEQ7sUFWFXWaX8o/edit#slide=id.p1">张幻灯片</a></li>
</ul>
<h1 id="课程"><a href="#课程" class="headerlink" title="课程"></a>课程</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~urtasun/courses/CSC2541/06_3D_detection.pdf">多伦多大学，csc2541</a></li>
<li><a target="_blank" rel="noopener" href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/">图宾根大学，自动驾驶汽车</a> <em>（强烈推荐）</em></li>
<li><a target="_blank" rel="noopener" href="https://apollo.auto/devcenter/devcenter.html">百度-Udacity</a></li>
<li><a target="_blank" rel="noopener" href="http://bit.baidu.com/Subject/index/id/16.html">百度-阿波罗</a></li>
<li><a target="_blank" rel="noopener" href="https://www.coursera.org/specializations/self-driving-cars?ranMID=40328&ranEAID=9IqCvd3EEQc&ranSiteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&siteID=9IqCvd3EEQc-MlZGCwEU2294XsVYWDNwzw&utm_content=10&utm_medium=partners&utm_source=linkshare&utm_campaign=9IqCvd3EEQc">多伦多大学，课程</a></li>
</ul>
<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><ul>
<li><a target="_blank" rel="noopener" href="https://blog.waymo.com/">Waymo 博客</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142401769">apollo介绍之感知模块</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/daohu527/Dig-into-Apollo#ledger-%E7%9B%AE%E5%BD%95">Apollo 笔记（Apollo 学习笔记）— Apollo 初学者学习笔记。</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44809266">PointNet系列论文解读</a></li>
<li><a target="_blank" rel="noopener" href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/deep3dbox.html">Deep3dBox：使用深度学习和几何进行 3D 边界框估计</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356892010">SECOND算法解析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361973979">PointRCNN深度解析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363926237">Fast PointRCNN论文解读</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357626425">PointPillars论文和代码解析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352419316">VoxelNet论文和代码解析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/444447881">CenterPoint分析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148942116">PV-RCNN：3D目标检测Waymo模态挑战赛+KITTI榜单模态第一模挑战赛</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/359800738">LiDAR R-CNN：一种快速、通用的二类3D检测器</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/122426949">混合体素网络（HVNet）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420708905">自动驾驶汽车| 范围图像纸分享</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476056546">SST：单步放大装置Transformer 3D探测仪</a></li>
</ul>
<h1 id="著名研究组-x2F-学者"><a href="#著名研究组-x2F-学者" class="headerlink" title="著名研究组&#x2F;学者"></a>著名研究组&#x2F;学者</h1><ul>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=yAWtq6QAAAAJ&hl=en">王乃燕@Tusimple</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en">李洪生@CUHK</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=Fe7NTe0AAAAJ&hl=en">一次 Tuzel@Apple</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=XP_Hxm4AAAAJ&hl=en">奥斯卡Beijbom@nuTonomy</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=jyxO2akAAAAJ&hl=en">Raquel Urtasun@多伦多大学</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=dzOd2hgAAAAJ&view_op=list_works&sortby=pubdate">Philipp Krähenbühl@UT Austin</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=9B8PoXUAAAAJ&view_op=list_works&sortby=pubdate">德瓦拉马南@CMU</a></li>
<li><a target="_blank" rel="noopener" href="https://jiaya.me/">贾家亚@CUHK</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser@princeton</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=5JlEyTAAAAAJ&view_op=list_works&sortby=pubdate">列奥尼达斯·吉巴斯@斯坦福</a></li>
<li><a target="_blank" rel="noopener" href="https://www.trailab.utias.utoronto.ca/">史蒂文·瓦斯兰德@多伦多大学</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=nFefEI8AAAAJ&view_op=list_works&sortby=pubdate">Ouais Alsharif@Google 大脑</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=i7U4YogAAAAJ&view_op=list_works&sortby=pubdate">柴育宁（前）@waymo</a></li>
<li><a target="_blank" rel="noopener" href="http://yulanguo.me/">郭玉兰@NUDT</a></li>
<li><a target="_blank" rel="noopener" href="https://www4.comp.polyu.edu.hk/~cslzhang/">张磊@香港理工大学</a></li>
<li><a target="_blank" rel="noopener" href="https://lihongyang.info/">李洪洋@sensetime</a></li>
</ul>
<h1 id="著名的代码库"><a href="#著名的代码库" class="headerlink" title="著名的代码库"></a>著名的代码库</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/PointCloudLibrary/pcl">点云库 (PCL)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/traveller59/spconv">Spconv</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/poodarchu/Det3D">Det3D</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdetection3d">毫米检测3d</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/OpenPCDet">开放PCDet</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tianweiy/CenterPoint">中心点</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ApolloAuto">Apollo Auto——百度开放自动驾驶平台</a></li>
<li><a target="_blank" rel="noopener" href="https://www.autoware.org/">AutoWare——东京大学自动驾驶平台</a></li>
<li><a target="_blank" rel="noopener" href="https://comma.ai/">Openpilot — 一种开源软件，旨在改进当今道路上大多数新车的现有驾驶员辅助</a></li>
</ul>
<h1 id="深度学习点云参考论文来源"><a href="#深度学习点云参考论文来源" class="headerlink" title="深度学习点云参考论文来源"></a>深度学习点云参考论文来源</h1><h1>

<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">- Recent papers (from 2017)</span></span><br></pre></td></tr></table></figure>

</h1>

<h3> Keywords </h3>

<p><strong><code>dat.</code></strong>: dataset &amp;emsp; | &amp;emsp; <strong><code>cls.</code></strong>: classification &amp;emsp; | &amp;emsp; <strong><code>rel.</code></strong>: retrieval &amp;emsp; | &amp;emsp; <strong><code>seg.</code></strong>: segmentation<br><strong><code>det.</code></strong>: detection &amp;emsp; | &amp;emsp; <strong><code>tra.</code></strong>: tracking &amp;emsp; | &amp;emsp; <strong><code>pos.</code></strong>: pose &amp;emsp; | &amp;emsp; <strong><code>dep.</code></strong>: depth<br><strong><code>reg.</code></strong>: registration &amp;emsp; | &amp;emsp; <strong><code>rec.</code></strong>: reconstruction &amp;emsp; | &amp;emsp; <strong><code>aut.</code></strong>: autonomous driving<br><strong><code>oth.</code></strong>: other, including normal-related, correspondence, mapping, matching, alignment, compression, generative model…</p>
<p>Statistics: 🔥 code is available &amp; stars &gt;&#x3D; 100 &amp;emsp;|&amp;emsp; ⭐️ citation &gt;&#x3D; 50</p>
<hr>
<h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf">CVPR</a>] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/pointnet">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/fxia22/pointnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.pdf">CVPR</a>] Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs. [<strong><code>cls.</code></strong>] ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yi_SyncSpecCNN_Synchronized_Spectral_CVPR_2017_paper.pdf">CVPR</a>] SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/ericyi/SyncSpecCNN">torch</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf">CVPR</a>] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. [<a target="_blank" rel="noopener" href="http://www.scan-net.org/">project</a>][<a target="_blank" rel="noopener" href="http://www.scan-net.org/">git</a>] [<strong><code>dat.</code></strong> <strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Mostegel_Scalable_Surface_Reconstruction_CVPR_2017_paper.pdf">CVPR</a>] Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Straub_Efficient_Global_Point_CVPR_2017_paper.pdf">CVPR</a>] Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures. [<a target="_blank" rel="noopener" href="http://people.csail.mit.edu/jstraub/">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Vongkulbhisal_Discriminative_Optimization_Theory_CVPR_2017_paper.pdf">CVPR</a>] Discriminative Optimization: Theory and Applications to Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Elbaz_3D_Point_Cloud_CVPR_2017_paper.pdf">CVPR</a>] 3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder. [<a target="_blank" rel="noopener" href="https://github.com/gilbaz/LORAX">git</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf">CVPR</a>] Multi-View 3D Object Detection Network for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/bostondiditeam/MV3D">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf">CVPR</a>] 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions. [<a target="_blank" rel="noopener" href="https://github.com/andyzeng/3dmatch-toolbox">code</a>] [<strong><code>dat.</code></strong> <strong><code>pos.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.pdf">CVPR</a>] OctNet: Learning Deep 3D Representations at High Resolutions. [<a target="_blank" rel="noopener" href="https://github.com/griegler/octnet">torch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥 ⭐️</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Klokov_Escape_From_Cells_ICCV_2017_paper.pdf">ICCV</a>] Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models. [<a target="_blank" rel="noopener" href="https://github.com/fxia22/kdnet.pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>] ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper.pdf">ICCV</a>] 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/CKchaos/scn2pointcloud_tool">code</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Park_Colored_Point_Cloud_ICCV_2017_paper.pdf">ICCV</a>] Colored Point Cloud Registration Revisited. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Nan_PolyFit_Polygonal_Surface_ICCV_2017_paper.pdf">ICCV</a>] PolyFit: Polygonal Surface Reconstruction from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/LiangliangNan/PolyFit">code</a>] [<strong><code>rec.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ladicky_From_Point_Clouds_ICCV_2017_paper.pdf">ICCV</a>] From Point Clouds to Mesh using Regression. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Qi_3D_Graph_Neural_ICCV_2017_paper.pdf">ICCV</a>] 3D Graph Neural Networks for RGBD Semantic Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/yanx27/3DGNN_pytorch">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space">NeurIPS</a>] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/pointnet2">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/erikwijmans/Pointnet2_PyTorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6931-deep-sets">NeurIPS</a>] Deep Sets. [<a target="_blank" rel="noopener" href="https://github.com/manzilzaheer/DeepSets">pytorch</a>] [<strong><code>cls.</code></strong>] ⭐️</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989161">ICRA</a>] Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks. [<a target="_blank" rel="noopener" href="https://github.com/lijiannuist/Vote3Deep_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989591">ICRA</a>] Fast segmentation of 3D point clouds: A paradigm on LiDAR data for autonomous vehicle applications. [<a target="_blank" rel="noopener" href="https://github.com/VincentCheungM/Run_based_segmentation">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989618">ICRA</a>] SegMatch: Segment based place recognition in 3D point clouds. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7989664">ICRA</a>] Using 2 point+normal sets for fast registration of point clouds with small overlap. [<strong><code>reg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8202234">IROS</a>] Car detection for autonomous vehicle: LIDAR and vision fusion approach through deep learning framework. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8202239">IROS</a>] 3D object classification with point convolution network. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8205955">IROS</a>] 3D fully convolutional network for vehicle detection in point cloud. [<a target="_blank" rel="noopener" href="https://github.com/yukitsuji/3D_CNN_tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8206488">IROS</a>] Deep learning of directional truncated signed distance function for robust 3D object recognition. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8206584">IROS</a>] Analyzing the quality of matched 3D point clouds of objects. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://segcloud.stanford.edu/segcloud_2017.pdf">3DV</a>] SEGCloud: Semantic Segmentation of 3D Point Clouds. [<a target="_blank" rel="noopener" href="http://segcloud.stanford.edu/">project</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>] ⭐️</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/ielx7/34/8454009/08046026.pdf?tp=&arnumber=8046026&isnumber=8454009&ref=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8=">TPAMI</a>] Structure-aware Data Consolidation. [<strong><code>oth.</code></strong>]</li>
</ul>
<hr>
<h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf">CVPR</a>] SPLATNet: Sparse Lattice Networks for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/NVlabs/splatnet">caffe</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.pdf">CVPR</a>] Attentional ShapeContextNet for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Mining_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling. [<a target="_blank" rel="noopener" href="http://www.merl.com/research/license#KCNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. [<a target="_blank" rel="noopener" href="http://www.merl.com/research/license#FoldingNet">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.pdf">CVPR</a>] Pointwise Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/scenenn/pointwise">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] PU-Net: Point Cloud Upsampling Network. [<a target="_blank" rel="noopener" href="https://github.com/yulequan/PU-Net">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf">CVPR</a>] SO-Net: Self-Organizing Network for Point Cloud Analysis. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/SO-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.pdf">CVPR</a>] Recurrent Slice Networks for 3D Segmentation of Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/qianguih/RSNet">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf">CVPR</a>] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/SparseConvNet">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf">CVPR</a>] Deep Parametric Continuous Convolutional Neural Networks. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf">CVPR</a>] PIXOR: Real-time 3D Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/ankita-kalra/PIXOR">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf">CVPR</a>] SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/laughtervv/SGPN">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf">CVPR</a>] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. [<a target="_blank" rel="noopener" href="https://github.com/loicland/superpoint_graph">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf">CVPR</a>] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/tsinghua-rll/VoxelNet-tensorflow">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yun_Reflection_Removal_for_CVPR_2018_paper.pdf">CVPR</a>] Reflection Removal for Large-Scale 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">CVPR</a>] Hand PointNet: 3D Hand Pose Estimation using Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/3huo/Hand-Pointnet">pytorch</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.pdf">CVPR</a>] PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition. [<a target="_blank" rel="noopener" href="https://github.com/mikacuy/pointnetvlad.git">tensorflow</a>] [<strong><code>rel.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf">CVPR</a>] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf">CVPR</a>] Density Adaptive Point Set Registration. [<a target="_blank" rel="noopener" href="https://github.com/felja633/DARE">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Birdal_A_Minimalist_Approach_CVPR_2018_paper.pdf">CVPR</a>] A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf">CVPR</a>] Inverse Composition Discriminative Optimization for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf">CVPR</a>] CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles. [<strong><code>tra.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf">CVPR</a>] PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf">CVPR</a>] PointGrid: A Deep Network for 3D Shape Understanding. [<a target="_blank" rel="noopener" href="https://github.com/trucleduc/PointGrid">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf">CVPR</a>] PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation. [<a target="_blank" rel="noopener" href="https://github.com/malavikabindhi/CS230-PointFusion">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf">CVPR</a>] Frustum PointNets for 3D Object Detection from RGB-D Data. [<a target="_blank" rel="noopener" href="https://github.com/charlesq34/frustum-pointnets">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf">CVPR</a>] Tangent Convolutions for Dense Prediction in 3D. [<a target="_blank" rel="noopener" href="https://github.com/tatarchm/tangent_conv">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Matheus_Gadelha_Multiresolution_Tree_Networks_ECCV_2018_paper.pdf">ECCV</a>] Multiresolution Tree Networks for 3D Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/matheusgadelha/MRTNet">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lequan_Yu_EC-Net_an_Edge-aware_ECCV_2018_paper.pdf">ECCV</a>] EC-Net: an Edge-aware Point set Consolidation Network. [<a target="_blank" rel="noopener" href="https://github.com/yulequan/EC-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaoqing_Ye_3D_Recurrent_Neural_ECCV_2018_paper.pdf">ECCV</a>] 3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhou_Learning_and_Matching_ECCV_2018_paper.pdf">ECCV</a>] Learning and Matching Multi-View Descriptors for Registration of Point Clouds. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf">ECCV</a>] 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration. [<a target="_blank" rel="noopener" href="https://github.com/yewzijian/3DFeatNet">tensorflow</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.pdf">ECCV</a>] Local Spectral Graph Convolution for Point Set Feature Learning. [<a target="_blank" rel="noopener" href="https://github.com/fate3439/LocalSpecGCN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yifan_Xu_SpiderCNN_Deep_Learning_ECCV_2018_paper.pdf">ECCV</a>] SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. [<a target="_blank" rel="noopener" href="https://github.com/xyf513/SpiderCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yinlong_Liu_Efficient_Global_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Global Point Cloud Registration by Matching Rotation Invariant Features Through Translation Search. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Kejie_Li_Efficient_Dense_Point_ECCV_2018_paper.pdf">ECCV</a>] Efficient Dense Point Cloud Object Reconstruction using Deformation Vector Fields. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Dario_Rethage_Fully-Convolutional_Point_Networks_ECCV_2018_paper.pdf">ECCV</a>] Fully-Convolutional Point Networks for Large-Scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/drethage/fully-convolutional-point-network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf">ECCV</a>] Deep Continuous Fusion for Multi-Sensor 3D Object Detection. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Benjamin_Eckart_Fast_and_Accurate_ECCV_2018_paper.pdf">ECCV</a>] HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf">ECCV</a>] Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tolga_Birdal_PPF-FoldNet_Unsupervised_Learning_ECCV_2018_paper.pdf">ECCV</a>] PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Zeng_3DContextNet_K-d_Tree_Guided_Hierarchical_Learning_of_Point_Clouds_Using_ECCVW_2018_paper.pdf">ECCVW</a>] 3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.pdf">ECCVW</a>] YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16530/16302">AAAI</a>] Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction. [<a target="_blank" rel="noopener" href="https://github.com/chenhsuanlin/3D-point-cloud-generation">tensorflow</a>] [<strong><code>rec.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/aaai/junzhou_-AAAI-Adaptive_Graph_Convolutional_Neural_NetworksI.pdf">AAAI</a>] Adaptive Graph Convolutional Neural Networks. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7545-unsupervised-learning-of-shape-and-pose-with-differentiable-point-clouds">NeurIPS</a>] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/eldar/differentiable-point-clouds">tensorflow</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points">NeurIPS</a>] PointCNN: Convolution On X-Transformed Points. [<a target="_blank" rel="noopener" href="https://github.com/yangyanli/PointCNN">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/hxdengBerkeley/PointCNN.Pytorch">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.02392">ICML</a>] Learning Representations and Generative Models for 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/optas/latent_3d_points">code</a>] [<strong><code>oth.</code></strong>] 🔥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/ft_gateway.cfm?id=3201301&ftid=1991771&dwn=1&CFID=155708095&CFTOKEN=598df826a5b545a7-3E7CE91C-DE12-F588-FAEEF2551115E64E">TOG</a>] Point Convolutional Neural Networks by Extension Operators. [<a target="_blank" rel="noopener" href="https://github.com/matanatz/pcnn">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09263">SIGGRAPH</a>] P2P-NET: Bidirectional Point Displacement Net for Shape Transform. [<a target="_blank" rel="noopener" href="https://github.com/kangxue/P2P-NET">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.01759">SIGGRAPH Asia</a>] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/viscom-ulm/MCCNN">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.04496">SIGGRAPH</a>] Learning local shape descriptors from part correspondences with multi-view convolutional networks. [<a target="_blank" rel="noopener" href="https://people.cs.umass.edu/~hbhuang/local_mvcnn/index.html">project</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.07659">MM</a>] PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02952">MM</a>] RGCNN: Regularized Graph CNN for Point Cloud Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/tegusi/RGCNN">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10783">MM</a>] Hybrid Point Cloud Attribute Compression Using Slice-based Layered Structure and Block-based Intra Prediction. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462884">ICRA</a>] End-to-end Learning of Multi-sensor 3D Tracking by Detection. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460837">ICRA</a>] Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462926">ICRA</a>] SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/priyankanagaraj1494/Squeezseg">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461257">ICRA</a>] Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461048">ICRA</a>] High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion. [<strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461095">ICRA</a>] Sampled-Point Network for Classification of Deformed Building Element Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460532">ICRA</a>] Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460605">ICRA</a>] Signature of Topologically Persistent Points for 3D Point Cloud Description. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461232">ICRA</a>] A General Pipeline for 3D Detection of Vehicles. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460716">ICRA</a>] Robust and Fast 3D Scan Alignment Using Mutual Information. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460940">ICRA</a>] Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460862">ICRA</a>] Surface-Based Exploration for Autonomous 3D Modeling. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460554">ICRA</a>] Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460887">ICRA</a>] Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460639">ICRA</a>] Real-Time Object Tracking in Sparse Point Clouds Based on 3D Interpolation. [<strong><code>tra.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460825">ICRA</a>] Robust Generalized Point Cloud Registration Using Hybrid Mixture Model. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461049">ICRA</a>] A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461000">ICRA</a>] Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8461102">ICRA</a>] Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460910">ICRA</a>] Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460204">ICRA</a>] Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8460834">ICRA</a>] Complex Urban LiDAR Data Set. [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IguZjmLf5V0&feature=youtu.be">video</a>] [<strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593693">IROS</a>] CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.[<a target="_blank" rel="noopener" href="https://github.com/epiception/CalibNet">tensorflow</a>] [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593839">IROS</a>] Dynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593733">IROS</a>] A 3D Laparoscopic Imaging System Based on Stereo-Photogrammetry with Random Patterns. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593558">IROS</a>] Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594024">IROS</a>] Octree map based on sparse point cloud and heuristic probability distribution for labeled images. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593854">IROS</a>] PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593953">IROS</a>] Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594299">IROS</a>] LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.[<a target="_blank" rel="noopener" href="https://github.com/RobustFieldAutonomyLab/LeGO-LOAM">code</a>] [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593741">IROS</a>] Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594362">IROS</a>] Stereo Camera Localization in 3D LiDAR Maps. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594049">IROS</a>] Joint 3D Proposal Generation and Object Detection from View Aggregation. [<strong><code>det.</code></strong>] ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594318">IROS</a>] Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593910">IROS</a>] Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594175">IROS</a>] NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor. [<a target="_blank" rel="noopener" href="https://github.com/CTTCGeoLab/VI_ROS">code</a>][<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8593837">IROS</a>] A 3D Convolutional Neural Network Towards Real-Time Amodal 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594356">IROS</a>] Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models. [<strong><code>seg.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594514">IROS</a>] PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08241">IROS</a>] [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/3DmFV-Net">Tensorflow</a>]3DmFV: Point Cloud Classification and segmentation for unstructured 3D point clouds. [<strong><code>cls.</code></strong> ]</li>
<li>[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8594042">IROS</a>] Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments. [<strong><code>oth.</code></strong> ]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://www.mdpi.com/1424-8220/18/10/3337">SENSORS</a>] SECOND: Sparsely Embedded Convolutional Detection. [<a target="_blank" rel="noopener" href="https://github.com/traveller59/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07289">ACCV</a>] Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds). [<a target="_blank" rel="noopener" href="https://github.com/cgtuebingen/Flex-Convolution">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.00671">3DV</a>] PCN: Point Completion Network. [<a target="_blank" rel="noopener" href="https://github.com/TonythePlaneswalker/pcn">tensorflow</a>] [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>] 🔥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01711">ICASSP</a>] A Graph-CNN for 3D Point Cloud Classification. [<a target="_blank" rel="noopener" href="https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification">tensorflow</a>] [<strong><code>cls.</code></strong>] 🔥</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.01195.pdf">ITSC</a>] BirdNet: a 3D Object Detection Framework from LiDAR information. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.00652">arXiv</a>] PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/MVIG-SJTU/pointSIFT">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.07872">arXiv</a>] Spherical Convolutional Neural Network for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07605">arXiv</a>] Adversarial Autoencoders for Generating 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11209">arXiv</a>] Iterative Transformer Network for 3D Point Cloud. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.12543">arXiv</a>] Topology-Aware Surface Reconstruction for Point Clouds. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01402">arXiv</a>] Inferring Point Clouds from Single Monocular Images by Depth Intermediation. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04302">arXiv</a>] Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05276">arXiv</a>] IPOD: Intensive Point-based Object Detector for Point Cloud. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11383">arXiv</a>] Feature Preserving and Uniformity-controllable Point Cloud Simplification on Graph. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.01060">arXiv</a>] POINTCLEANNET: Learning to Denoise and Remove Outliers from Dense Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/mrakotosaon/pointcleannet">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.06199">arXiv</a>] Complex-YOLO: Real-time 3D Object Detection on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03818">arxiv</a>] RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement. [<a target="_blank" rel="noopener" href="https://github.com/Kiwoo/RoarNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11029">arXiv</a>] Multi-column Point-CNN for Sketch Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05591">arXiv</a>] PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention. [<a target="_blank" rel="noopener" href="https://liuziwei7.github.io/projects/PointGrow">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.05795.pdf">arXiv</a>] Point Cloud GAN. [<a target="_blank" rel="noopener" href="https://github.com/chunliangli/Point-Cloud-GAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
</ul>
<hr>
<h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><ul>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.07601">CVPR</a>] Relation-Shape Convolutional Neural Network for Point Cloud Analysis. [<a target="_blank" rel="noopener" href="https://github.com/Yochengliu/Relation-Shape-CNN">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://raoyongming.github.io/files/SFCNN.pdf">CVPR</a>] Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11397">CVPR</a>] DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds. [<a target="_blank" rel="noopener" href="https://ai4ce.github.io/DeepMapping/">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.07179">CVPR</a>] Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/mileyan/pseudo_lidar">code</a>] [<strong><code>det.</code></strong> <strong><code>dep.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04244">CVPR</a>] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/sshaoshuai/PointRCNN">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.07016">CVPR</a>] Generating 3D Adversarial Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/xiangchong1/3d-adv-pc">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03375v1">CVPR</a>] Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.08017">CVPR</a>] A-CNN: Annularly Convolutional Neural Networks on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/artemkomarichev/a-cnn">tensorflow</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07246">CVPR</a>] PointConv: Deep Convolutional Networks on 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/DylanWusee/pointconv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11647">CVPR</a>] Path-Invariant Map Networks. [<a target="_blank" rel="noopener" href="https://github.com/zaiweizhang/path_invariance_map_network">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02713">CVPR</a>] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding. [<a target="_blank" rel="noopener" href="https://github.com/daerduoCarey/partnet_dataset">code</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.00680">CVPR</a>] GeoNet: Deep Geodesic Networks for Point Cloud Analysis. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.09852">CVPR</a>] Associatively Segmenting Instances and Semantics in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/WXinlong/ASIS">tensorflow</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.08988">CVPR</a>] Supervised Fitting of Geometric Primitives to 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/csimstu2/SPFN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00343">CVPR</a>] Octree guided CNN with Spherical Kernels for 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.09287.pdf">extension</a>] [<a target="_blank" rel="noopener" href="https://github.com/hlei-ziyan/SPH3D-GCN">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Point Cloud Registration using PointNet. [<a target="_blank" rel="noopener" href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00699v1">CVPR</a>] JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields. [<a target="_blank" rel="noopener" href="https://github.com/pqhieu/JSIS3D">pytorch</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02113">CVPR</a>] Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05784">CVPR</a>] PointPillars: Fast Encoders for Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/nutonomy/second.pytorch">pytorch</a>] [<strong><code>det.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11286">CVPR</a>] Patch-based Progressive 3D Point Set Upsampling. [<a target="_blank" rel="noopener" href="https://github.com/yifita/3PU">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09793">CVPR</a>] PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval. [<a target="_blank" rel="noopener" href="https://github.com/XLechter/PCAN">code</a>] [<strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00709">CVPR</a>] PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/FoggYu/PartNet">pytorch</a>] [<strong><code>dat.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02170">CVPR</a>] PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/aseembehl/pointflownet">code</a>] [<strong><code>det.</code></strong> <strong><code>dat.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03483">CVPR</a>] SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences. [<a target="_blank" rel="noopener" href="https://github.com/intellhave/SDRSAC">matlab</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.04019">CVPR</a>] Deep Reinforcement Learning of Volume-guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03461">CVPR</a>] Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10775v1">CVPR</a>] 3D Point-Capsule Networks. [<a target="_blank" rel="noopener" href="https://github.com/yongheng1991/3D-point-capsule-networks">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1904.08755">CVPR</a>] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/StanfordVL/MinkowskiEngine">pytorch</a>] [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.06879v2">CVPR</a>] The Perfect Match: 3D Point Cloud Matching with Smoothed Densities. [<a target="_blank" rel="noopener" href="https://github.com/zgojcic/3DSmoothNet">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.10136">CVPR</a>] FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization. [<a target="_blank" rel="noopener" href="https://bitbucket.org/gaowei19951004/poser/src/master/">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.01411">CVPR</a>] FlowNet3D: Learning Scene Flow in 3D Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07782">CVPR</a>] Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.linliang.net/wp-content/uploads/2019/04/CVPR2019_PointClound.pdf">CVPR</a>] ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://jiaya.me/papers/pointweb_cvpr19.pdf">CVPR</a>] PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/hszhao/PointWeb">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.12304">CVPR</a>] RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion. [<a target="_blank" rel="noopener" href="https://github.com/iSarmad/RL-GAN-Net">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05711">CVPR</a>] PointNetLK: Robust &amp; Efficient Point Cloud Registration using PointNet. [<a target="_blank" rel="noopener" href="https://github.com/hmgoforth/PointNetLK">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/332240602_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes">CVPR</a>] Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes. [<a target="_blank" rel="noopener" href="https://github.com/ziquan111/RobustPCLReconstruction">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.00709">CVPR</a>] Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/Nesti-Net">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03320">CVPR</a>] GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf">CVPR</a>] Graph Attention Convolution for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02050">CVPR</a>] Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.08701v1">CVPR</a>] LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03498.pdf">CVPR</a>] LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks. [<a target="_blank" rel="noopener" href="https://sites.google.com/view/lp-3dcnn/home">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.pdf">CVPR</a>] Structural Relational Reasoning of Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.03322">CVPR</a>] 3DN: 3D Deformation Network. [<a target="_blank" rel="noopener" href="https://github.com/laughtervv/3DN">tensorflow</a>] [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.pdf">CVPR</a>] Privacy Preserving Image-Based Localization. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR</a>] Argoverse: 3D Tracking and Forecasting With Rich Maps.[<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Giancola_Leveraging_Shape_Completion_for_3D_Siamese_Tracking_CVPR_2019_paper.pdf">CVPR</a>] Leveraging Shape Completion for 3D Siamese Tracking. [<a target="_blank" rel="noopener" href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking">pytorch</a>] [<strong><code>tra.</code></strong> ]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf">CVPRW</a>] Attentional PointNet for 3D-Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/anshulpaigwar/Attentional-PointNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_3D_Local_Features_for_Direct_Pairwise_Registration_CVPR_2019_paper.pdf">CVPR</a>] 3D Local Features for Direct Pairwise Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dovrat_Learning_to_Sample_CVPR_2019_paper.pdf">CVPR</a>] Learning to Sample. [<a target="_blank" rel="noopener" href="https://github.com/orendv/learning_to_sample">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pittaluga_Revealing_Scenes_by_Inverting_Structure_From_Motion_Reconstructions_CVPR_2019_paper.pdf">CVPR</a>] Revealing Scenes by Inverting Structure from Motion Reconstructions. [<a target="_blank" rel="noopener" href="https://github.com/francescopittaluga/invsfm">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.pdf">CVPR</a>] DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image. [<a target="_blank" rel="noopener" href="https://github.com/JiaxiongQ/DeepLiDAR">pytorch</a>] [<strong><code>dep.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_HPLFlowNet_Hierarchical_Permutohedral_Lattice_FlowNet_for_Scene_Flow_Estimation_on_CVPR_2019_paper.pdf">CVPR</a>] HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/laoreja/HPLFlowNet">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09664v1">ICCV</a>] Deep Hough Voting for 3D Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/votenet">pytorch</a>] [<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/VoteNet">tensorflow</a>] [<strong><code>det.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03751">ICCV</a>] DeepGCNs: Can GCNs Go as Deep as CNNs? [<a target="_blank" rel="noopener" href="https://github.com/lightaime/deep_gcns">tensorflow</a>] <a target="_blank" rel="noopener" href="https://github.com/lightaime/deep_gcns_torch">[pytorch]</a> [<strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10844.pdf">ICCV</a>] PU-GAN: a Point Cloud Upsampling Adversarial Network. [<a target="_blank" rel="noopener" href="https://github.com/liruihui/PU-GAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition. [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.12320.pdf">ICCV</a>] PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows. [<a target="_blank" rel="noopener" href="https://github.com/stevenygd/PointFlow">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.12704.pdf">ICCV</a>] Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/11GJzouV6jt_aOpvrJ8l3J5x_R_-m-Lg8/view">ICCV</a>] SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [<a target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/SO-HandNet">code</a>] [<strong><code>pos.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11017">ICCV</a>] DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.04616">ICCV</a>] Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data. [<strong><code>cls.</code></strong> <strong><code>dat.</code></strong>] [<a target="_blank" rel="noopener" href="https://github.com/hkust-vgd/scanobjectnn">code</a>] [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/scanobjectnn/">dataset</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.08889">ICCV</a>] KPConv: Flexible and Deformable Convolution for Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/HuguesTHOMAS/KPConv">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.06295.pdf">ICCV</a>] ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics. [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/shellnet/">project</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.04422.pdf">ICCV</a>] Point-Based Multi-View Stereo Network. [<a target="_blank" rel="noopener" href="https://github.com/callmeray/PointMVSNet">pytorch</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03669">ICCV</a>] DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/Yochengliu/DensePoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.04153v2">ICCV</a>] DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.06292.pdf">ICCV</a>] 3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions. [<a target="_blank" rel="noopener" href="https://github.com/seowok/TreeGAN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.10469.pdf">ICCV</a>] Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.html">ICCV</a>] Fully Convolutional Geometric Features. [<a target="_blank" rel="noopener" href="https://github.com/chrischoy/FCGF">pytorch</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.07050.pdf">ICCV</a>] LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis. [<strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.pdf">ICCV</a>] Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning. [<a target="_blank" rel="noopener" href="https://github.com/phermosilla/TotalDenoising">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00229">ICCV</a>] USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/USIP">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.pdf">ICCV</a>] Interpolated Convolutional Networks for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf">ICCV</a>] PointCloud Saliency Maps. [<a target="_blank" rel="noopener" href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10471.pdf">ICCV</a>] STD: Sparse-to-Dense 3D Object Detector for Point Cloud. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.pdf">ICCV</a>] Accelerated Gravitational Point Set Alignment with Altered Physical Laws. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf">ICCV</a>] Deep Closest Point: Learning Representations for Point Cloud Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.pdf">ICCV</a>] Efficient Learning on Point Clouds with Basis Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/sergeyprokudin/bps">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.pdf">ICCV</a>] PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.pdf">ICCV</a>] Dynamic Points Agglomeration for Hierarchical Point Sets Learning. [<a target="_blank" rel="noopener" href="https://github.com/yuyi1005/DPAM">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.pdf">ICCV</a>] Unsupervised Multi-Task Feature Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.pdf">ICCV</a>] VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/xianyuMeng/VV-Net-Voxel-VAE-Net-with-Group-Convolutions-for-Point-Cloud-Segmentation">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.pdf">ICCV</a>] GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion. [<a target="_blank" rel="noopener" href="https://github.com/justanhduc/graphx-conv">pytorch</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MeteorNet_Deep_Learning_on_Dynamic_3D_Point_Cloud_Sequences_ICCV_2019_paper.pdf">ICCV</a>] MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences. [<a target="_blank" rel="noopener" href="https://github.com/xingyul/meteornet">code</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.02990">ICCV</a>] Fast Point R-CNN. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.pdf">ICCV</a>] Robust Variational Bayesian Point Set Registration. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf">ICCV</a>] DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Learning an Effective Equivariant 3D Descriptor Without Supervision. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.pdf">ICCV</a>] 3D Instance Segmentation via Multi-Task Metric Learning. [<a target="_blank" rel="noopener" href="https://sites.google.com/view/3d-instance-mtml">code</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.pdf">ICCV</a>] 3D Face Modeling From Diverse Raw Scan Data. [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.12249">ICCVW</a>] Range Adaptation for 3D Object Detection in LiDAR. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.08396.pdf">NeurIPS</a>] Self-Supervised Deep Learning on Point Clouds by Reconstructing Space. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01140">NeurIPS</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8706-exploiting-local-and-global-structure-for-point-cloud-semantic-segmentation-with-contextual-point-representations.pdf">NeurIPS</a>] Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations. [<a target="_blank" rel="noopener" href="https://github.com/fly519/ELGS">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.03739.pdf">NeurIPS</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8940-pointdan-a-multi-scale-3d-domain-adaption-network-for-point-cloud-representation.pdf">NeurIPS</a>] PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation. [<a target="_blank" rel="noopener" href="https://github.com/canqin001/PointDAN">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=SJeXSo09FQ">ICLR</a>] Learning Localized Generative Models for 3D Point Clouds via Graph Convolution. [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07290">ICMLW</a>] LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11731">AAAI</a>] CAPNet: Continuous Approximation Projection For 3D Point Cloud Reconstruction Using 2D Supervision. [<a target="_blank" rel="noopener" href="https://github.com/val-iisc/capnet">code</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.02565">AAAI</a>] Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network. [<a target="_blank" rel="noopener" href="https://github.com/liuxinhai/Point2Sequence">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://par.nsf.gov/biblio/10086163">AAAI</a>] Point Cloud Processing via Recurrent Set Encoding. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.00333">AAAI</a>] PVRNet: Point-View Relation Neural Network for 3D Shape Recognition. [<a target="_blank" rel="noopener" href="https://github.com/Hxyou/PVRNet">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://gaoyue.org/paper/HGNN.pdf">AAAI</a>] Hypergraph Neural Networks. [<a target="_blank" rel="noopener" href="https://github.com/iMoonLab/HGNN">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.07829">TOG</a>] Dynamic Graph CNN for Learning on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/WangYueFt/dgcnn">tensorflow</a>][<a target="_blank" rel="noopener" href="https://github.com/WangYueFt/dgcnn">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>] 🔥 ⭐️</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.10170.pdf">TOG</a>] LOGAN: Unpaired Shape Transform in Latent Overcomplete Space. [<a target="_blank" rel="noopener" href="https://github.com/kangxue/LOGAN">tensorflow</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3355089.3356573">SIGGRAPH Asia</a>] RPM-Net: recurrent prediction of motion and parts from point cloud. [<a target="_blank" rel="noopener" href="https://github.com/Salingo/RPM-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.00575v1">SIGGRAPH Asia</a>] StructureNet: Hierarchical Graph Networks for 3D Shape Generation. [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3343031.3351009">MM</a>] MMJN: Multi-Modal Joint Networks for 3D Shape Recognition. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351061">MM</a>] 3D Point Cloud Geometry Compression on Deep Learning. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351042">MM</a>] SRINet: Learning Strictly Rotation-Invariant Representations for Point Cloud Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/tasx0823/SRINet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3350960">MM</a>] L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3351076">MM</a>] Ground-Aware Point Cloud Semantic Segmentation for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/Jaiy/Ground-aware-Seg">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.08996">ICME</a>] Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By Lookup Tables. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04427">ICASSP</a>] 3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation. [<a target="_blank" rel="noopener" href="https://github.com/chaojingduan/Neural-Projection">code</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.06371">BMVC</a>] Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00319">ICRA</a>] Discrete Rotation Equivariance for Point Cloud Recognition. [<a target="_blank" rel="noopener" href="https://github.com/lijx10/rot-equ-net">pytorch</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.08495">ICRA</a>] SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/xuanyuzhou98/SqueezeSegV2">tensorflow</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.ais.uni-bonn.de/papers/ICRA_2019_Razlaw.pdf">ICRA</a>] Detection and Tracking of Small Objects in Sparse 3D Laser Range Data. [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.02553">ICRA</a>] Oriented Point Sampling for Plane Detection in Unorganized Point Clouds. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_1.html">ICRA</a>] Point Cloud Compression for 3D LiDAR Sensor Using Recurrent Neural Network with Residual Blocks. [<a target="_blank" rel="noopener" href="https://github.com/ChenxiTU/Point-cloud-compression-by-RNN">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06065">ICRA</a>] Focal Loss in 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/pyun-ram/FL3D">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06267">ICRA</a>] PointNetGPD: Detecting Grasp Configurations from Point Sets. [<a target="_blank" rel="noopener" href="https://github.com/lianghongzhuo/PointNetGPD">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09742">ICRA</a>] 2D3D-MatchNet: Learning to Match Keypoints across 2D Image and 3D Point Cloud. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Speeding up Iterative Closest Point Using Stochastic Gradient Descent. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] Uncertainty Estimation for Projecting Lidar Points Onto Camera Images for Moving Platforms. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06405v1">ICRA</a>] BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/VCCIV/BLVD">project</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_2.html">ICRA</a>] A Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arpg.colorado.edu/papers/hmrf_icp.pdf">ICRA</a>] Robust low-overlap 3-D point cloud registration for outlier rejection. [<a target="_blank" rel="noopener" href="https://github.com/JStech/ICP">matlab</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ras.papercept.net/conferences/conferences/ICRA19/program/ICRA19_ContentListWeb_3.html">ICRA</a>] Robust Generalized Point Set Registration Using Inhomogeneous Hybrid Mixture Models Via Expectation. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.07511">ICRA</a>] Dense 3D Visual Mapping via Semantic Simplification. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01649">ICRA</a>] MVX-Net: Multimodal VoxelNet for 3D Object Detection. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1810.01470">ICRA</a>] CELLO-3D: Estimating the Covariance of ICP in the Real World. [<strong><code>reg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/334720713_EPN_Edge-Aware_PointNet_for_Object_Recognition_from_Multi-View_25D_Point_Clouds">IROS</a>] EPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Merium88/Edge-Aware-PointNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.13030.pdf">IROS</a>] SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles. [<strong><code>oth.</code></strong>] [<strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01643v1.pdf">IROS</a>] PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.10964">IV</a>] End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving. [<strong><code>seg.</code></strong>] [<strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02375">Eurographics Workshop</a>] Generalizing Discrete Convolutions for Unstructured Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/aboulch/ConvPoint">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.02191">WACV</a>] 3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.06297.pdf">3DV</a>] Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. [<a target="_blank" rel="noopener" href="https://hkust-vgd.github.io/riconv/">project</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.11555">3DV</a>] Effective Rotation-invariant Point CNN with Spherical Harmonics kernels. [<a target="_blank" rel="noopener" href="https://github.com/adrienPoulenard/SPHnet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.13538.pdf">TVCG</a>] LassoNet: Deep Lasso-Selection of 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://lassonet.github.io/">project</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02532">arXiv</a>] Fast 3D Line Segment Detection From Unorganized Point Cloud. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01687">arXiv</a>] Point-Cloud Saliency Maps. [<a target="_blank" rel="noopener" href="https://github.com/tianzheng4/PointCloud-Saliency-Maps">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1901.03006">arXiv</a>] Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers. [<a target="_blank" rel="noopener" href="https://github.com/Daniel-Liu-c0deb0t/3D-Neural-Network-Adversarial-Attacks">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.08396">arxiv</a>] Context Prediction for Unsupervised Deep Learning on Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.09280">arXiv</a>] Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://export.arxiv.org/abs/1901.09394">arXiv</a>] NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler. [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.05247">arXiv</a>] 3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.10272">arXiv</a>] Zero-shot Learning of 3D Point Cloud Objects. [<a target="_blank" rel="noopener" href="https://github.com/alichr/Zero-shot-Learning-of-3D-Point-Cloud-Objects">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.09847">arXiv</a>] Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.01695">arXiv</a>] Real-time Multiple People Hand Localization in 4D Point Clouds. [<strong><code>det.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.02858">arXiv</a>] Variational Graph Methods for Efficient Point Cloud Sparsification. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05807">arXiv</a>] Neural Style Transfer for Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07918">arXiv</a>] OREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios. [<strong><code>pos.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.10750">arXiv</a>] FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/LordLiang/FVNet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00069">arXiv</a>] Unpaired Point Cloud Completion on Real Scans using Adversarial Training. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00230">arXiv</a>] MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00817">arXiv</a>] DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric Learning on 3D Point Clouds. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.07537">arXiv</a>] Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/AI-liu/Complex-YOLO">pytorch</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>] 🔥</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.10795">arXiv</a>] Graph-based Inpainting for 3D Dynamic Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.11027">arXiv</a>] nuScenes: A multimodal dataset for autonomous driving. [<a target="_blank" rel="noopener" href="https://www.nuscenes.org/overview">link</a>] [<strong><code>dat.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.08373">arXiv</a>] 3D Backbone Network for 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/Benzlxs/tDBN">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07605v3">arXiv</a>] Adversarial Autoencoders for Compact Representations of 3D Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/MaciejZamorski/3d-AAE">pytorch</a>] [<strong><code>rel.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.10014.pdf">arXiv</a>] Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.08705">arXiv</a>] GAPNet: Graph Attention based Point Neural Network for Exploiting Local Feature of Point Cloud. [<a target="_blank" rel="noopener" href="https://github.com/FrankCAN/GAPNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01140">arXiv</a>] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/Yang7879/3D-BoNet">tensorflow</a>] [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://export.arxiv.org/abs/1906.04173">arXiv</a>] Differentiable Surface Splatting for Point-based Geometry Processing. [<a target="_blank" rel="noopener" href="https://github.com/yifita/DSS">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.10887">arXiv</a>] Spatial Transformer for 3D Points. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03739">arXiv</a>] Point-Voxel CNN for Efficient 3D Deep Learning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08240">arXiv</a>] Neural Point-Based Graphics. [<a target="_blank" rel="noopener" href="https://dmitryulyanov.github.io/neural_point_based_graphics">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.02111.pdf">arXiv</a>] Point Cloud Super Resolution with Adversarial Residual Graph Networks. [<strong><code>oth.</code></strong>] [<a target="_blank" rel="noopener" href="https://github.com/wuhuikai/PointCloudSuperResolution">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.10209.pdf">arXiv</a>] Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.11069v1.pdf">arXiv</a>] StarNet: Targeted Computation for Object Detection in Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo">tensorflow</a>] [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.10168.pdf">arXiv</a>] Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDAR. [<strong><code>tra.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.07650v1.pdf">arXiv</a>] SAWNet: A Spatially Aware Deep Neural Network for 3D Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/balwantraikekutte/SAWNet">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.03670">arXiv</a>] Part-A^2 Net: 3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud. [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.03299.pdf">arXiv</a>] PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding Module for Classification and Segmentation. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.08287.pdf">arXiv</a>] PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/hehefan/PointRNN">tensorflow</a>] [<strong><code>tra.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.09798">arXiv</a>] PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points. [<a target="_blank" rel="noopener" href="https://github.com/paul007pl/PointAtrousGraph">tensorflow</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.05279.pdf">arXiv</a>] Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.09040.pdf">arXiv</a>] 3D-Rotation-Equivariant Quaternion Neural Networks. [<strong><code>cls.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.11026.pdf">arXiv</a>] Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules. [<strong><code>cls.</code></strong> <strong><code>rel.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.12885">arXiv</a>] Geometric Feedback Network for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00202">arXiv</a>] Relation Graph Network for 3D Object Detection in Point Clouds. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.13079.pdf">arXiv</a>] Deformable Filter Convolution for Point Cloud Reasoning. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.03264.pdf">arXiv</a>] PU-GCN: Point Cloud Upsampling via Graph Convolutional Network. [<a target="_blank" rel="noopener" href="https://sites.google.com/kaust.edu.sa/pugcn">project</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.11098.pdf">arXiv</a>] StructEdit: Learning Structural Shape Variations. [<a target="_blank" rel="noopener" href="https://github.com/daerduoCarey/structedit">project</a>] [<strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02984v1.pdf">arXiv</a>] Grid-GCN for Fast and Scalable Point Cloud Learning. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.10150.pdf">arXiv</a>] PointPainting: Sequential Fusion for 3D Object Detection. [<strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.07161.pdf">arXiv</a>] Transductive Zero-Shot Learning for 3D Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.10644.pdf">arXiv</a>] Geometry Sharing Network for 3D Point Cloud Classification and Segmentation. [<a target="_blank" rel="noopener" href="https://github.com/MingyeXu/GS-Net">pytorch</a>] [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.12033">arvix</a>] Deep Learning for 3D Point Clouds: A Survey. [<a target="_blank" rel="noopener" href="https://github.com/QingyongHu/SoTA-Point-Cloud">code</a>] [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.01800v1.pdf">arXiv</a>] Spectral-GANs for High-Resolution 3D Point-cloud Generation. [<strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12663.pdf">arXiv</a>] Point Attention Network for Semantic Segmentation of 3D Point Clouds. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07137v1.pdf">arXiv</a>] PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation. [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.08159">arXiv</a>] 3D Object Recognition with Ensemble Learning — A Study of Point Cloud-Based Deep Learning Models. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong>]</li>
</ul>
<hr>
<h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00280">AAAI</a>] Morphing and Sampling Network for Dense Point Cloud Completion. [<a target="_blank" rel="noopener" href="https://github.com/Colin97/MSN-Point-Cloud-Completion">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.05163.pdf">AAAI</a>] TANet: Robust 3D Object Detection from Point Clouds with Triple Attention. [<a target="_blank" rel="noopener" href="https://github.com/happinesslz/TANet">code</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.10775">AAAI</a>] Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.09361">AAAI</a>] PRIN: Pointwise Rotation-Invariant Network. [<strong><code>seg.</code></strong> <strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00497">CVPR</a>] Just Go with the Flow: Self-Supervised Scene Flow Estimation. [<a target="_blank" rel="noopener" href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">code</a>][<strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.00195">CVPR</a>] SGAS: Sequential Greedy Architecture Search. [<a target="_blank" rel="noopener" href="https://github.com/lightaime/sgas">code</a>] [<strong><code>cls.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.11236.pdf">CVPR</a>] RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/QingyongHu/RandLA-Net">tensorflow</a>] [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.05119">CVPR</a>] Learning multiview 3D point cloud registration. [<a target="_blank" rel="noopener" href="https://github.com/zgojcic/3D_multiview_reg">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.00410.pdf">CVPR</a>] PF-Net: Point Fractal Network for 3D Point Cloud Completion. [<a target="_blank" rel="noopener" href="https://github.com/zztianzz/PF-Net-Point-Fractal-Network.git">pytorch</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.05679.pdf">CVPR</a>] MLCVNet: Multi-Level Context VoteNet for 3D Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/NUAAXQ/MLCVNet">code</a>] [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lang_SampleNet_Differentiable_Point_Cloud_Sampling_CVPR_2020_paper.pdf">CVPR</a>] SampleNet: Differentiable Point Cloud Sampling. [<a target="_blank" rel="noopener" href="https://github.com/itailang/SampleNet">code</a>] [<strong><code>cls.</code></strong> <strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.html">CVPR</a>] MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment.  [<strong><code>reg.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.01014">CVPR</a>] Feature-metric Registration: A Fast Semi-supervised Approach for Robust Point Cloud Registration without Correspondences. [<a target="_blank" rel="noopener" href="https://github.com/XiaoshuiHuang/fmr">code</a>] [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.02545.pdf">CVPR</a>] Attentive Context Normalization for Robust Permutation-Equivariant Learning. [<a target="_blank" rel="noopener" href="https://github.com/vcg-uvic/acne">code</a>] [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.01456.pdf">CVPR</a>] Implicit Functions in Feature Space for Shape Reconstruction and Completion. [<a target="_blank" rel="noopener" href="https://github.com/jchibane/if-net">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.10876.pdf">CVPR</a>] PointAugment: an Auto-Augmentation Framework for Point Cloud Classification. [<strong><code>cls.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.08487.pdf">WACV</a>] FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.10692">arXiv</a>] ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes. [<strong><code>det.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.12098.pdf">ECCV</a>] Quaternion Equivariant Capsule Networks for 3D Point Clouds. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.10985.pdf">ECCV</a>] PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10826">ECCV</a>] DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares. [<a target="_blank" rel="noopener" href="https://github.com/sitzikbs/DeepFit">code</a>] [<strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.11784v2">ECCV</a>] DPDist: Comparing Point Clouds Using Deep Point Cloud Distance. [<a target="_blank" rel="noopener" href="https://github.com/dahliau/DPDist">code</a>] [<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://hal.inria.fr/hal-02927350/document">IROS</a>] GndNet: Fast Ground Plane Estimation and Point Cloud Segmentation for Autonomous Vehicles. [<a target="_blank" rel="noopener" href="https://github.com/anshulpaigwar/GndNet">code</a>] [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.00118.pdf">ICLR</a>] AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing. [<a target="_blank" rel="noopener" href="https://github.com/xingzhehe/AdvectiveNet-An-Eulerian-Lagrangian-Fluidic-Reservoir-for-Point-Cloud-Processing">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04569">arXiv</a>] Parameter-Efficient Person Re-identification in the 3D Space. <a target="_blank" rel="noopener" href="https://github.com/layumi/person-reid-3d">[code]</a>[<strong><code>rel.</code></strong>] 🔥</li>
</ul>
<h2 id="2021"><a href="#2021" class="headerlink" title="2021"></a>2021</h2><ul>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=O3bqkf_Puys">ICLR</a>] PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://hehefan.github.io/pdfs/p4transformer.pdf">CVPR</a>] Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos. [<a target="_blank" rel="noopener" href="https://github.com/hehefan/P4Transformer">code</a>][<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.00987">CVPR</a>] PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds. [<a target="_blank" rel="noopener" href="https://github.com/weiyithu/PV-RAFT">code</a>][<strong><code>oth.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.07647">ICRA</a>] FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection. [<a target="_blank" rel="noopener" href="https://github.com/weiyithu/FGR">code</a>][<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li></li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hamdi_MVTN_Multi-View_Transformation_Network_for_3D_Shape_Recognition_ICCV_2021_paper.pdf">ICCV</a>] MVTN: Multi-View Transformation Network for 3D Shape Recognition. [<a target="_blank" rel="noopener" href="https://github.com/ajhamdi/MVTN">code</a>][<strong><code>det.</code></strong> <strong><code>rel.</code></strong>]</li>
</ul>
<h1>

<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">- Datasets</span></span><br></pre></td></tr></table></figure>

</h1>

<ul>
<li>[<a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/">KITTI</a>] The KITTI Vision Benchmark Suite. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://modelnet.cs.princeton.edu/">ModelNet</a>] The Princeton ModelNet . [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.shapenet.org/">ShapeNet</a>]  A collaborative dataset between researchers at Princeton, Stanford and TTIC. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://shapenet.org/download/parts">PartNet</a>] The PartNet dataset provides fine grained part annotation of objects in ShapeNetCore. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://kevinkaixu.net/projects/partnet.html">PartNet</a>] PartNet benchmark from Nanjing University and National University of Defense Technology. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://buildingparser.stanford.edu/dataset.html#Download">S3DIS</a>] The Stanford Large-Scale 3D Indoor Spaces Dataset. [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.scan-net.org/">ScanNet</a>] Richly-annotated 3D Reconstructions of Indoor Scenes. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D</a>] The Stanford 3D Scanning Repository. [<strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://staffhome.ecm.uwa.edu.au/~00053650/databases.html">UWA Dataset</a>] . [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>reg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://shape.cs.princeton.edu/benchmark/">Princeton Shape Benchmark</a>] The Princeton Shape Benchmark.</li>
<li>[<a target="_blank" rel="noopener" href="http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml">SYDNEY URBAN OBJECTS DATASET</a>] This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://projects.asl.ethz.ch/datasets/doku.php?id=home">ASL Datasets Repository(ETH)</a>] This site is dedicated to provide datasets for the Robotics community with the aim to facilitate result evaluations and comparisons. [<strong><code>cls.</code></strong> <strong><code>match.</code></strong> <strong><code>reg.</code></strong> <strong><code>det</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.semantic3d.net/">Large-Scale Point Cloud Classification Benchmark(ETH)</a>] This benchmark closes the gap and provides a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://asrl.utias.utoronto.ca/datasets/3dmap/">Robotic 3D Scan Repository</a>] The Canadian Planetary Emulation Terrain 3D Mapping Dataset is a collection of three-dimensional laser scans gathered at two unique planetary analogue rover test facilities in Canada.</li>
<li>[<a target="_blank" rel="noopener" href="http://radish.sourceforge.net/">Radish</a>] The Robotics Data Set Repository (Radish for short) provides a collection of standard robotics data sets.</li>
<li>[<a target="_blank" rel="noopener" href="http://data.ign.fr/benchmarks/UrbanAnalysis/#">IQmulus &amp; TerraMobilita Contest</a>] The database contains 3D MLS data from a dense urban environment in Paris (France), composed of 300 million points. The acquisition was made in January 2013. [<strong><code>cls.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/">Oakland 3-D Point Cloud Dataset</a>] This repository contains labeled 3-D point cloud laser data collected from a moving platform in a urban environment.</li>
<li>[<a target="_blank" rel="noopener" href="http://kos.informatik.uni-osnabrueck.de/3Dscans/">Robotic 3D Scan Repository</a>] This repository provides 3D point clouds from robotic experiments，log files of robot runs and standard 3D data sets for the robotics community.</li>
<li>[<a target="_blank" rel="noopener" href="http://robots.engin.umich.edu/SoftwareData/Ford">Ford Campus Vision and Lidar Data Set</a>] The dataset is collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck.</li>
<li>[<a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/teichman/stc/">The Stanford Track Collection</a>] This dataset contains about 14,000 labeled tracks of objects as observed in natural street scenes by a Velodyne HDL-64E S2 LIDAR.</li>
<li>[<a target="_blank" rel="noopener" href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL3D+</a>] Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild. [<strong><code>pos.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.kaggle.com/daavoo/3d-mnist">3D MNIST</a>] The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition. [<strong><code>cls.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://wad.ai/2019/challenge.html">WAD</a>] [<a target="_blank" rel="noopener" href="http://apolloscape.auto/tracking.html">ApolloScape</a>] The datasets are provided by Baidu Inc. [<strong><code>tra.</code></strong> <strong><code>seg.</code></strong> <strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://d3u7q4379vrm7e.cloudfront.net/object-detection">nuScenes</a>] The nuScenes dataset is a large-scale autonomous driving dataset.</li>
<li>[<a target="_blank" rel="noopener" href="https://uwaterloo.ca/waterloo-intelligent-systems-engineering-lab/projects/precise-synthetic-image-and-lidar-presil-dataset-autonomous">PreSIL</a>] Depth information, semantic segmentation (images), point-wise segmentation (point clouds), ground point labels (point clouds), and detailed annotations for all vehicles and people. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.00160">paper</a>] [<strong><code>det.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://3dmatch.cs.princeton.edu/">3D Match</a>] Keypoint Matching Benchmark, Geometric Registration Benchmark, RGB-D Reconstruction Datasets. [<strong><code>reg.</code></strong> <strong><code>rec.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/VCCIV/BLVD">BLVD</a>] (a) 3D detection, (b) 4D tracking, (c) 5D interactive event recognition and (d) 5D intention prediction. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06405v1">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong> <strong><code>oth.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.03605">PedX</a>] 3D Pose Estimation of Pedestrians, more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.03605">ICRA 2019 paper</a>] [<strong><code>pos.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://usa.honda-ri.com/H3D">H3D</a>] Full-surround 3D multi-object detection and tracking dataset. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.01568">ICRA 2019 paper</a>] [<strong><code>det.</code></strong> <strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li><a target="_blank" rel="noopener" href="https://www.argoverse.org/">[Argoverse BY ARGO AI]</a> Two public datasets (3D Tracking and Motion Forecasting) supported by highly detailed maps to test, experiment, and teach self-driving vehicles how to understand the world around them.[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html">CVPR 2019 paper</a>][<strong><code>tra.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://niessner.github.io/Matterport/">Matterport3D</a>] RGB-D: 10,800 panoramic views from 194,400 RGB-D images. Annotations: surface reconstructions, camera poses, and 2D and 3D semantic segmentations. Keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and scene classification. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.06158">3DV 2017 paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/niessner/Matterport">code</a>] [<a target="_blank" rel="noopener" href="https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/">blog</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.04758">SynthCity</a>] SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Nine categories. [<strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://level5.lyft.com/dataset/?source=post_page">Lyft Level 5</a>] Include high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://semantic-kitti.org/">SemanticKITTI</a>] Sequential Semantic Segmentation, 28 classes, for autonomous driving. All sequences of KITTI odometry labeled. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01416">ICCV 2019 paper</a>] [<strong><code>seg.</code></strong> <strong><code>oth.</code></strong> <strong><code>aut.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="http://npm3d.fr/paris-lille-3d">NPM3D</a>] The Paris-Lille-3D  has been produced by a Mobile Laser System (MLS) in two different cities in France (Paris and Lille). [<strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://waymo.com/open/">The Waymo Open Dataset</a>] The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/I2RDL2/ASTAR-3D">A*3D: An Autonomous Driving Dataset in Challeging Environments</a>] A*3D: An Autonomous Driving Dataset in Challeging Environments. [<strong><code>det.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://github.com/canqin001/PointDAN">PointDA-10 Dataset</a>] Domain Adaptation for point clouds.</li>
<li>[<a target="_blank" rel="noopener" href="https://robotcar-dataset.robots.ox.ac.uk/">Oxford Robotcar</a>] The dataset captures many different combinations of weather, traffic and pedestrians. [<strong><code>cls.</code></strong> <strong><code>det.</code></strong> <strong><code>rec.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://scale.com/open-datasets/pandaset">PandaSet</a>] Public large-scale dataset for autonomous driving provided by Hesai &amp; Scale. It enables researchers to study challenging urban driving situations using the full sensor suit of a real self-driving-car. [<strong><code>det.</code></strong> <strong><code>seg.</code></strong>]</li>
<li>[<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset">3D-FRONT</a> <a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future">3D-FUTURE</a>] [Alibaba] 3D-FRONT contains 10,000 houses (or apartments) and ~70,000 rooms with layout information. 3D-FUTURE contains 20,000+ clean and realistic synthetic scenes in 5,000+ diverse rooms which contain 10,000+ unique high quality 3D instances of furniture.</li>
<li>[<a target="_blank" rel="noopener" href="https://3d.dataset.site/">Campus3D</a>] The Campus3D contains a photogrametry point cloud which has 931.7 million points, covering 1.58 km2 of 6 connected campus regions of NUS. The dataset are point-wisely annotated with a hierarchical structure of 24 semantic labels and contains 2,530 instances based on the labels. [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.04968.pdf">MM 2020 paper</a>][<a target="_blank" rel="noopener" href="https://github.com/shinke-li/Campus3D">code</a>][ <strong><code>det.</code></strong> <strong><code>cls.</code></strong> <strong><code>seg.</code></strong>]</li>
</ul>
<h1 id="参考来源"><a href="#参考来源" class="headerlink" title="参考来源"></a>参考来源</h1><p><a target="_blank" rel="noopener" href="https://cvpr2021.thecvf.com/">https://cvpr2021.thecvf.com/</a><br><a target="_blank" rel="noopener" href="https://cvpr2022.thecvf.com/">https://cvpr2022.thecvf.com/</a></p>
<p>论文与code查询网站：<a target="_blank" rel="noopener" href="https://paperswithcode.com/">https://paperswithcode.com/</a><br>AI论文查询地址：<a target="_blank" rel="noopener" href="https://arxiv.org/list/cs.AI/recent">https://arxiv.org/list/cs.AI/recent</a></p>
<p>论文：<a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation">https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation</a>综述：<a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/survey-computer-vision-2020">https://github.com/extreme-assistant/survey-computer-vision-2020</a>\</p>
<ul>
<li>推荐阅读：<code>&lt;br&gt;</code><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation">ICCV2021&#x2F;2019&#x2F;2017 论文&#x2F;代码&#x2F;解读&#x2F;直播合集</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/survey-computer-vision">2020-2021年计算机视觉综述论文汇总</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/Awesome-CV-Team">国内外优秀的计算机视觉团队汇总</a></li>
</ul>
</li>
</ul>
<hr>
<h1 id="cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017（Papers-x2F-Codes-x2F-Project-x2F-Paper-reading）"><a href="#cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017（Papers-x2F-Codes-x2F-Project-x2F-Paper-reading）" class="headerlink" title="cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017（Papers&#x2F;Codes&#x2F;Project&#x2F;Paper reading）"></a>cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017（Papers&#x2F;Codes&#x2F;Project&#x2F;Paper reading）</h1><p>论文解读汇总：<a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/3031">https://bbs.cvmart.net/articles/3031</a> <code>&lt;br&gt;</code><br>论文分类汇总：<a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4267%60">https://bbs.cvmart.net/articles/4267`</a><br><br><code> 2000~2020年历届CVPR最佳论文代码，解读等汇总：http://bbs.cvmart.net/topics/665/CVPR-Best-Paper</code><br><code> </code><br>&#96;</p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p><a href="#8">8. CVPR2021最新信息及论文下载</a><code>&lt;br&gt;</code><br><a href="#7">7. CVPR2021论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#6">6. CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播</a><code>&lt;br&gt;</code><br><a href="#5">5. CVPR2020论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#4">4. CVPR2019全部论文下载&#x2F;开源代码</a><code>&lt;br&gt;</code><br><a href="#3">3. CVPR2019论文分方向盘点</a><code>&lt;br&gt;</code><br><a href="#2">2. CVPR2019论文直播分享</a><code>&lt;br&gt;</code><br><a href="#1">1. CVPR2018&#x2F;CVPR2017</a><code>&lt;br&gt;</code></p>
<br>
<a name="8">

<h1 id="8-CVPR2021最新论文分类汇总-持续更新"><a href="#8-CVPR2021最新论文分类汇总-持续更新" class="headerlink" title="8.CVPR2021最新论文分类汇总(持续更新)"></a><a href>8.CVPR2021最新论文分类汇总(持续更新)</a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation/blob/master/CVPR2021.md">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReading／Demos&#x2F;直播分享／论文分享会等</a><code>&lt;br&gt;</code><ul>
<li><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1TWPkRukz9JC4Br-g_Ws5OA">CVPR2021全部论文下载（共1661篇）</a> 提取码：su7e</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4368">CVPR2021 论文解读汇总 + 技术直播汇总</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/4366">CVPR2021 Oral论文汇总&#x2F;解读</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="7">

<h1 id="7-CVPR2021论文分方向盘点-lt-br-gt"><a href="#7-CVPR2021论文分方向盘点-lt-br-gt" class="headerlink" title="7.CVPR2021论文分方向盘点&lt;br&gt;"></a><a href>7.CVPR2021论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Ho7qtrpF9FhHGaamkQo6Lw">一文看尽CVPR2021 2D 目标检测论文（27篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ysfwYQ3sVvXINPzBR91S7A">一文看尽CVPR2021 图像异常检测论文（6篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/w1jPD2AbxnENUBgfdFLFSg">一文看尽CVPR2021 伪装目标检测+旋转目标检测论文（6篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_a0UmZSSxvVUFUGOnMrMhw">CVPR2021 论文大盘点：全景分割论文汇总（共15篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5832">CVPR2021 论文大盘点：人员重识别汇总（共26篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5831">CVPR2021 论文大盘点：行人技术汇总（共7篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5829">CVPR2021 论文大盘点：医学影像汇总（共22篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5560">CVPR2021 论文大盘点：超分辨率汇总（共32篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5824">CVPR2021 论文大盘点：图像修复汇总（共20篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5828">CVPR2021 论文大盘点：图像去噪汇总（共14篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5827">CVPR2021 论文大盘点：去雾去模糊汇总（共14篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5826">CVPR2021 论文大盘点：图像视频去雨汇总（共10篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5562">CVPR2021 论文大盘点：文本图像汇总（共17篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5811">CVPR2021 论文大盘点：人脸识别汇总（共15篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5810">CVPR2021 论文大盘点：人脸造假检测汇总（共9篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5809">CVPR2021 论文大盘点：图像压缩汇总（共5篇）</a></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/5830">CVPR2021 论文大盘点：遥感与航拍影像汇总（共7篇）</a></li>
</ul>
<br>
<a name="6">

<h1 id="6-CVPR2020论文下载-x2F-代码-x2F-解读-x2F-直播"><a href="#6-CVPR2020论文下载-x2F-代码-x2F-解读-x2F-直播" class="headerlink" title="6.CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播"></a><a href>6.CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播</a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2020/blob/master/CVPR2020.md#cvpr2020%E6%9C%80%E6%96%B0%E4%BF%A1%E6%81%AF%E5%8F%8A%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD%E8%B4%B4paperscodesprojectpaperreadingdemos%E7%9B%B4%E6%92%AD%E5%88%86%E4%BA%AB%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E4%BC%9A%E7%AD%89">Papers&#x2F;Codes&#x2F;Project&#x2F;PaperReading／Demos&#x2F;直播分享／论文分享会等</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1UXW6iviZ_d3wpdujNgWJSQ">CVPR2020全部论文下载（共1467篇）</a><code>&lt;br&gt;</code><br>提取码：pun7<code>&lt;br&gt;&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/articles/3031">CVPR2020 论文解读汇总 + 技术直播汇总</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="5">

<h1 id="5-CVPR2020论文分方向盘点-lt-br-gt"><a href="#5-CVPR2020论文分方向盘点-lt-br-gt" class="headerlink" title="5.CVPR2020论文分方向盘点&lt;br&gt;"></a><a href>5.CVPR2020论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/3028">20.CVPR 2020 论文大盘点-动作检测与动作分割（13篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/3000">19.CVPR 2020 论文大盘点-动作识别（21篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2992">18.CVPR 2020 论文大盘点-光流（12篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2964">17.CVPR 2020 论文大盘点-图像与视频检索（16篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2953">16.CVPR 2020 论文大盘点-遥感与航拍影像处理识别（18篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2923">15.CVPR 2020 论文大盘点-图像质量评价（7篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2903">14.CVPR 2020 论文大盘点-图像修复 Inpainting （7篇）</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2902">13.CVPR 2020 论文大盘点-图像增强与图像恢复（22篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2876">12.CVPR 2020 论文大盘点-去雨去雾去模糊（8篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2855">11.CVPR 2020 论文大盘点-医学影像处理识别（19篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2854">10.CVPR 2020 论文大盘点-抠图 Matting （3篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2829">9.CVPR 2020 论文大盘点-图像分割（25篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2818">8.CVPR 2020 论文大盘点-全景分割与视频目标分割（8篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2725">7.CVPR 2020 论文大盘点-超分辨（21篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2732">6.CVPR 2020 论文大盘点-目标检测（64篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2720">5.CVPR 2020 论文大盘点-人脸技术（64篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2733">4.CVPR 2020 论文大盘点-目标跟踪（33篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2778">3.CVPR 2020 论文大盘点-文本图像（16篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2751">2.CVPR 2020 论文大盘点-行人检测与重识别（33篇）</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://bbs.cvmart.net/topics/2806">1.CVPR 2020 论文大盘点-实例分割（18篇）</a><code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<p><code>&lt;br&gt;&lt;br&gt;</code></p>
<br>
<a name="4">

<h1 id="4-CVPR2019全部论下载-x2F-开源代码-lt-br-gt"><a href="#4-CVPR2019全部论下载-x2F-开源代码-lt-br-gt" class="headerlink" title="4.CVPR2019全部论下载&#x2F;开源代码&lt;br&gt;"></a><a href>4.CVPR2019全部论下载&#x2F;开源代码<code>&lt;br&gt;</code></a></h1><p><a href>全部1294篇<code>&lt;br&gt;</code></a></p>
<ul>
<li><a href>全部链接：http://openaccess.thecvf.com/CVPR2019.py <code>&lt;br&gt;</code></a></li>
<li><a href>下载链接:<code>&lt;br&gt;</code><br>链接:https://pan.baidu.com/s/1dhXrWFHeKeJ1kFsKBxQzVg  密码:f53l</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/extreme-assistant/cvpr2019/blob/master/cvpr_2019_githublinks.csv">CVPR 2019全部论文开源源码汇总Excel点这里</a></li>
</ul>
<p><code>&lt;br&gt;&lt;br&gt;</code></p>
<br>
<a name="3">

<h1 id="3-CVPR2019论文分方向盘点-lt-br-gt"><a href="#3-CVPR2019论文分方向盘点-lt-br-gt" class="headerlink" title="3.CVPR2019论文分方向盘点&lt;br&gt;"></a><a href>3.CVPR2019论文分方向盘点<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/articles/523/cvpr-2019-lun-wen-da-pan-dian-mu-biao-gen-zong-pian">CVPR 2019 论文大盘点-目标跟踪篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/452/cvpr-2019-lun-wen-da-pan-dian-chao-fen-bian-lv-pian">CVPR 2019 论文大盘点-超分辨率篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/451/cvpr-2019-lun-wen-da-pan-dian-ren-lian-ji-shu-pian">CVPR 2019 论文大盘点-人脸技术篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/l8Cfi3CIt2gqVC9i3LV6hw">CVPR 2019 论文大盘点—目标检测篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/535/CVPR2019-Text">CVPR 2019 论文大盘点—文本图像篇</a><code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/464/cvpr-2019-gong-bu-mo-xing-jian-zhi-lun-wen-hui-zong">CVPR2019模型剪枝论文汇总</a><code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<br>
<a name="2">

<h1 id="2-CVPR2019论文直播分享-lt-br-gt"><a href="#2-CVPR2019论文直播分享-lt-br-gt" class="headerlink" title="2.CVPR2019论文直播分享&lt;br&gt;"></a><a href>2.CVPR2019论文直播分享<code>&lt;br&gt;</code></a></h1><ul>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/609/CVPR-2019">微软亚研院CVPR2019线下分享会视频回放及PPT下载</a></li>
<li>3&#x2F;28晚点云分割分享回放<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/topics/351/%E8%81%94%E5%90%88%E5%88%86%E5%89%B2%E7%82%B9%E4%BA%91%E4%B8%AD%E7%9A%84%E5%AE%9E%E4%BE%8B%E5%92%8C%E8%AF%AD%E4%B9%89">王鑫龙：联合分割点云中的实例和语义（开源，列表id 27)</a><code>&lt;br&gt;</code></li>
<li>4月18日晚目标检测分享回放<code>&lt;br&gt;</code><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/CvzFG63c1bTuWFSIzNSxBA">CMU诸宸辰:基于Anchor-free特征选择模块的单阶目标检测(CVPR2019，列表id 88)</a> <code>&lt;br&gt;</code></li>
<li>5月9日晚单目标跟踪分享回放<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/3vlVXQDh6ou8Gdhg4xY2Tg">张志鹏:基于siamese网络的单目标跟踪(CVPR2019 Oral，列表id 65)</a><code>&lt;br&gt;</code></li>
<li>[5月30日晚人脸识别分享回放<code>&lt;br&gt;</code><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/SIHFTbDc_XjbfYfpgwNYeQ">邓健康-CVPR2019:ArcFace 构建高效的人脸识别系统(CVPR2019，列表id 243)</a>：<code>&lt;br&gt;</code></li>
<li>6月13日晚三维多人多视角姿态识别分享回放<code>&lt;br&gt;</code><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Td510LMs3UWV_8d5kDgFYw">董峻廷：多视角下多人三维姿态估计 CVPR2019，列表id 106</a><code>&lt;br&gt;</code></li>
</ul>
<br>
<a name="1">

<h1 id="1-CVPR2018-x2F-CVPR2017-lt-br-gt"><a href="#1-CVPR2018-x2F-CVPR2017-lt-br-gt" class="headerlink" title="1.CVPR2018&#x2F;CVPR2017&lt;br&gt;"></a><a href>1.CVPR2018&#x2F;CVPR2017<code>&lt;br&gt;</code></a></h1><ul>
<li><a href>CVPR 2018全部论文下载百度云链接：https://pan.baidu.com/s/1bhYzNz2TGijUdfPIdyEGtg <code>&lt;br&gt;</code> 密码:gyk2</a></li>
<li><a target="_blank" rel="noopener" href="http://bbs.cvmart.net/articles/56/cvpr-2018-lun-wen-jie-du-ji-jin-190326-geng-xin"><strong>CVPR 2018论文解读汇总</strong></a></li>
<li>CVPR 2017全部论文下载百度云链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w">https://pan.baidu.com/s/1p_If8S_AAgnTlZxfzBya2w</a>  <code>&lt;br&gt;</code> 密码:o6tu</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27651707"><strong>CVPR 2017论文解读集锦</strong></a></li>
</ul>
<h3 id="参考链接-lt-br-gt"><a href="#参考链接-lt-br-gt" class="headerlink" title="参考链接&lt;br&gt;"></a>参考链接<code>&lt;br&gt;</code></h3><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A">https://mp.weixin.qq.com/s/YRcajgSTJq_evwtn7ZFo4A</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hoya012/CVPR-2019-Paper-Statistics">https://github.com/hoya012/CVPR-2019-Paper-Statistics</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jonahthelion/cvpr_with_code">https://github.com/jonahthelion/cvpr_with_code</a> <code>&lt;br&gt;</code></li>
<li><a target="_blank" rel="noopener" href="https://github.com/amusi/daily-paper-computer-vision">https://github.com/amusi/daily-paper-computer-vision</a> <code>&lt;br&gt;&lt;br&gt;</code></li>
</ul>
<h1 id="NLPs"><a href="#NLPs" class="headerlink" title="NLPs"></a>NLPs</h1><p>Deep learning speech learning library</p>
<p>Py2neo 手册<br>Py2neo是一个客户端库和工具包，用于从Python应用程序和命令行中使用Neo4j 。该库支持 Bolt 和 HTTP，并提供高级 API、OGM、管理工具、交互式控制台、Pygments 的 Cypher 词法分析器以及许多其他花里胡哨。从版本 2021.1 开始，Py2neo 包含对路由的完全支持，正如 Neo4j 集群所公开的那样。这可以使用neo4j:&#x2F;&#x2F;…URI 或传递routing&#x3D;True给Graph构造函数来启用。<a target="_blank" rel="noopener" href="https://py2neo.org/2021.1/">https://py2neo.org/2021.1/</a></p>
<h1 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h1><p>中文、英文NER、英汉机器翻译数据集。中英文实体识别数据集，中英文机器翻译数据集，中文分词数据集：<a target="_blank" rel="noopener" href="https://github.com/quincyliang/nlp-public-dataset">https://github.com/quincyliang/nlp-public-dataset</a></p>
<p>CTB词性标注集</p>
<p><img src="https://user-images.githubusercontent.com/36963108/170655243-6267fbc4-246d-40f0-8303-7c97a934918a.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/36963108/170656452-738fbb77-03bc-4315-a3fb-d14a50a9b5f6.png" alt="image"></p>
<p>ck: <a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173">https://help.aliyun.com/document_detail/179146.html?scm=20140722.184.2.173</a></p>
<p>标注标签说明：<a target="_blank" rel="noopener" href="https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf">https://verbs.colorado.edu/chinese/segguide.3rd.ch.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40332976/article/details/120331450">https://blog.csdn.net/qq_40332976/article/details/120331450</a></p>
<h1 id="资料汇总"><a href="#资料汇总" class="headerlink" title="资料汇总"></a>资料汇总</h1><p>一个轻量级、简单易用的 RNN 唤醒词监听器: <a target="_blank" rel="noopener" href="https://github.com/MycroftAI/mycroft-precise">https://github.com/MycroftAI/mycroft-precise</a></p>
<p>zh:<a target="_blank" rel="noopener" href="http://fancyerii.github.io/books/mycroft-precise/">http://fancyerii.github.io/books/mycroft-precise/</a></p>
<p>基于树莓派的人工智能小车，实现识别、提示、智能旅游线路、离线图像:<br><a target="_blank" rel="noopener" href="https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi">https://github.com/dalinzhangzdl/AI_Car_Raspberry-pi</a></p>
<p>中文NLP数据集:<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUEDatasetSearch">https://github.com/CLUEbenchmark/CLUEDatasetSearch</a></p>
<p>模型：<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p>
<p>中文 NLP 资源精选列表 中文自然语言处理相关资料:<br><a target="_blank" rel="noopener" href="https://github.com/crownpku/Awesome-Chinese-NLP">https://github.com/crownpku/Awesome-Chinese-NLP</a></p>
<p>视觉聊天机器人:<a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/visual-dialog">https://paperswithcode.com/paper/visual-dialog</a></p>
<p>Bert&#x2F;Transformer模型压缩与优化加速: <a target="_blank" rel="noopener" href="https://blog.csdn.net/nature553863/article/details/120292394%EF%BC%9A">https://blog.csdn.net/nature553863/article/details/120292394：</a></p>
<p>可以压缩 BERT 的所有方式：<a target="_blank" rel="noopener" href="http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html">http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html</a><br><a target="_blank" rel="noopener" href="https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html">https://www.leiphone.com/category/academic/MkV1j604LvPt1wcx.html</a></p>
<p>BERT轻量化探索—模型剪枝（BERT Pruning）—Rasa维度剪枝:<a target="_blank" rel="noopener" href="https://blog.csdn.net/ai_1046067944/article/details/103609152">https://blog.csdn.net/ai_1046067944/article/details/103609152</a></p>
<p>压缩 BERT 以加快预测速度:<a target="_blank" rel="noopener" href="https://rasa.com/blog/compressing-bert-for-faster-prediction-2/">https://rasa.com/blog/compressing-bert-for-faster-prediction-2/</a></p>
<p>论文综述与BERT相关最新论文:<a target="_blank" rel="noopener" href="https://github.com/tomohideshibata/BERT-related-papers">https://github.com/tomohideshibata/BERT-related-papers</a></p>
<p>中文自然语言排行榜及论文查询:<a target="_blank" rel="noopener" href="https://www.cluebenchmarks.com/index.html">https://www.cluebenchmarks.com/index.html</a></p>
<p>计算语言学国际会议论文集:<a target="_blank" rel="noopener" href="https://aclanthology.org/volumes/2020.coling-main/">https://aclanthology.org/volumes/2020.coling-main/</a></p>
<p>计算语言学协会第 58 届年会论文集:<a target="_blank" rel="noopener" href="https://aclanthology.org/volumes/2020.acl-main/">https://aclanthology.org/volumes/2020.acl-main/</a></p>
<p>计算语言学2协会2021年会论文搜集：<a target="_blank" rel="noopener" href="https://aclanthology.org/events/acl-2021/">https://aclanthology.org/events/acl-2021/</a></p>
<p>中文BERT全词掩蔽预训练（中文BERT-wwm系列模型）<a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p>
<p>一个大规模的中文跨领域面向任务的对话数据集:<a target="_blank" rel="noopener" href="https://github.com/thu-coai/CrossWOZ">https://github.com/thu-coai/CrossWOZ</a></p>
<p>关于ConvLab-2：用于构建、评估和诊断对话系统的开源工具包（支持中文）：<a target="_blank" rel="noopener" href="https://github.com/thu-coai/ConvLab-2">https://github.com/thu-coai/ConvLab-2</a></p>
<p>视觉和语言预训练模型 (VL-PTM) 的最新进展(语音视觉融合):<a target="_blank" rel="noopener" href="https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers">https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers</a></p>
<p>深度学习和自然语言处理阅读清单:<a target="_blank" rel="noopener" href="https://github.com/IsaacChanghau/DL-NLP-Readings">https://github.com/IsaacChanghau/DL-NLP-Readings</a></p>
<p>视觉问答 (VQA)（图像&#x2F;视频问答）、视觉问题生成、视觉对话、视觉常识推理和相关领域的精选列表：<a target="_blank" rel="noopener" href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></p>
<p>汇总得不错的nlp学习资料:<a target="_blank" rel="noopener" href="https://jackkuo666.github.io/">https://jackkuo666.github.io/</a></p>
<p>dl4nlp自然语言处理深度学习课程材料:<a target="_blank" rel="noopener" href="https://github.com/liu-nlp/dl4nlp">https://github.com/liu-nlp/dl4nlp</a></p>
<p>论文与数据集网站：<a target="_blank" rel="noopener" href="https://www.ai2news.com/area/">https://www.ai2news.com/area/</a></p>
<p>HanLP的Python接口，支持自动下载与升级HanLP，兼容py2、py3。内部算法经过工业界和学术界考验，配套书籍《自然语言处理入门》已经出版，欢迎查阅随书代码:<a target="_blank" rel="noopener" href="https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1">https://github.com/jiajunhua/hankcs-pyhanlp/tree/3fc9c7d8a3f5eae00988db743c44b7708520b5f1</a></p>
<h1 id="pyhanlp文本训练与预测API接口"><a href="#pyhanlp文本训练与预测API接口" class="headerlink" title="pyhanlp文本训练与预测API接口"></a>pyhanlp文本训练与预测API接口</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyhanlp <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tests.test_utility <span class="keyword">import</span> ensure_data</span><br><span class="line"></span><br><span class="line">IClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.IClassifier&#x27;</span>)</span><br><span class="line">NaiveBayesClassifier = JClass(<span class="string">&#x27;com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier&#x27;</span>)</span><br><span class="line"><span class="comment"># 中文情感挖掘语料-ChnSentiCorp 数据来自：</span></span><br><span class="line">chn_senti_corp = ensure_data(<span class="string">&quot;ChnSentiCorp情感分析酒店评论&quot;</span>, <span class="string">&quot;http://file.hankcs.com/corpus/ChnSentiCorp.zip&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">classifier, text</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;《%s》 情感极性是 【%s】&quot;</span> % (text, classifier.classify(text)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classifier = NaiveBayesClassifier()</span><br><span class="line">    <span class="comment">#  创建分类器，更高级的功能请参考IClassifier的接口定义</span></span><br><span class="line">    classifier.train(chn_senti_corp)</span><br><span class="line">    <span class="comment">#  训练后的模型支持持久化，下次就不必训练了</span></span><br><span class="line">    predict(classifier, <span class="string">&quot;前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;结果大失所望，灯光昏暗，空间极其狭小，床垫质量恶劣，房间还伴着一股霉味。&quot;</span>)</span><br><span class="line">    predict(classifier, <span class="string">&quot;可利用文本分类实现情感分析，效果不是不行&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>原创来自这里:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0a131e042238">https://www.jianshu.com/p/0a131e042238</a></p>
<p><img src="https://user-images.githubusercontent.com/36963108/188380358-f15635f6-7ff8-4467-9aeb-88ecfef0acaa.png" alt="image"></p>
<p>好东西：<a target="_blank" rel="noopener" href="https://github.com/Kyubyong/nlp_tasks">https://github.com/Kyubyong/nlp_tasks</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/songyingxin/NLPer-Interview">https://github.com/songyingxin/NLPer-Interview</a></p>
<p>总结梳理自然语言处理工程师(NLP)需要积累的各方面知识，包括面试题，各种基础知识，工程能力等等，提升核心竞争力 <a target="_blank" rel="noopener" href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p>
<p>史上最全Transformer面试题<br>答案解析(1)-史上最全Transformer面试题<br>Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点<br>解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数(附Pytorch代码详细解读)<br>3分钟从零解读Transformer的Encoder<br>原版Transformer的位置编码究竟有没有包含相对位置信息<br>BN踩坑记–谈一下Batch Normalization的优缺点和适用场景<br>谈一下相对位置编码<br>NLP任务中-layer-norm比BatchNorm好在哪里<br>谈一谈Decoder模块<br>Transformer的并行化<br>Transformer全部文章合辑<br>RNN的梯度消失有什么与众不同的地方.md<br>VIT-如何将Transformer更好的应用到CV领域</p>
<p>好书：<a target="_blank" rel="noopener" href="https://github.com/FudanNLP/nlp-beginner">https://github.com/FudanNLP/nlp-beginner</a></p>
<p>论文与code:<a target="_blank" rel="noopener" href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p>
<p>跟踪自然语言处理的进展:<a target="_blank" rel="noopener" href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p>
<p>元研究：<a target="_blank" rel="noopener" href="https://research.facebook.com/research-areas/">https://research.facebook.com/research-areas/</a></p>
<p>此项目是机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现，也是作为一个算法工程师必会的理论基础知识。<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb">https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb</a></p>
<p>NLP-Models-Tensorflow  <a target="_blank" rel="noopener" href="https://github.com/huseinzol05/NLP-Models-Tensorflow">https://github.com/huseinzol05/NLP-Models-Tensorflow</a></p>
<p>相似度 <a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md">https://github.com/duoergun0729/nlp/blob/master/%E6%96%87%E6%A1%A3%E7%9B%B8%E4%BC%BC%E5%BA%A6.md</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/fighting41love/funNLP">https://github.com/fighting41love/funNLP</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/keon/awesome-nlp">https://github.com/keon/awesome-nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/duoergun0729/nlp">https://github.com/duoergun0729/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/sebastianruder/NLP-progress">https://github.com/sebastianruder/NLP-progress</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/DA-southampton/NLP_ability">https://github.com/DA-southampton/NLP_ability</a></p>
<p>NLP 领域经典书籍《Speech and Language Processing》第三版   <a target="_blank" rel="noopener" href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p>
<p>项目是机器学习（Machine Learning）、深度学习（Deep Learning）、NLP面试中常考到的知识点和代码实现，也是一个算法工程师会必选的理论基础知识 <a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p>NLP以及相关的学习实践 <a target="_blank" rel="noopener" href="https://github.com/jarvisqi/machine_learning">https://github.com/jarvisqi/machine_learning</a></p>
<p>机器学习&amp;深入学习资料笔记&amp;基本算法实现&amp;资源整理（ML &#x2F; CV &#x2F; NLP &#x2F; DM…）<a target="_blank" rel="noopener" href="https://github.com/fire717/Machine-Learning">https://github.com/fire717/Machine-Learning</a></p>
<p>Datawhale成员整理的面经内容，包括机器学习，CV，NLP，推荐 <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/daily-interview">https://github.com/datawhalechina/daily-interview</a></p>
<p>人工智能实战大学（面试）学习路线图  <a target="_blank" rel="noopener" href="https://github.com/tangyudi/Ai-Learn">https://github.com/tangyudi/Ai-Learn</a></p>
<p>2018&#x2F;2019&#x2F;校招笔记&#x2F;春招&#x2F;秋招&#x2F;自然者语言处理(NLP)&#x2F;深度机器学习(深度学习)&#x2F;学习(机器学习) <a target="_blank" rel="noopener" href="https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese">https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese</a></p>
<p>深度学习算法教程</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial/tree/master">https://github.com/KeKe-Li/tutorial/tree/master</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/KeKe-Li/tutorial">https://github.com/KeKe-Li/tutorial</a></li>
</ul>
<p>深度学习100例、深度识别学习、图片分类、目标、目标检测、自然语言处理nlp、文本分类、TensorFlow、PyTorch  <a target="_blank" rel="noopener" href="https://github.com/kzbkzb/Python-AI">https://github.com/kzbkzb/Python-AI</a></p>
<h1 id="pyhanlp句法训练"><a href="#pyhanlp句法训练" class="headerlink" title="pyhanlp句法训练"></a>pyhanlp句法训练</h1><p><img src="https://user-images.githubusercontent.com/36963108/169463356-d2faf6c3-557d-49f4-83d7-235ec657c5b3.png" alt="image"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">python train.py </span><br><span class="line">下载 http://file.hankcs.com/corpus/ctb8.0-dep.zip 到 /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/ctb8.0-dep.zip</span><br><span class="line">100%   3.5 MiB 114.3 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">下载 http://file.hankcs.com/corpus/wiki-cn-cluster.zip 到 /opt/conda/lib/python3.6/site-packages/pyhanlp/static/data/test/wiki-cn-cluster.txt.zip</span><br><span class="line">100% 763.9 KiB 353.9 KiB/s ETA:  0 s [=============================================================]</span><br><span class="line">训练集句子数量: 14863                                                                             </span><br><span class="line">迭代 1/20 100.00%  耗时 544 秒。UAS=83.23 LAS=80.84 最高分！保存中...</span><br><span class="line">迭代 2/20 100.00%  耗时 569 秒。UAS=84.06 LAS=81.97 最高分！保存中...</span><br><span class="line">迭代 3/20 100.00%  耗时 557 秒。UAS=84.55 LAS=82.48 最高分！保存中...</span><br><span class="line">迭代 4/20 100.00%  耗时 556 秒。UAS=84.82 LAS=82.74 最高分！保存中...</span><br><span class="line">迭代 5/20 100.00%  耗时 553 秒。UAS=84.95 LAS=82.89 最高分！保存中...</span><br><span class="line">迭代 6/20 100.00%  耗时 549 秒。UAS=85.18 LAS=83.15 最高分！保存中...</span><br><span class="line">迭代 7/20 100.00%  耗时 560 秒。UAS=85.25 LAS=83.26 最高分！保存中...</span><br><span class="line">迭代 8/20 100.00%  耗时 562 秒。UAS=85.12 LAS=83.11</span><br><span class="line">迭代 9/20 100.00%  耗时 569 秒。UAS=85.23 LAS=83.24</span><br><span class="line">迭代 10/20 100.00%  耗时 571 秒。UAS=85.17 LAS=83.23</span><br><span class="line">迭代 11/20 100.00%  耗时 571 秒。UAS=85.20 LAS=83.22</span><br><span class="line">迭代 12/20 100.00%  耗时 581 秒。UAS=85.09 LAS=83.16</span><br><span class="line">迭代 13/20 100.00%  耗时 652 秒。UAS=85.16 LAS=83.24</span><br><span class="line">迭代 14/20 100.00%  耗时 677 秒。UAS=85.21 LAS=83.26</span><br><span class="line">迭代 15/20 100.00%  耗时 586 秒。UAS=85.24 LAS=83.31</span><br><span class="line">迭代 16/20 100.00%  耗时 554 秒。UAS=85.26 LAS=83.33 最高分！保存中...</span><br><span class="line">迭代 17/20 100.00%  耗时 537 秒。UAS=85.38 LAS=83.46 最高分！保存中...</span><br><span class="line">迭代 18/20 100.00%  耗时 548 秒。UAS=85.43 LAS=83.49 最高分！保存中...</span><br><span class="line">迭代 19/20 100.00%  耗时 553 秒。UAS=85.39 LAS=83.43</span><br><span class="line">迭代 20/20 100.00%  耗时 554 秒。UAS=85.41 LAS=83.46</span><br><span class="line">1       人      人      N       NN      _       2       nsubj   _       _</span><br><span class="line">2       吃      吃      V       VV      _       0       ROOT    _       _</span><br><span class="line">3       鱼      鱼      N       NN      _       2       dobj    _       _</span><br><span class="line"></span><br><span class="line">100 ... 200 ... 300 ... 400 ... 500 ... 600 ... 700 ... 800 ... 900 ... 1000 ... 1100 ... 1200 ... 1300 ... 1400 ... 1500 ... 1600 ... 1700 ... 1800 ... 1900 ... UAS=85.4 LAS=83.5</span><br></pre></td></tr></table></figure>

<h1 id="spacy句法"><a href="#spacy句法" class="headerlink" title="spacy句法"></a>spacy句法</h1><p>句法是指句子的各个组成部分的相互关系，句法分析分为句法结构分析（syntactic structure parsing）和依存关系分析(dependency parsing)。句法结构分析用于获取整个句子的句法结构，依存分析用于获取词汇之间的依存关系，目前的句法分析已经从句法结构分析转向依存句法分析。</p>
<p>依存语法通过分析语言单位内成分之间的依存关系揭示其句法结构，主张句子中核心动词是支配其它成分的中心成分，而它本身却不受其它任何成分的支配，所有受支配成分都以某种依存关系从属于支配者。</p>
<p>在20世纪70年代，Robinson提出依存语法中关于依存关系的四条公理：</p>
<ul>
<li>一个句子中只有一个成分是独立的；</li>
<li>其它成分直接依存于某一成分；</li>
<li>任何一个成分都不能依存与两个或两个以上的成分；</li>
<li>如果A成分直接依存于B成分，而C成分在句中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分；</li>
</ul>
<p>SpaCy 中文模型:<a target="_blank" rel="noopener" href="https://github.com/howl-anderson/Chinese_models_for_SpaCy">https://github.com/howl-anderson/Chinese_models_for_SpaCy</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/lllhhhv/article/details/123335675">https://blog.csdn.net/lllhhhv/article/details/123335675</a><br>zh_core_web_trf、zh_core_web_md 等,它们的区别在于准确度和体积大小, zh_core_web_sm 体积小,准确度相比zh_core_web_trf差,zh_core_web_trf相对就体积大。这样可以适应不同场景.</p>
<h1 id="数据参考"><a href="#数据参考" class="headerlink" title="数据参考"></a>数据参考</h1><p>hanlp.pretrained.dep。CTB5_BIAFFINE_DEP_ZH&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb5_20191229_025833.zip&#39;</a><br>在 CTB5 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017）。</p>
<p>hanlp.pretrained.dep。CTB7_BIAFFINE_DEP_ZH&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;">https://file.hankcs.com/hanlp/dep/biaffine_ctb7_20200109_022431.zip&#39;</a><br>在 CTB7 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017）。</p>
<p>hanlp.pretrained.dep。CTB9_DEP_ELECTRA_SMALL&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;">https://file.hankcs.com/hanlp/dep/ctb9_dep_electra_small_20220216_100306.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 CTB9-SD330 上训练。性能为 UAS&#x3D;87.68% LAS&#x3D;83.54%。</p>
<p>hanlp.pretrained.dep。CTB9_UDC_ELECTRA_SMALL&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;">https://file.hankcs.com/hanlp/dep/udc_dep_electra_small_20220218_095452.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 CTB9-UD420 上训练。性能是 UAS&#x3D;85.92% LAS&#x3D;81.13% 。</p>
<p>hanlp.pretrained.dep。PMT1_DEP_ELECTRA_SMALL&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;">https://file.hankcs.com/hanlp/dep/pmt_dep_electra_small_20220218_134518.zip&#39;</a><br>Electra 小型编码器 ( Clark et al. 2020 ) 和 Biaffine 解码器 ( Dozat &amp; Manning 2017 ) 在 PKU Multi-view Chinese Treebank (PMT) 1.0 ( Qiu et al. 2014 ) 上训练。性能是 UAS&#x3D;91.21% LAS&#x3D;88.65%。</p>
<p>hanlp.pretrained.dep。PTB_BIAFFINE_DEP_EN&#x3D; ‘<a target="_blank" rel="noopener" href="https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;">https://file.hankcs.com/hanlp/dep/ptb_dep_biaffine_20200101_174624.zip&#39;</a><br>在 PTB 上训练的Biaffine LSTM 模型（Dozat &amp; Manning 2017 ）。</p>
<p>参考来自：<a target="_blank" rel="noopener" href="https://hanlp.hankcs.com/docs/api/hanlp/index.html">https://hanlp.hankcs.com/docs/api/hanlp/index.html</a></p>
<h1 id="ctb数据集相关论文"><a href="#ctb数据集相关论文" class="headerlink" title="ctb数据集相关论文"></a>ctb数据集相关论文</h1><p><a target="_blank" rel="noopener" href="https://github.com/textflint/textflint.github.io/blob/b387b412642bdaf61fc49173a4e6077c8a0d372a/Tasks/DPCN/paper_list.json">DPCN&#x2F;dataset_paper.json</a></p>
<p>hanlp句法分析训练问题解决：<a target="_blank" rel="noopener" href="https://bbs.hankcs.com/t/topic/2868">https://bbs.hankcs.com/t/topic/2868</a></p>
<h1 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别:"></a>命名实体识别:</h1><p>命名实体识别从早期基于词典和规则的方法，到传统机器学习的方法， 后来采用基于深度学习的方法，一直到当下热门的注意力机制、图神经网络等研究方法， 命名实体识别技术路线随着时间在不断发展。</p>
<p><img src="https://user-images.githubusercontent.com/36963108/178393815-01045ee4-885e-4aec-9231-c65cfa3af835.png" alt="image"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TianRanPig/chinese_ner">https://github.com/TianRanPig/chinese_ner</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUENER2020">https://github.com/CLUEbenchmark/CLUENER2020</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/hemingkx/CLUENER2020">https://github.com/hemingkx/CLUENER2020</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lonePatient/BERT-NER-Pytorch">https://github.com/lonePatient/BERT-NER-Pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lemonhu/NER-BERT-pytorch">https://github.com/lemonhu/NER-BERT-pytorch</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/TobiasLee/ChineseNER">https://github.com/TobiasLee/ChineseNER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice">https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/luopeixiang/named_entity_recognition">https://github.com/luopeixiang/named_entity_recognition</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/F-debug/Medical-named-entity-recognition">https://github.com/F-debug/Medical-named-entity-recognition</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kyzhouhzau/BERT-NER">https://github.com/kyzhouhzau/BERT-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/macanv/BERT-BiLSTM-CRF-NER">https://github.com/macanv/BERT-BiLSTM-CRF-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/xuanzebi/BERT-CH-NER">https://github.com/xuanzebi/BERT-CH-NER</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p>
<p>使用bert做领域分类、意图识别和槽位填充任务 <a target="_blank" rel="noopener" href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p>
<p>基于pytorch的中文意图识别和槽位填充 <a target="_blank" rel="noopener" href="https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling">https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling</a></p>
<p>基于BERT+Tensorflow-1.15+Horovod-0.22的NLU（意图识别+槽位填充）分布式GPU训练模块 <a target="_blank" rel="noopener" href="https://github.com/jx1100370217/JointBERT_nlu_tf">https://github.com/jx1100370217/JointBERT_nlu_tf</a></p>
<p>使用bert做领域分类、配置识别和位置填充任务 <a target="_blank" rel="noopener" href="https://github.com/xiaopp123/bert-joint-NLU">https://github.com/xiaopp123/bert-joint-NLU</a></p>
<p>中文语言理解基准、基准中文语言理解评估基准：数据集、预训练模型、语料库 <a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/CLUE">https://github.com/CLUEbenchmark/CLUE</a></p>
<p>用于联合意图分类和插槽填充的 BERT <a target="_blank" rel="noopener" href="https://github.com/monologg/JointBERT">https://github.com/monologg/JointBERT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification">https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/pymacbit/BERT-Intent-Classification">https://github.com/pymacbit/BERT-Intent-Classification</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/ensembles4612/medical_intent_detector_using_BERT">https://github.com/ensembles4612/medical_intent_detector_using_BERT</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb">https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/sz128/slot_filling_and_intent_detection_of_SLU">https://github.com/sz128/slot_filling_and_intent_detection_of_SLU</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/471417367/bert_intention_zh">https://github.com/471417367/bert_intention_zh</a></p>
<p>数据集自动标注工具–释放AI潜力！<a target="_blank" rel="noopener" href="https://www.modelfun.cn/home">https://www.modelfun.cn/home</a></p>
<p>实体识别数据集 <a target="_blank" rel="noopener" href="https://github.com/juand-r/entity-recognition-datasets">https://github.com/juand-r/entity-recognition-datasets</a></p>
<p>ner综述： <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45884316/article/details/118684681">https://blog.csdn.net/weixin_45884316/article/details/118684681</a></p>
<p>使用 CLIP 将图像和句子嵌入到固定长度的向量中 <a target="_blank" rel="noopener" href="https://github.com/jina-ai/clip-as-service">https://github.com/jina-ai/clip-as-service</a></p>
<p>other:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset">https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Hyfred/Pytroch_NER_tutorial">https://github.com/Hyfred/Pytroch_NER_tutorial</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp">https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py">https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kamalkraj/BERT-NER">https://github.com/kamalkraj/BERT-NER</a></p>
<p>参考：<a target="_blank" rel="noopener" href="https://github.com/kyzhouhzau/NLPGNN">https://github.com/kyzhouhzau/NLPGNN</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[1] BERT：用于语言理解的深度双向转换器的预训练</span><br><span class="line">[2] ALBERT：用于语言表示的自监督学习的 Lite BERT</span><br><span class="line">[3]语言模型是无监督的多任务学习者</span><br><span class="line">[4]用于量子化学的神经消息传递</span><br><span class="line">[ 5]使用图卷积网络进行半监督分类</span><br><span class="line">[6]图注意网络</span><br><span class="line">[7]图神经网络有多强大？</span><br><span class="line">[8] GraphSAGE：大图上的归纳表示学习</span><br><span class="line">[9]扩散改进了图学习</span><br><span class="line">[10]基准图神经网络</span><br><span class="line">[11]用于文本分类的文本级图神经网络</span><br><span class="line">[12]用于文本分类的图卷积网络</span><br><span class="line">[13]用于文本分类的张量图卷积网络</span><br><span class="line">[14]深入了解用于半监督学习的图卷积网络</span><br></pre></td></tr></table></figure>

<p>命名实体识别（NER）标注神器: <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44193969/article/details/123298406">https://blog.csdn.net/qq_44193969/article/details/123298406</a></p>
<p>实践：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44193969/article/details/116008734">https://blog.csdn.net/qq_44193969/article/details/116008734</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction">https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40846933/article/details/106384566">https://blog.csdn.net/weixin_40846933/article/details/106384566</a></p>
</a></a></a></a></a></a></a></a>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AE%BA%E6%96%87/">人工智能论文</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI%E8%AE%BA%E6%96%87/" rel="tag">AI论文</a></li></ul>

      
        <div id="donation_div"></div>


<script src="/js/vdonate.js"></script>

<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: '打赏支持', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://github.com/KangChou/2020PythonKangChou/blob/master/wxzf.jpg',
  alipayImage: ''
});
</script>
      

      <!-- 要添加的内容 -->
      
      <div>
        <ul class="post-copyright">
          <li class="post-copyright-author">
          <strong>作者:  </strong>KangChou</a>
          </li>
          <li class="post-copyright-link">
          <strong>文章链接:  </strong>
          <a href="/2019/07/05/papers/" target="_blank" title="计算机科学与人工智能论文汇集">https://www.coomatrix.com/2019/07/05/papers/</a>
          </li>
          <li class="post-copyright-license">
            <strong>版权声明:   </strong>
            本博客所有文章除特别声明外，均采用 <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
            许可协议。转载请注明出处！
          </li>
        </ul>
      <div>
      
      <!---->

            
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC80OTQ4NC8yNTk3Ng==">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/04/05/blog/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          值得你阅读的Hexo个人博客搭建：不用购买服务器，不用购买域名，不要钱，不用敲代码等等，是的，你没有看错，快来转载学习吧！
        
      </div>
    </a>
  
  
    <a href="/2018/04/05/latex/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">学术论文排版工具LaTeX</div>
    </a>
  
</nav>

  
</article>



<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article" style="overflow-y: scroll; max-width: 28%;">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E4%B9%A6%E7%B1%8D"><span class="nav-number">1.</span> <span class="nav-text">计算机科学技术书籍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AI%E4%B9%A6%E7%B1%8D%E4%B8%8E%E7%AE%97%E6%B3%95%E6%BA%90%E7%A0%81"><span class="nav-number">2.</span> <span class="nav-text">AI书籍与算法源码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81"><span class="nav-number">3.</span> <span class="nav-text">计算机视觉实时动态</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D-%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B"><span class="nav-number">4.</span> <span class="nav-text">3D 对象检测</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A1%B6%E7%BA%A7%E4%BC%9A%E8%AE%AE%E5%92%8C%E7%A0%94%E8%AE%A8%E4%BC%9A"><span class="nav-number">6.</span> <span class="nav-text">顶级会议和研讨会</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%9A%E8%AE%AE"><span class="nav-number">7.</span> <span class="nav-text">会议</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%9C%E5%9D%8A"><span class="nav-number">8.</span> <span class="nav-text">作坊</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">论文（基于激光雷达的方法）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AB%9E%E8%B5%9B%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">10.</span> <span class="nav-text">竞赛解决方案</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B7%A5%E7%A8%8B"><span class="nav-number">11.</span> <span class="nav-text">工程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B0%83%E6%9F%A5"><span class="nav-number">12.</span> <span class="nav-text">调查</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B9%A6"><span class="nav-number">13.</span> <span class="nav-text">书</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%86%E9%A2%91"><span class="nav-number">14.</span> <span class="nav-text">视频</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B"><span class="nav-number">15.</span> <span class="nav-text">课程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%9A%E5%AE%A2"><span class="nav-number">16.</span> <span class="nav-text">博客</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%91%97%E5%90%8D%E7%A0%94%E7%A9%B6%E7%BB%84-x2F-%E5%AD%A6%E8%80%85"><span class="nav-number">17.</span> <span class="nav-text">著名研究组&#x2F;学者</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%91%97%E5%90%8D%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BA%93"><span class="nav-number">18.</span> <span class="nav-text">著名的代码库</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%82%B9%E4%BA%91%E5%8F%82%E8%80%83%E8%AE%BA%E6%96%87%E6%9D%A5%E6%BA%90"><span class="nav-number">19.</span> <span class="nav-text">深度学习点云参考论文来源</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">20.</span> <span class="nav-text">

1
- Recent papers (from 2017)


</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">20.0.1.</span> <span class="nav-text"> Keywords </span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017"><span class="nav-number">20.1.</span> <span class="nav-text">2017</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2018"><span class="nav-number">20.2.</span> <span class="nav-text">2018</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">20.3.</span> <span class="nav-text">2019</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2020"><span class="nav-number">20.4.</span> <span class="nav-text">2020</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2021"><span class="nav-number">20.5.</span> <span class="nav-text">2021</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">21.</span> <span class="nav-text">

1
- Datasets


</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%9D%A5%E6%BA%90"><span class="nav-number">22.</span> <span class="nav-text">参考来源</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cvpr2021-x2F-cvpr2020-x2F-cvpr2019-x2F-cvpr2018-x2F-cvpr2017%EF%BC%88Papers-x2F-Codes-x2F-Project-x2F-Paper-reading%EF%BC%89"><span class="nav-number">23.</span> <span class="nav-text">cvpr2021&#x2F;cvpr2020&#x2F;cvpr2019&#x2F;cvpr2018&#x2F;cvpr2017（Papers&#x2F;Codes&#x2F;Project&#x2F;Paper reading）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95"><span class="nav-number">24.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-CVPR2021%E6%9C%80%E6%96%B0%E8%AE%BA%E6%96%87%E5%88%86%E7%B1%BB%E6%B1%87%E6%80%BB-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0"><span class="nav-number">25.</span> <span class="nav-text">8.CVPR2021最新论文分类汇总(持续更新)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-CVPR2021%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">26.</span> <span class="nav-text">7.CVPR2021论文分方向盘点&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-CVPR2020%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD-x2F-%E4%BB%A3%E7%A0%81-x2F-%E8%A7%A3%E8%AF%BB-x2F-%E7%9B%B4%E6%92%AD"><span class="nav-number">27.</span> <span class="nav-text">6.CVPR2020论文下载&#x2F;代码&#x2F;解读&#x2F;直播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-CVPR2020%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">28.</span> <span class="nav-text">5.CVPR2020论文分方向盘点&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-CVPR2019%E5%85%A8%E9%83%A8%E8%AE%BA%E4%B8%8B%E8%BD%BD-x2F-%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81-lt-br-gt"><span class="nav-number">29.</span> <span class="nav-text">4.CVPR2019全部论下载&#x2F;开源代码&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-CVPR2019%E8%AE%BA%E6%96%87%E5%88%86%E6%96%B9%E5%90%91%E7%9B%98%E7%82%B9-lt-br-gt"><span class="nav-number">30.</span> <span class="nav-text">3.CVPR2019论文分方向盘点&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-CVPR2019%E8%AE%BA%E6%96%87%E7%9B%B4%E6%92%AD%E5%88%86%E4%BA%AB-lt-br-gt"><span class="nav-number">31.</span> <span class="nav-text">2.CVPR2019论文直播分享&lt;br&gt;</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-CVPR2018-x2F-CVPR2017-lt-br-gt"><span class="nav-number">32.</span> <span class="nav-text">1.CVPR2018&#x2F;CVPR2017&lt;br&gt;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5-lt-br-gt"><span class="nav-number">32.0.1.</span> <span class="nav-text">参考链接&lt;br&gt;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLPs"><span class="nav-number">33.</span> <span class="nav-text">NLPs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="nav-number">34.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB"><span class="nav-number">35.</span> <span class="nav-text">资料汇总</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pyhanlp%E6%96%87%E6%9C%AC%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8BAPI%E6%8E%A5%E5%8F%A3"><span class="nav-number">36.</span> <span class="nav-text">pyhanlp文本训练与预测API接口</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#pyhanlp%E5%8F%A5%E6%B3%95%E8%AE%AD%E7%BB%83"><span class="nav-number">37.</span> <span class="nav-text">pyhanlp句法训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spacy%E5%8F%A5%E6%B3%95"><span class="nav-number">38.</span> <span class="nav-text">spacy句法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%82%E8%80%83"><span class="nav-number">39.</span> <span class="nav-text">数据参考</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ctb%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-number">40.</span> <span class="nav-text">ctb数据集相关论文</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB"><span class="nav-number">41.</span> <span class="nav-text">命名实体识别:</span></a></li></ol>
    
    </div>
  </aside>


</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2022 ovo$^{mc^2}$ All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->

<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/photography" class="mobile-nav-link">photography</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/guestbook" class="mobile-nav-link">guestbook</a>
  
    <a href="/AI" class="mobile-nav-link">AI</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>




<script src="/js/scripts.js"></script>


<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>


<script src="/js/main.js"></script>








  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="https://dnqof95d40fo6.cloudfront.net/atw7f8.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body>
</html>


